{"pages":[{"title":"17 sets, how to display all intersections with UpSetR plot?","text":"Hello! Trying to make an UpSetR plot showing overlapping gene annotations between 17 transcriptomes. What am I doing wrong? Is there a bug in the UpSetR library? I posted on the UpSetR library's community GitHub forum , but did not really get an answer to my questions. My R language expert colleagues at UC Davis could not seem to figure it out either. Here is what I know: There ARE overlapping identities between all 17 of my sets (see code and output below). However, my UpSetR plots do not reflect this. Below is some R code with a downloadable set of data. library ( UpSetR ) # https://cran.r-project.org/web/packages/UpSetR/ # https://cran.r-project.org/web/packages/UpSetR/vignettes/basic.usage.html # Download and import to a data.frame if ( ! file . exists ( 'presence_absence.csv' )){ download . file ( 'https://raw.githubusercontent.com/johnsolk/RNAseq_15killifish/master/evaluation/presence_absence.csv' , 'presence_absence.csv' )} pa <- read . csv ( \"presence_absence.csv\" ) pa <- pa [, - c ( 1 )] rownames ( pa ) <- pa $ Ensembl pa <- pa [, - c ( 1 )] This plot will display 4 intersects. THIS WORKS. This is what we should see for all 17 sets. upset(pa, sets = c(c(\"A_xenica\",\"F_catanatus\"),c(\"F_chrysotus\",\"F_diaphanus\")),mb.ratio = c(0.55, 0.45),keep.order = F) But, as soon as I move to 6, I am confused - there is no longer a bar crossing all 6 sets. upset(pa, sets = c(c(\"A_xenica\",\"F_catanatus\",\"F_notatus\"), c(\"F_chrysotus\",\"F_diaphanus\",\"F_nottii\")),mb.ratio = c(0.55, 0.45),nsets = 6,keep.order = F) The full 17 sets does not make any sense upset(pa, sets = c(c(\"A_xenica\",\"F_catanatus\"),c(\"F_chrysotus\",\"F_diaphanus\"),c(\"F_grandis\",\"F_heteroclitusMDPL\"),c(\"F_heteroclitusMDPP\",\"F_notatus\"),c(\"F_nottii\",\"F_olivaceous\"),c(\"F_parvapinis\",\"F_rathbuni\"),c(\"F_sciadicus\",\"F_similis\"),c(\"F_zebrinus\",\"L_goodei\"),c(\"L_parva\")),mb.ratio = c(0.55, 0.45),keep.order = F) This confirms that there are overlaps between all 17 sets (and 16, 15, 14, etc) test <- rowSums ( pa ) test <- as.data.frame ( test ) colnames ( test ) <- c ( \"sum\" ) sum ( test $ sum == 17 ) sum ( test $ sum == 16 ) sum ( test $ sum == 15 ) sum ( test $ sum == 14 ) Output: > sum(test$sum == 17) [1] 10897 > sum(test$sum == 16) [1] 2979 > sum(test$sum == 15) [1] 1585 > sum(test$sum == 14) [1] 1247 I'm wondering if there is a bug in the UpSetR library that prevents displaying overlaps from such a large number of sets. I remembered Titus made a similar set of data looking at annotations from a few species with the python library, pyupset . When I made a similar notebook using pyupset with my data , there was a performance issue. The notebook ran overnight and didn't produce any output. This makes me think that it might not be possible to produce a plot with 17 intersections? Any insight anyone has would be greatly appreciated!","tags":"visualizations","url":"https://johnsolk.github.io/blog/17-sets-how-to-display-all-intersections-with-upsetr-plot.html"},{"title":"XSEDE login with bridges","text":"Quick tutorial for how to use PSC Bridges with an XSEDE allocation. See here for our research proposal and here for examples of startup allocation proposals. 1. Login login info There are two ways. (Pick one. If one doesn't work, try the other.) A. Direct login to bridges ssh -p 2222 ljcohen@bridges.psc.xsede.org password select DUO option Now you're on bridges. The command prompt should look similar to this: [ljcohen@login006 ~]$ B. SSO (single sign-on hub) ssh ljcohen@login.xsede.org password select DUO option Prompt will then look like this: [ljcohen@ssohub ~]$ Then gsissh bridges Now you're on bridges. The command prompt should look similar to this: [ljcohen@login018 ~]$ 2. Where are we? Type: projects This will show you the allocations we have. We have three separate PSC bridges allocations . Keep track of how much allocation you're using with this projects command. 1. Storage 2. RM 3. LM Under \"BRIDGES PYLON STORAGE\", it will list the directories path where the Lustre storage is located. All users who are on the allocation will have their own directory within this project. By default, you do not have read or write access to other users' directories, only your own. So, don't worry about making a mistake and deleting someone else's work. Note that space in your ~/ home directory is limited. PSC says 10GB , but I've run out of space installing miniconda software, so I think it's more like 10 or 100 MB. (but haven't confirmed this) Because of this limited space in home, all files should go in your Lustre storage directory. For example, I've setup miniconda to install software in my storage directory: /pylon5/bi5fpmp/ljcohen/miniconda3/bin If you're on more than one allocation, your working directories should be in account-specific, project-specific storage. For example, I'm on 2 allocations: Allocation 1: cd /pylon5/bi5fpmp/ljcohen/ ls -lah Allocation 2: cd /pylon5/mc5phkp/ljcohen/ ls -lah 3. Computing Things. PSC bridges user guide Running jobs There are generally 3 compute node partitions available , depending on what you need to do: Large Memory (LM) Regular Memory (RM) Regular Memory-Interactive (RM-interactive), for interactive login to test commands so they don't run on the head node. Examples: Run an interactive job for 1 hr to see if commands/software will run. (It might take a little while, ~5-10 min for job to be queued, so be patient): interact -p RM-shared --ntasks-per-node=4 -N 1 -t 1:00:00 LM job example script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #!/bin/bash -l #SBATCH -D /pylon5/mc5phkp/ljcohen/kfish_abyss #SBATCH -J Axen-abyss #SBATCH -o /pylon5/mc5phkp/ljcohen/kfish_abyss/abyss-Axen%j.o #SBATCH -e /pylon5/mc5phkp/ljcohen/kfish_abyss/abyss-Axen%j.o #SBATCH -t 120:00:00 #SBATCH -p LM #SBATCH --mem=1000GB #SBATCH --ntasks-per-node 14 source ~/.bashrc source activate assembly cd /pylon5/mc5phkp/ljcohen/kfish_abyss SPECIES = Axen PROJECTDIR = $LOCAL / $SPECIES / mkdir $PROJECTDIR cd $PROJECTDIR cp /pylon5/mc5phkp/ljcohen/kfish_abyss/Axenica_USPD16092508-N706-AK391_HV3JCCCXY_L6_1.qc.fq.gz . cp /pylon5/mc5phkp/ljcohen/kfish_abyss/Axenica_USPD16092508-N706-AK391_HV3JCCCXY_L6_2.qc.fq.gz . cp /pylon5/mc5phkp/ljcohen/kfish_abyss/Axenica_USPD16092508-N706-AK391_HV3JCCCXY_L7_1.qc.fq.gz . cp /pylon5/mc5phkp/ljcohen/kfish_abyss/Axenica_USPD16092508-N706-AK391_HV3JCCCXY_L7_2.qc.fq.gz . abyss-pe k = 51 name = Axen_abyss in = \"Axenica_USPD16092508-N706-AK391_HV3JCCCXY_L6_1.qc.fq.gz Axenica_USPD16092508-N706-AK391_HV3JCCCXY_L6_2.qc.fq.gz Axenica_USPD16092508-N706-AK391_HV3JCCCXY_L7_1.qc.fq.gz Axenica_USPD16092508-N706-AK391_HV3JCCCXY_L7_2.qc.fq.gz\" contigs # Grab output assembly files and copy to your storage cp $PROJECTDIR /Axen_abyss /pylon5/mc5phkp/ljcohen/kfish_abyss/ # remove temp files on $LOCAL rm -rf $PROJECTDIR RM job example script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash -l #SBATCH -D /pylon5/mc5phkp/ljcohen/kfish_Illumina/ #SBATCH -J Axen-diginorm #SBATCH -o /pylon5/mc5phkp/ljcohen/kfish_Illumina/diginorm-Axen%j.o #SBATCH -e /pylon5/mc5phkp/ljcohen/kfish_Illumina/diginorm-Axen%j.o #SBATCH -t 48:00:00 #SBATCH -p RM #SBATCH --ntasks-per-node 8 #SBATCH --cpus-per-task 1 source /home/ljcohen/.bashrc source activate sourmash_conda cd /pylon5/mc5phkp/ljcohen/kfish_Illumina/ ( paste \\ < ( zcat { Axenica_USPD16092508-N706-AK391_HV3JCCCXY_L6_1.qc.fq.gz,Axenica_USPD16092508-N706-AK391_HV3JCCCXY_L7_1.qc.fq.gz } | paste - - - - ) \\ < ( zcat { Axenica_USPD16092508-N706-AK391_HV3JCCCXY_L6_2.qc.fq.gz,Axenica_USPD16092508-N706-AK391_HV3JCCCXY_L7_2.qc.fq.gz } | paste - - - - ) \\ | tr '\\t' '\\n' ) | \\ ( trim-low-abund.py -k 20 -C 2 - -o - -x 5e7 ) | \\ ( extract-paired-reads.py --gzip -p A_xen.paired.gz -s A_xen.single.gz ) > /dev/null Queue management Submit script to slurm queue as a job: sbatch yourfile Check the queue status: squeue -u ljcohen Cancel job: scancel 5280622 In an emergency, if you've (hypothetically) submitted a lot of jobs you don't want, cancel all job ID beginning with numbers 528 : squeue -u ljcohen | grep \"528\" | sed -r -e \"s/[\\t\\ ]+/./g\" | cut -d\".\" -f 2 | xargs scancel Let's do something simple. From the blast commandline tutorial from angus : cd /pylon5/mc5phkp/ljcohen mkdir test cd test Download: curl -o mouse.1.protein.faa.gz -L https://osf.io/v6j9x/download curl -o mouse.2.protein.faa.gz -L https://osf.io/j2qxk/download curl -o zebrafish.1.protein.faa.gz -L https://osf.io/68mgf/download Decompress: gunzip *.faa.gz Load module: module load blast / 2.7.1 Make makeblastdb -in zebrafish.1.protein.faa -dbtype prot Run interactive job: interact -p RM-shared --ntasks-per-node=4 -N 1 -t 1:00:00 Run this interactively, or create a script file and submit as an RM job: cd test blastp -query mouse.1.protein.faa -db zebrafish.1.protein.faa -out mm.x.zebrafish.tsv -outfmt 6 4. How to Transfer files scp a directory of files from bridges to your local computer, must use port 2222 : scp -r -P 2222 ljcohen@data.bridges.psc.edu:/location/ .","tags":"computing","url":"https://johnsolk.github.io/blog/xsede-login-with-bridges.html"},{"title":"Annotating multiple de novo transcriptome assemblies","text":"Collapsing contigs by annotation is an appropriate method for a de novo transcriptome assembly to be used as a reference for differential expression analysis. Background I have been working with RNAseq data for several years now, which we're using for a cross-species differential expression analysis. This is not a trivial analysis, and I've had to go back and re-do everything a few times now because the annotations and/or quantification results haven't made sense for various reasons. I'm hoping that we're getting closer. De novo transcriptome assemblies were made with Trinity (version 2.8.4) from 17 different species of North American Fundulus killifish (17 separate assemblies with input ranging from 50 million to ~450 million PEx100 reads each). Expression was quantified using salmon (version 0.12.0) with the trimmed reads against Trinity assemblies. I've annotated the Trinity assemblies using the dammit pipeline (version 1.1). I would like to use OrthoFinder to guide a cross-species comparison of differential expression. I was thinking this would be a good way to ensure that we are comparing the same genes across all species. I ran OrthoFinder version 2.2.7 ( conda install in a python 2 conda environment ) with all of the translated Trinity assemblies (.pep files from Transdecoder version 3.0.1). From the Orthogroups.csv output, I assigned OGs to their corresponding transcript ID from Trinity. Then, I summarized OG expression per species by collapsing multiple contigs per OG with tximport . I did the same for the NCBI annotations, collapsing multiple contigs per gene ID. However, I found that the expression tables per species summarized by OGs did not appear to be as specific as they are when summarized by NCBI genes. OGs: NCBI: This was confusing, so I went digging a little further and posted this question on the OrthoFinder repository. Is this what one might expect from expression summarized by OGs? There might be a different way to incorporate OrthoFinder results to this analysis. (Open to suggestions!) After going through this exercise, I'm further convinced that it is fine to use counts summarized by NCBI genes (or another annotation database). Decisions for what gene to assign to a contig might need some tweaking. This was helpful to go through by hand. Below I've pasted pieces of output files that I've created through scripts and various tools. Example 1: What do the salmon quantification counts look like? I started off by looking at the annotations and raw salmon output from each replicate for one of my species, Adinia xenica (the diamond killfish). Since Trinity groups contigs based on shared sequence content , we would expect this transcript cluster to encode for mostly same gene. According to the Trinity assembler, these contigs should be grouped together. TrinityID seqid TRINITY_DN8_c0_g1_i7 Transcript_0 TRINITY_DN8_c0_g1_i6 Transcript_1 TRINITY_DN8_c0_g1_i1 Transcript_2 TRINITY_DN8_c0_g1_i9 Transcript_3 TRINITY_DN8_c0_g1_i4 Transcript_4 TRINITY_DN8_c0_g1_i5 Transcript_5 TRINITY_DN8_c0_g1_i2 Transcript_6 TRINITY_DN8_c0_g1_i3 Transcript_7 TRINITY_DN8_c0_g2_i1 Transcript_8 TRINITY_DN8_c1_g1_i8 Transcript_9 TRINITY_DN8_c1_g1_i1 Transcript_10 TRINITY_DN8_c1_g1_i6 Transcript_11 TRINITY_DN8_c1_g1_i9 Transcript_12 TRINITY_DN8_c1_g1_i3 Transcript_13 TRINITY_DN8_c1_g1_i4 Transcript_14 TRINITY_DN8_c1_g1_i7 Transcript_15 TRINITY_DN8_c1_g1_i5 Transcript_16 TRINITY_DN8_c6_g1_i2 Transcript_17 TRINITY_DN8_c6_g1_i7 Transcript_18 TRINITY_DN8_c6_g1_i4 Transcript_19 TRINITY_DN8_c6_g1_i1 Transcript_20 TRINITY_DN8_c12_g1_i1 Transcript_21 At a glance, the transcript-level quantification results for the same block of transcripts TRINITY_DN8 seem reasonably consistent across samples. A_xenica_BW_1.quant Name Length EffectiveLength TPM NumReads TRINITY_DN8_c0_g1_i7 1034 813.841 99.087379 1090.219 TRINITY_DN8_c0_g1_i6 1035 814.841 508.960349 5606.770 TRINITY_DN8_c0_g1_i1 3195 2974.841 174.769095 7028.853 TRINITY_DN8_c0_g1_i9 2874 2653.841 142.575381 5115.352 TRINITY_DN8_c0_g1_i4 2155 1934.841 121.333793 3173.826 TRINITY_DN8_c0_g1_i5 2107 1886.841 62.580847 1596.367 TRINITY_DN8_c0_g1_i2 1083 862.841 648.745796 7567.654 TRINITY_DN8_c0_g1_i3 986 766.007 75.482570 781.692 TRINITY_DN8_c0_g2_i1 961 741.012 5.490111 55.000 TRINITY_DN8_c1_g1_i8 5552 5331.841 0.213893 15.418 TRINITY_DN8_c1_g1_i1 4350 4129.841 14.202186 792.948 TRINITY_DN8_c1_g1_i6 1785 1564.841 2.334244 49.382 TRINITY_DN8_c1_g1_i9 5729 5508.841 0.000000 0.000 TRINITY_DN8_c1_g1_i3 1920 1699.841 25.058676 575.867 TRINITY_DN8_c1_g1_i4 4375 4154.841 0.000000 0.000 TRINITY_DN8_c1_g1_i7 4198 3977.841 37.372865 2009.833 TRINITY_DN8_c1_g1_i5 5704 5483.841 1.180930 87.552 TRINITY_DN8_c6_g1_i2 2373 2152.841 0.000000 0.000 TRINITY_DN8_c6_g1_i7 2371 2150.841 1.360954 39.574 TRINITY_DN8_c6_g1_i4 1995 1774.841 0.309493 7.426 TRINITY_DN8_c6_g1_i1 2369 2148.841 0.000000 0.000 TRINITY_DN8_c12_g1_i1 391 172.918 0.855528 2.000 A_xenica_BW_2.quant Name Length EffectiveLength TPM NumReads TRINITY_DN8_c0_g1_i7 1034 784.489 59.392151 460.803 TRINITY_DN8_c0_g1_i6 1035 785.489 388.568757 3018.610 TRINITY_DN8_c0_g1_i1 3195 2945.489 51.097357 1488.521 TRINITY_DN8_c0_g1_i9 2874 2624.489 425.297355 11039.176 TRINITY_DN8_c0_g1_i4 2155 1905.489 118.953657 2241.731 TRINITY_DN8_c0_g1_i5 2107 1857.489 41.453127 761.523 TRINITY_DN8_c0_g1_i2 1083 833.489 793.453684 6540.645 TRINITY_DN8_c0_g1_i3 986 736.730 31.564827 229.991 TRINITY_DN8_c0_g2_i1 961 711.735 1.136507 8.000 TRINITY_DN8_c1_g1_i8 5552 5302.489 0.544643 28.562 TRINITY_DN8_c1_g1_i1 4350 4100.489 9.770186 396.221 TRINITY_DN8_c1_g1_i6 1785 1535.489 2.184526 33.174 TRINITY_DN8_c1_g1_i9 5729 5479.489 0.000000 0.000 TRINITY_DN8_c1_g1_i3 1920 1670.489 13.325110 220.147 TRINITY_DN8_c1_g1_i4 4375 4125.489 0.000000 0.000 TRINITY_DN8_c1_g1_i7 4198 3948.489 34.021297 1328.560 TRINITY_DN8_c1_g1_i5 5704 5454.489 1.822869 98.335 TRINITY_DN8_c6_g1_i2 2373 2123.489 1.634330 34.323 TRINITY_DN8_c6_g1_i7 2371 2121.489 0.000000 0.000 TRINITY_DN8_c6_g1_i4 1995 1745.489 1.429456 24.677 TRINITY_DN8_c6_g1_i1 2369 2119.489 0.000000 0.000 TRINITY_DN8_c12_g1_i1 391 148.487 0.680946 1.000 A_xenica_BW_3.quant Name Length EffectiveLength TPM NumReads TRINITY_DN8_c0_g1_i7 1034 806.011 81.299939 878.686 TRINITY_DN8_c0_g1_i6 1035 807.011 419.558693 4540.195 TRINITY_DN8_c0_g1_i1 3195 2967.011 110.800505 4408.222 TRINITY_DN8_c0_g1_i9 2874 2646.011 364.374601 12928.325 TRINITY_DN8_c0_g1_i4 2155 1927.011 102.662765 2652.770 TRINITY_DN8_c0_g1_i5 2107 1879.011 47.025571 1184.857 TRINITY_DN8_c0_g1_i2 1083 855.011 752.616894 8628.753 TRINITY_DN8_c0_g1_i3 986 758.184 44.182512 449.188 TRINITY_DN8_c0_g2_i1 961 733.190 1.118854 11.000 TRINITY_DN8_c1_g1_i8 5552 5324.011 1.615538 115.334 TRINITY_DN8_c1_g1_i1 4350 4122.011 14.339296 792.573 TRINITY_DN8_c1_g1_i6 1785 1557.011 1.727863 36.075 TRINITY_DN8_c1_g1_i9 5729 5501.011 0.000000 0.000 TRINITY_DN8_c1_g1_i3 1920 1692.011 15.569566 353.250 TRINITY_DN8_c1_g1_i4 4375 4147.011 0.000000 0.000 TRINITY_DN8_c1_g1_i7 4198 3970.011 33.733068 1795.767 TRINITY_DN8_c1_g1_i5 5704 5476.011 0.000000 0.000 TRINITY_DN8_c6_g1_i2 2373 2145.011 0.000000 0.000 TRINITY_DN8_c6_g1_i7 2371 2143.011 0.000000 0.000 TRINITY_DN8_c6_g1_i4 1995 1767.011 0.832253 19.720 TRINITY_DN8_c6_g1_i1 2369 2141.011 1.890698 54.280 TRINITY_DN8_c12_g1_i1 391 165.937 0.449423 1.000 A_xenica_FW_1.quant Name Length EffectiveLength TPM NumReads TRINITY_DN8_c0_g1_i7 1034 788.009 47.336066 401.851 TRINITY_DN8_c0_g1_i6 1035 789.009 312.887387 2659.571 TRINITY_DN8_c0_g1_i1 3195 2949.009 112.315309 3568.261 TRINITY_DN8_c0_g1_i9 2874 2628.009 324.583185 9189.553 TRINITY_DN8_c0_g1_i4 2155 1909.009 148.931103 3062.915 TRINITY_DN8_c0_g1_i5 2107 1861.009 63.996918 1283.066 TRINITY_DN8_c0_g1_i2 1083 837.009 673.591852 6073.912 TRINITY_DN8_c0_g1_i3 986 740.278 12.936307 103.168 TRINITY_DN8_c0_g2_i1 961 715.285 1.816801 14.000 TRINITY_DN8_c1_g1_i8 5552 5306.009 0.334470 19.119 TRINITY_DN8_c1_g1_i1 4350 4104.009 6.898364 304.997 TRINITY_DN8_c1_g1_i6 1785 1539.009 1.348775 22.363 TRINITY_DN8_c1_g1_i9 5729 5483.009 0.000000 0.000 TRINITY_DN8_c1_g1_i3 1920 1674.009 7.257498 130.884 TRINITY_DN8_c1_g1_i4 4375 4129.009 0.211485 9.407 TRINITY_DN8_c1_g1_i7 4198 3952.009 29.853242 1271.016 TRINITY_DN8_c1_g1_i5 5704 5458.009 1.330174 78.214 TRINITY_DN8_c6_g1_i2 2373 2127.009 1.172117 26.859 TRINITY_DN8_c6_g1_i7 2371 2125.009 0.000000 0.000 TRINITY_DN8_c6_g1_i4 1995 1749.009 0.236377 4.454 TRINITY_DN8_c6_g1_i1 2369 2123.009 0.292400 6.688 TRINITY_DN8_c12_g1_i1 391 151.312 0.613457 1.000 A_xenica_FW_2.quant Name Length EffectiveLength TPM NumReads TRINITY_DN8_c0_g1_i7 1034 774.910 26.168216 92.852 TRINITY_DN8_c0_g1_i6 1035 775.910 368.774479 1310.196 TRINITY_DN8_c0_g1_i1 3195 2935.910 65.835643 885.050 TRINITY_DN8_c0_g1_i9 2874 2614.910 344.014191 4119.052 TRINITY_DN8_c0_g1_i4 2155 1895.910 96.229381 835.391 TRINITY_DN8_c0_g1_i5 2107 1847.910 46.501945 393.474 TRINITY_DN8_c0_g1_i2 1083 823.910 763.951306 2882.103 TRINITY_DN8_c0_g1_i3 986 727.293 18.882308 62.882 TRINITY_DN8_c0_g2_i1 961 702.316 0.932878 3.000 TRINITY_DN8_c1_g1_i8 5552 5292.910 1.212649 29.390 TRINITY_DN8_c1_g1_i1 4350 4090.910 1.688974 31.638 TRINITY_DN8_c1_g1_i6 1785 1525.910 0.000000 0.000 TRINITY_DN8_c1_g1_i9 5729 5469.910 0.000000 0.000 TRINITY_DN8_c1_g1_i3 1920 1660.910 14.384900 109.400 TRINITY_DN8_c1_g1_i4 4375 4115.910 3.276124 61.743 TRINITY_DN8_c1_g1_i7 4198 3938.910 19.118940 344.829 TRINITY_DN8_c1_g1_i5 5704 5444.910 0.000000 0.000 TRINITY_DN8_c6_g1_i2 2373 2113.910 0.000000 0.000 TRINITY_DN8_c6_g1_i7 2371 2111.910 0.000000 0.000 TRINITY_DN8_c6_g1_i4 1995 1735.910 1.028673 8.177 TRINITY_DN8_c6_g1_i1 2369 2109.910 0.602774 5.823 TRINITY_DN8_c12_g1_i1 391 145.374 0.000000 0.000 A_xenica_FW_3.quant Name Length EffectiveLength TPM NumReads TRINITY_DN8_c0_g1_i7 1034 792.418 22.799205 135.849 TRINITY_DN8_c0_g1_i6 1035 793.418 367.594140 2193.075 TRINITY_DN8_c0_g1_i1 3195 2953.418 174.862317 3883.328 TRINITY_DN8_c0_g1_i9 2874 2632.418 237.778604 4706.635 TRINITY_DN8_c0_g1_i4 2155 1913.418 122.259545 1759.039 TRINITY_DN8_c0_g1_i5 2107 1865.418 54.748614 767.949 TRINITY_DN8_c0_g1_i2 1083 841.418 639.029431 4043.110 TRINITY_DN8_c0_g1_i3 986 744.682 1.430236 8.009 TRINITY_DN8_c0_g2_i1 961 719.700 2.032631 11.000 TRINITY_DN8_c1_g1_i8 5552 5310.418 1.565068 62.495 TRINITY_DN8_c1_g1_i1 4350 4108.418 5.263584 162.607 TRINITY_DN8_c1_g1_i6 1785 1543.418 5.029330 58.368 TRINITY_DN8_c1_g1_i9 5729 5487.418 0.000000 0.000 TRINITY_DN8_c1_g1_i3 1920 1678.418 10.760137 135.800 TRINITY_DN8_c1_g1_i4 4375 4133.418 3.783087 117.581 TRINITY_DN8_c1_g1_i7 4198 3956.418 22.526090 670.148 TRINITY_DN8_c1_g1_i5 5704 5462.418 0.000000 0.000 TRINITY_DN8_c6_g1_i2 2373 2131.418 0.000000 0.000 TRINITY_DN8_c6_g1_i7 2371 2129.418 0.000000 0.000 TRINITY_DN8_c6_g1_i4 1995 1753.418 1.050799 13.854 TRINITY_DN8_c6_g1_i1 2369 2127.418 0.509199 8.146 TRINITY_DN8_c12_g1_i1 391 156.028 1.704682 2.000 A_xenica_transfer_1.quant Name Length EffectiveLength TPM NumReads TRINITY_DN8_c0_g1_i7 1034 761.427 42.045850 183.616 TRINITY_DN8_c0_g1_i6 1035 762.427 421.240379 1841.993 TRINITY_DN8_c0_g1_i1 3195 2922.427 82.092914 1375.969 TRINITY_DN8_c0_g1_i9 2874 2601.427 431.688220 6440.822 TRINITY_DN8_c0_g1_i4 2155 1882.427 119.424714 1289.353 TRINITY_DN8_c0_g1_i5 2107 1834.427 49.064477 516.211 TRINITY_DN8_c0_g1_i2 1083 810.427 626.969454 2914.204 TRINITY_DN8_c0_g1_i3 986 713.872 24.696449 101.115 TRINITY_DN8_c0_g2_i1 961 688.889 1.012398 4.000 TRINITY_DN8_c1_g1_i8 5552 5279.427 1.365849 41.357 TRINITY_DN8_c1_g1_i1 4350 4077.427 19.437530 454.555 TRINITY_DN8_c1_g1_i6 1785 1512.427 0.000000 0.000 TRINITY_DN8_c1_g1_i9 5729 5456.427 0.000000 0.000 TRINITY_DN8_c1_g1_i3 1920 1647.427 32.650452 308.500 TRINITY_DN8_c1_g1_i4 4375 4102.427 0.000000 0.000 TRINITY_DN8_c1_g1_i7 4198 3925.427 37.025857 833.588 TRINITY_DN8_c1_g1_i5 5704 5431.427 0.000000 0.000 TRINITY_DN8_c6_g1_i2 2373 2100.427 0.000000 0.000 TRINITY_DN8_c6_g1_i7 2371 2098.427 0.000000 0.000 TRINITY_DN8_c6_g1_i4 1995 1722.427 1.187644 11.732 TRINITY_DN8_c6_g1_i1 2369 2096.427 2.683661 32.268 TRINITY_DN8_c12_g1_i1 391 135.535 0.000000 0.000 A_xenica_transfer_2.quant Name Length EffectiveLength TPM NumReads TRINITY_DN8_c0_g1_i7 1034 771.393 39.898805 341.813 TRINITY_DN8_c0_g1_i6 1035 772.393 1003.467606 8607.854 TRINITY_DN8_c0_g1_i1 3195 2932.393 230.111051 7493.986 TRINITY_DN8_c0_g1_i9 2874 2611.393 256.161817 7429.162 TRINITY_DN8_c0_g1_i4 2155 1892.393 56.405708 1185.462 TRINITY_DN8_c0_g1_i5 2107 1844.393 43.565650 892.382 TRINITY_DN8_c0_g1_i2 1083 820.393 1091.461340 9944.513 TRINITY_DN8_c0_g1_i3 986 723.703 23.367980 187.817 TRINITY_DN8_c0_g2_i1 961 698.716 3.092980 24.001 TRINITY_DN8_c1_g1_i8 5552 5289.393 1.448678 85.100 TRINITY_DN8_c1_g1_i1 4350 4087.393 5.006828 227.281 TRINITY_DN8_c1_g1_i6 1785 1522.393 3.087889 52.209 TRINITY_DN8_c1_g1_i9 5729 5466.393 0.000000 0.000 TRINITY_DN8_c1_g1_i3 1920 1657.393 9.252460 170.308 TRINITY_DN8_c1_g1_i4 4375 4112.393 4.877161 222.749 TRINITY_DN8_c1_g1_i7 4198 3935.393 32.406373 1416.354 TRINITY_DN8_c1_g1_i5 5704 5441.393 0.000000 0.000 TRINITY_DN8_c6_g1_i2 2373 2110.393 0.261327 6.125 TRINITY_DN8_c6_g1_i7 2371 2108.393 0.000000 0.000 TRINITY_DN8_c6_g1_i4 1995 1732.393 0.159295 3.065 TRINITY_DN8_c6_g1_i1 2369 2106.393 2.257490 52.810 TRINITY_DN8_c12_g1_i1 391 138.140 1.303641 2.000 A_xenica_transfer_3.quant Name Length EffectiveLength TPM NumReads TRINITY_DN8_c0_g1_i7 1034 782.654 60.180869 339.372 TRINITY_DN8_c0_g1_i6 1035 783.654 448.120287 2530.270 TRINITY_DN8_c0_g1_i1 3195 2943.654 154.230800 3271.191 TRINITY_DN8_c0_g1_i9 2874 2622.654 303.041947 5726.533 TRINITY_DN8_c0_g1_i4 2155 1903.654 109.596180 1503.251 TRINITY_DN8_c0_g1_i5 2107 1855.654 43.912369 587.127 TRINITY_DN8_c0_g1_i2 1083 831.654 780.203110 4675.179 TRINITY_DN8_c0_g1_i3 986 734.924 35.517974 188.078 TRINITY_DN8_c0_g2_i1 961 709.935 2.736916 14.000 TRINITY_DN8_c1_g1_i8 5552 5300.654 0.000000 0.000 TRINITY_DN8_c1_g1_i1 4350 4098.654 6.297119 185.965 TRINITY_DN8_c1_g1_i6 1785 1533.654 6.600886 72.942 TRINITY_DN8_c1_g1_i9 5729 5477.654 1.543412 60.915 TRINITY_DN8_c1_g1_i3 1920 1668.654 33.524846 403.071 TRINITY_DN8_c1_g1_i4 4375 4123.654 7.936309 235.803 TRINITY_DN8_c1_g1_i7 4198 3946.654 50.227757 1428.305 TRINITY_DN8_c1_g1_i5 5704 5452.654 0.000000 0.000 TRINITY_DN8_c6_g1_i2 2373 2121.654 0.000000 0.000 TRINITY_DN8_c6_g1_i7 2371 2119.654 0.000000 0.000 TRINITY_DN8_c6_g1_i4 1995 1743.654 1.010916 12.701 TRINITY_DN8_c6_g1_i1 2369 2117.654 1.658089 25.299 TRINITY_DN8_c12_g1_i1 391 146.630 0.000000 0.000 Each of these contigs in theory represent a transcript. What are they? Annotations from the dammit pipeline I have annotated each transcriptome using Camille Scott's dammit (version 1.1) pipeline using three custom amino acid databases from Ensembl , EviGene , and NCBI annotations of the Fundulus heteroclitus genome. Several steps were taken to choose which annotation to assign to each contig. Make .csv file: connect Trinity transcript ID to dammit transcript ID Append .csv file connecting to transcript annotations from each database (NCBI, Ensembl, EviGene) . Assumes one transcript makes one protein. Picks the annotation from each database with the highest length and drops all other duplicates from the same transcript/database. Ends up with three columns of annotations for each transcript, one from each database. Connect annotation table to OGs The resulting file A_xenica.annotations.OGs.merged.csv contains all merged annotations for this one species, Adinia xenica . The transcript, TRINITY_DN8_c0_g1_i7 is also known as Transcript_0 (the dammit pipeline renames the transcripts) and matches with the annotations Funhe2EKm033401t1 and XP_012730079.1 , which is a cold-inducible RNA-binding protein isoform X2 from the Fundulus heteroclitus genome . Transcripts 17-21 have no annotation from any of the databases provided to the dammit pipeline. Were they quantified by salmon? Not really. See last 5 contigs in each list above. They are consistently lower compared to quant of other contigs, which supposedly are the same thing. This indicates a potential assembly mishap with these contigs. There is no OG for this grouping. Why not? Now, let's compare the 3 different annotation types that we have available: NCBI, EviGene, and Ensembl. Therea are five NCBI IDs that match to sequences in this transcript block: NCBI start_x end_x length_x ref | XP_012730079 .1 | cold - inducible RNA - binding protein isoform X2 [ Fundulus heteroclitus ] 82 258 176 ref | XP_012730080 .1 | cold - inducible RNA - binding protein isoform X3 [ Fundulus heteroclitus ] 82 244 162 ( missing Transcript_2 ) ( missing Transcript_3 ) ref | XP_012730079 .1 | cold - inducible RNA - binding protein isoform X2 [ Fundulus heteroclitus ] 82 259 177 ref | XP_012730081 .1 | cold - inducible RNA - binding protein isoform X4 [ Fundulus heteroclitus ] 82 243 161 ref | XP_012730076 .1 | cold - inducible RNA - binding protein isoform X1 [ Fundulus heteroclitus ] 82 260 178 ref | XP_012730080 .1 | cold - inducible RNA - binding protein isoform X3 [ Fundulus heteroclitus ] 82 242 160 ( missing Transcript_8 ) ref | XP_012735776 .1 | zinc finger protein aebp2 isoform X1 [ Fundulus heteroclitus ] 394 604 210 ref | XP_012735776 .1 | zinc finger protein aebp2 isoform X1 [ Fundulus heteroclitus ] 445 657 212 ref | XP_012735776 .1 | zinc finger protein aebp2 isoform X1 [ Fundulus heteroclitus ] 394 499 105 ref | XP_012735776 .1 | zinc finger protein aebp2 isoform X1 [ Fundulus heteroclitus ] 453 663 210 ref | XP_012735776 .1 | zinc finger protein aebp2 isoform X1 [ Fundulus heteroclitus ] 394 595 201 ref | XP_012735776 .1 | zinc finger protein aebp2 isoform X1 [ Fundulus heteroclitus ] 453 665 212 ref | XP_012735776 .1 | zinc finger protein aebp2 isoform X1 [ Fundulus heteroclitus ] 394 606 212 ref | XP_012735776 .1 | zinc finger protein aebp2 isoform X1 [ Fundulus heteroclitus ] 445 655 210 Ensembl IDs are slightly different: Ensembl_info start end length ENSFHEP00000001828 82 259 177 ENSFHEP00000001826 82 244 162 (missing Transcript_2) ENSFHEP00000001828 82 243 161 (missing Transcript_4) ENSFHEP00000022306 82 243 161 ENSFHEP00000001826 82 260 178 ENSFHEP00000001828 82 243 161 (missing Transcript_8) ENSFHEP00000025887 394 604 210 ENSFHEP00000025887 445 657 212 ENSFHEP00000025887 394 499 105 ENSFHEP00000025887 453 663 210 ENSFHEP00000025887 394 595 201 ENSFHEP00000025887 453 665 212 ENSFHEP00000025887 394 606 212 ENSFHEP00000025887 445 655 210 and EviGene simliar to Ensembl: EviGene_transcript start_y end_y length_y Funhe2EKm033401t1 82 258 176 Funhe2EKm033401t1 82 244 162 (missing Transcript_2) Funhe2EKm033401t1 82 242 160 (missing Transcript_4) Funhe2EKm033401t1 82 242 160 Funhe2EKm033401t1 82 260 178 Funhe2EKm033401t1 82 242 160 (missing Transcript_8) Funhe2EKm038372t1 394 604 210 Funhe2EKm038372t1 445 657 212 Funhe2EKm038372t2 394 499 105 Funhe2EKm038372t1 453 663 210 Funhe2EKm038372t1 394 595 201 Funhe2EKm038372t1 453 665 212 Funhe2EKm038372t2 394 606 212 Funhe2EKm038372t1 445 655 210 Collapsing with tximport The Bioconductor package, tximport by Soneson et al. 2015 (awesome paper showing that gene-level analysis is more accurate than transcript-level) collapses/summarizes the counts for the same gene ID together. From Soneson et al. 2015 : \"...isoform quantification is more complex than the simple counting, due to the high degree of overlap among transcripts. Currently, there is no consensus regarding the optimal resolution or method for quantification and downstream analysis of transcriptomic output.\" The steps for tximport: Make separate tx2gene files for Ensembl, Evigene, NCBI Run tximport 3 times (1 for each database) to collapse transcript counts by unique protein annotation ID Summarize counts tables across species, one table for each: Evigene , NCBI , Ensembl A file called 'tx2gene` is required for tximport, which consists of two columns: tx2gene_ncbi <- read.csv ( \"A_xenica_ncbi.tx2gene.csv\" ) > colnames ( tx2gene_ncbi ) <- c ( \"Name\" , \"NCBI\" ) > head ( tx2gene_ncbi ) Name NCBI 1 TRINITY_DN8_c0_g1_i7 XP_012730079.1 2 TRINITY_DN8_c0_g1_i6 XP_012730080.1 3 TRINITY_DN8_c1_g1_i1 XP_012735776.1 4 TRINITY_DN14_c1_g1_i8 XP_021179903.1 5 TRINITY_DN7641_c0_g1_i5 XP_012714310.2 6 TRINITY_DN7606_c0_g2_i4 XP_012735011.1 > dim ( tx2gene_ncbi ) [ 1 ] 70774 2 The tx2gene file contains the information for which transcripts/contigs are to be collapsed by unique gene ID. The tximport function is run with the counts file, and summarized. 70,774 annotations get reduced to 21,650 genes. Note that the majority of the fragments (230,890) from the transcriptome are not annotated. > txi_ncbi.salmon <- tximport ( files , type = \"salmon\" , tx2gene = tx2gene_ncbi ) reading in files with read_tsv 1 2 3 4 5 6 7 8 9 transcripts missing from tx2gene : 230890 summarizing abundance summarizing counts summarizing length > print ( dim ( txi_ncbi.salmon $ counts )) [ 1 ] 21650 9 write.csv ( txi_ncbi.salmon $ counts , \"A_xenica_counts.ncbi.csv\" ) Output after tximport : ENSEMBL ENSFHEP00000001828 6991.557 11733.51 14262.502 9701.572 4274.786 4851.493 6727.553 7969.737 6261.57 ENSFHEP00000001826 13174.424 9559.255 13168.948 8733.483 4192.299 6236.185 4756.197 18552.367 7205.449 ENSFHEP00000022306 1596.367 761.523 1184.857 1283.066 393.474 767.949 516.211 892.382 587.127 ENSFHEP00000025887 4019 2385.999 3483.999 2027 669 1378.999 1836 2428.001 2801.001 NCBI: XP_012730079.1 4277.751 2712.994 3543.154 3477.77 937.243 1899.893 1479.687 1532.331 1847.036 XP_012730080.1 6390.756 3251.141 4992.686 2762.739 1373.078 2201.084 1944.108 8803.616 2723.935 XP_012730081.1 1596.367 761.523 1184.857 1283.066 393.474 767.949 516.211 892.382 587.127 XP_012730076.1 7578.476 6542.645 8633.753 6080.608 2888.103 4047.111 2917.204 9949.523 4679.179 XP_012735776.1 4019 2385.999 3483.999 2027 669 1378.999 1836 2428.001 2801.001 EviGene (collapses at gene level) Funhe2EKm033401 21786.876 22063.317 28626.56 19730.832 8872.15 11863.633 12008.679 27422.497 14058.146 Funhe2EKm038372 3934 2331.999 3389.999 1992 630 1326.999 1778 2366.001 2674.001 Which one is correct? Example 2: Which transcripts have OGs? From this block of Trinity contigs, there are two OGs: TRINITY_DN18_c0_g1_i16 Transcript_22 TRINITY_DN18_c0_g1_i6 Transcript_23 TRINITY_DN18_c0_g1_i17 Transcript_24 TRINITY_DN18_c0_g1_i4 Transcript_25 TRINITY_DN18_c0_g1_i8 Transcript_26 TRINITY_DN18_c0_g1_i21 Transcript_27 TRINITY_DN18_c0_g1_i9 Transcript_28 TRINITY_DN18_c0_g1_i3 Transcript_29 TRINITY_DN18_c0_g1_i11 Transcript_30 TRINITY_DN18_c0_g1_i15 Transcript_31 TRINITY_DN18_c0_g1_i7 Transcript_32 OG0001688 TRINITY_DN18_c0_g1_i10 Transcript_33 TRINITY_DN18_c0_g1_i5 Transcript_34 OG0006300 TRINITY_DN18_c0_g1_i14 Transcript_35 TRINITY_DN18_c0_g2_i1 Transcript_36 TRINITY_DN18_c1_g1_i1 Transcript_37 Why only these two contigs? OG0001688 TRINITY_DN18_c0_g1_i10 Transcript_33 OG0006300 TRINITY_DN18_c0_g1_i14 Transcript_35 The other contigs in the block have the same annotations. Ensembl: Ensembl_info start end length ENSFHEP00000034241.1 149 705 556 ENSFHEP00000034241.1 180 283 103 ENSFHEP00000034241.1 149 299 150 ENSFHEP00000034241.1 149 705 556 ENSFHEP00000034241.1 149 705 556 ENSFHEP00000034241.1 149 705 556 ENSFHEP00000034241.1 149 705 556 ENSFHEP00000034241.1 149 239 90 ENSFHEP00000034241.1 149 705 556 ENSFHEP00000034241.1 149 705 556 ENSFHEP00000034241.1 149 705 556 (missing Transcript_33) ENSFHEP00000034241.1 149 705 556 ENSFHEP00000034241.1 0 208 208 (missing Transcript_36) (missing Transcript_37) NCBI: NCBI start_x end_x length_x XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 115 1937 1822 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 180 283 103 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 115 299 184 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 115 1170 1055 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 115 1440 1325 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 115 1834 1719 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 115 1472 1357 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 115 239 124 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 115 1622 1507 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 115 1418 1303 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 115 1418 1303 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 182 1225 1043 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 115 1709 1594 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 0 216 216 ( missing Transcript_36 ) ( missing Transcript_37 ) EviGene: EviGene_transcript start_y end_y length_y Funhe2EKm036495t2 249 1937 1688 Funhe2EKm036495t2 180 283 103 Funhe2EKm036495t1 115 299 184 Funhe2EKm036495t1 115 1170 1055 Funhe2EKm036495t4 115 1514 1399 Funhe2EKm036495t2 249 1834 1585 Funhe2EKm036495t4 115 1457 1342 Funhe2EKm036495t1 115 239 124 Funhe2EKm036495t2 249 1622 1373 Funhe2EKm036495t1 115 1418 1303 Funhe2EKm036495t1 115 1418 1303 Funhe2EKm036495t2 182 1225 1043 Funhe2EKm036495t2 249 1709 1460 Funhe2EKm036495t7 0 254 254 (missing Transcript_36) (missing Transcript_37) Here are the quantifications from salmon: A_xenica_BW_1 Name Length EffectiveLength TPM NumReads TRINITY_DN18_c0_g1_i16 6454 6233.841 30.846165 2599.639 TRINITY_DN18_c0_g1_i6 849 629.036 1.393892 11.854 TRINITY_DN18_c0_g1_i17 1091 870.841 0.578251 6.808 TRINITY_DN18_c0_g1_i4 4712 4491.841 3.797259 230.595 TRINITY_DN18_c0_g1_i8 6533 6312.841 0.000000 0.000 TRINITY_DN18_c0_g1_i21 7883 7662.841 0.256581 26.581 TRINITY_DN18_c0_g1_i9 6587 6366.841 0.256579 22.085 TRINITY_DN18_c0_g1_i3 884 664.026 0.327397 2.939 TRINITY_DN18_c0_g1_i11 5220 4999.841 0.501775 33.917 TRINITY_DN18_c0_g1_i15 6566 6345.841 0.333891 28.645 TRINITY_DN18_c0_g1_i7 6645 6424.841 0.000000 0.000 TRINITY_DN18_c0_g1_i10 3678 3457.841 0.219124 10.244 TRINITY_DN18_c0_g1_i5 5585 5364.841 0.064707 4.693 TRINITY_DN18_c0_g1_i14 765 545.061 0.000000 0.000 TRINITY_DN18_c0_g2_i1 871 651.029 0.568085 5.000 TRINITY_DN18_c1_g1_i1 364 147.523 1.002797 2.000 A_xenica_BW_2 Name Length EffectiveLength TPM NumReads TRINITY_DN18_c0_g1_i16 6454 6204.489 22.527351 1382.342 TRINITY_DN18_c0_g1_i6 849 599.790 0.216304 1.283 TRINITY_DN18_c0_g1_i17 1091 841.489 1.008484 8.393 TRINITY_DN18_c0_g1_i4 4712 4462.489 1.861701 82.165 TRINITY_DN18_c0_g1_i8 6533 6283.489 0.000000 0.000 TRINITY_DN18_c0_g1_i21 7883 7633.489 0.537603 40.587 TRINITY_DN18_c0_g1_i9 6587 6337.489 0.000000 0.000 TRINITY_DN18_c0_g1_i3 884 634.776 0.000000 0.000 TRINITY_DN18_c0_g1_i11 5220 4970.489 0.364668 17.927 TRINITY_DN18_c0_g1_i15 6566 6316.489 0.536235 33.499 TRINITY_DN18_c0_g1_i7 6645 6395.489 0.510573 32.295 TRINITY_DN18_c0_g1_i10 3678 3428.489 0.606792 20.575 TRINITY_DN18_c0_g1_i5 5585 5335.489 0.207233 10.935 TRINITY_DN18_c0_g1_i14 765 515.845 0.000000 0.000 TRINITY_DN18_c0_g2_i1 871 621.778 0.487850 3.000 TRINITY_DN18_c1_g1_i1 364 125.722 0.000000 0.000 A_xenica_BW_3 Name Length EffectiveLength TPM NumReads TRINITY_DN18_c0_g1_i16 6454 6226.011 24.577486 2051.870 TRINITY_DN18_c0_g1_i6 849 621.219 0.514900 4.289 TRINITY_DN18_c0_g1_i17 1091 863.011 0.000000 0.000 TRINITY_DN18_c0_g1_i4 4712 4484.011 1.503027 90.372 TRINITY_DN18_c0_g1_i8 6533 6305.011 0.000000 0.000 TRINITY_DN18_c0_g1_i21 7883 7655.011 0.604365 62.037 TRINITY_DN18_c0_g1_i9 6587 6359.011 0.100474 8.567 TRINITY_DN18_c0_g1_i3 884 656.210 0.191564 1.686 TRINITY_DN18_c0_g1_i11 5220 4992.011 0.496279 33.220 TRINITY_DN18_c0_g1_i15 6566 6338.011 0.606954 51.584 TRINITY_DN18_c0_g1_i7 6645 6417.011 0.394091 33.910 TRINITY_DN18_c0_g1_i10 3678 3450.011 0.445096 20.591 TRINITY_DN18_c0_g1_i5 5585 5357.011 0.457638 32.874 TRINITY_DN18_c0_g1_i14 765 537.258 0.000000 0.000 TRINITY_DN18_c0_g2_i1 871 643.215 0.463769 4.000 TRINITY_DN18_c1_g1_i1 364 140.974 0.000000 0.000 A_xenica_FW_1 Name Length EffectiveLength TPM NumReads TRINITY_DN18_c0_g1_i16 6454 6208.009 28.137333 1881.814 TRINITY_DN18_c0_g1_i6 849 603.321 0.805496 5.235 TRINITY_DN18_c0_g1_i17 1091 845.009 0.401774 3.658 TRINITY_DN18_c0_g1_i4 4712 4466.009 0.113310 5.452 TRINITY_DN18_c0_g1_i8 6533 6287.009 0.588066 39.830 TRINITY_DN18_c0_g1_i21 7883 7637.009 0.466627 38.391 TRINITY_DN18_c0_g1_i9 6587 6341.009 0.000000 0.000 TRINITY_DN18_c0_g1_i3 884 638.311 0.000000 0.000 TRINITY_DN18_c0_g1_i11 5220 4974.009 0.312379 16.739 TRINITY_DN18_c0_g1_i15 6566 6320.009 0.447333 30.457 TRINITY_DN18_c0_g1_i7 6645 6399.009 0.000000 0.000 TRINITY_DN18_c0_g1_i10 3678 3432.009 0.838865 31.016 TRINITY_DN18_c0_g1_i5 5585 5339.009 0.076627 4.407 TRINITY_DN18_c0_g1_i14 765 519.364 0.000000 0.000 TRINITY_DN18_c0_g2_i1 871 625.314 0.148443 1.000 TRINITY_DN18_c1_g1_i1 364 128.239 0.000000 0.000 A_xenica_FW_2 Name Length EffectiveLength TPM NumReads TRINITY_DN18_c0_g1_i16 6454 6194.910 30.603870 868.112 TRINITY_DN18_c0_g1_i6 849 590.386 0.000000 0.000 TRINITY_DN18_c0_g1_i17 1091 831.910 0.000000 0.000 TRINITY_DN18_c0_g1_i4 4712 4452.910 1.350102 27.528 TRINITY_DN18_c0_g1_i8 6533 6273.910 0.404081 11.608 TRINITY_DN18_c0_g1_i21 7883 7623.910 0.267190 9.327 TRINITY_DN18_c0_g1_i9 6587 6327.910 0.000000 0.000 TRINITY_DN18_c0_g1_i3 884 625.368 0.000000 0.000 TRINITY_DN18_c0_g1_i11 5220 4960.910 0.360567 8.191 TRINITY_DN18_c0_g1_i15 6566 6306.910 0.000000 0.000 TRINITY_DN18_c0_g1_i7 6645 6385.910 0.000000 0.000 TRINITY_DN18_c0_g1_i10 3678 3418.910 0.589855 9.234 TRINITY_DN18_c0_g1_i5 5585 5325.910 0.000000 0.000 TRINITY_DN18_c0_g1_i14 765 506.466 0.000000 0.000 TRINITY_DN18_c0_g2_i1 871 612.375 0.000000 0.000 TRINITY_DN18_c1_g1_i1 364 124.670 0.000000 0.000 A_xenica_FW_3 Name Length EffectiveLength TPM NumReads TRINITY_DN18_c0_g1_i16 6454 6212.418 28.368698 1325.205 TRINITY_DN18_c0_g1_i6 849 607.760 0.000000 0.000 TRINITY_DN18_c0_g1_i17 1091 849.418 0.000000 0.000 TRINITY_DN18_c0_g1_i4 4712 4470.418 2.193529 73.735 TRINITY_DN18_c0_g1_i8 6533 6291.418 0.000000 0.000 TRINITY_DN18_c0_g1_i21 7883 7641.418 0.525952 30.221 TRINITY_DN18_c0_g1_i9 6587 6345.418 0.000000 0.000 TRINITY_DN18_c0_g1_i3 884 642.740 1.555589 7.518 TRINITY_DN18_c0_g1_i11 5220 4978.418 0.596278 22.321 TRINITY_DN18_c0_g1_i15 6566 6324.418 0.000000 0.000 TRINITY_DN18_c0_g1_i7 6645 6403.418 0.000000 0.000 TRINITY_DN18_c0_g1_i10 3678 3436.418 0.000000 0.000 TRINITY_DN18_c0_g1_i5 5585 5343.418 0.000000 0.000 TRINITY_DN18_c0_g1_i14 765 523.809 0.000000 0.000 TRINITY_DN18_c0_g2_i1 871 629.746 0.000000 0.000 TRINITY_DN18_c1_g1_i1 364 132.991 0.999992 1.000 A_xenica_transfer_1 Name Length EffectiveLength TPM NumReads TRINITY_DN18_c0_g1_i16 6454 6181.427 31.122892 1103.388 TRINITY_DN18_c0_g1_i6 849 576.983 1.439049 4.762 TRINITY_DN18_c0_g1_i17 1091 818.427 0.000000 0.000 TRINITY_DN18_c0_g1_i4 4712 4439.427 3.659486 93.176 TRINITY_DN18_c0_g1_i8 6533 6260.427 0.000000 0.000 TRINITY_DN18_c0_g1_i21 7883 7610.427 0.463348 20.224 TRINITY_DN18_c0_g1_i9 6587 6314.427 0.000000 0.000 TRINITY_DN18_c0_g1_i3 884 611.950 3.059572 10.738 TRINITY_DN18_c0_g1_i11 5220 4947.427 0.730161 20.718 TRINITY_DN18_c0_g1_i15 6566 6293.427 0.000000 0.000 TRINITY_DN18_c0_g1_i7 6645 6372.427 0.208309 7.613 TRINITY_DN18_c0_g1_i10 3678 3405.427 1.070710 20.912 TRINITY_DN18_c0_g1_i5 5585 5312.427 0.179431 5.467 TRINITY_DN18_c0_g1_i14 765 493.125 0.000000 0.000 TRINITY_DN18_c0_g2_i1 871 598.958 0.582203 2.000 TRINITY_DN18_c1_g1_i1 364 114.839 1.518283 1.000 A_xenica_transfer_2 Name Length EffectiveLength TPM NumReads TRINITY_DN18_c0_g1_i16 6454 6191.393 22.196557 1526.255 TRINITY_DN18_c0_g1_i6 849 586.777 0.601647 3.921 TRINITY_DN18_c0_g1_i17 1091 828.393 0.173338 1.595 TRINITY_DN18_c0_g1_i4 4712 4449.393 1.696002 83.807 TRINITY_DN18_c0_g1_i8 6533 6270.393 0.287994 20.055 TRINITY_DN18_c0_g1_i21 7883 7620.393 0.326335 27.618 TRINITY_DN18_c0_g1_i9 6587 6324.393 0.327247 22.985 TRINITY_DN18_c0_g1_i3 884 621.753 0.228701 1.579 TRINITY_DN18_c0_g1_i11 5220 4957.393 0.340199 18.730 TRINITY_DN18_c0_g1_i15 6566 6303.393 0.000000 0.000 TRINITY_DN18_c0_g1_i7 6645 6382.393 0.000000 0.000 TRINITY_DN18_c0_g1_i10 3678 3415.393 0.257022 9.749 TRINITY_DN18_c0_g1_i5 5585 5322.393 0.417951 24.705 TRINITY_DN18_c0_g1_i14 765 502.823 0.000000 0.000 TRINITY_DN18_c0_g2_i1 871 608.760 0.000000 0.000 TRINITY_DN18_c1_g1_i1 364 116.576 0.772392 1.000 A_xenica_transfer_3 Name Length EffectiveLength TPM NumReads TRINITY_DN18_c0_g1_i16 6454 6202.654 30.796098 1376.326 TRINITY_DN18_c0_g1_i6 849 598.002 0.000000 0.000 TRINITY_DN18_c0_g1_i17 1091 839.654 0.477857 2.891 TRINITY_DN18_c0_g1_i4 4712 4460.654 0.216472 6.957 TRINITY_DN18_c0_g1_i8 6533 6281.654 0.000000 0.000 TRINITY_DN18_c0_g1_i21 7883 7631.654 0.848633 46.665 TRINITY_DN18_c0_g1_i9 6587 6335.654 0.287240 13.112 TRINITY_DN18_c0_g1_i3 884 632.982 0.812986 3.708 TRINITY_DN18_c0_g1_i11 5220 4968.654 0.316341 11.325 TRINITY_DN18_c0_g1_i15 6566 6314.654 0.000000 0.000 TRINITY_DN18_c0_g1_i7 6645 6393.654 0.000000 0.000 TRINITY_DN18_c0_g1_i10 3678 3426.654 0.243650 6.016 TRINITY_DN18_c0_g1_i5 5585 5333.654 0.000000 0.000 TRINITY_DN18_c0_g1_i14 765 514.064 0.000000 0.000 TRINITY_DN18_c0_g2_i1 871 619.987 0.223856 1.000 TRINITY_DN18_c1_g1_i1 364 123.766 0.000000 0.000 After tximport , they all get collapsed. Ensembl ENSFHEP00000034241.1 2968.756 1609.426 2370.409 2025.983 924.766 1459 1266.086 1731.25 1461.984 NCBI: XP_012733577.2 2979 1630.001 2391 2056.999 934 1459 1286.998 1740.999 1468 EviGene: Funhe2EKm036495 2979 1630.001 2391 2056.999 934 1459 1286.998 1740.999 1468 But, if we were to have quantified by OG we would get this: OG0001688 10.244 20.575 20.591 31.016 9.234 0 20.912 9.749 6.016 OG0006300 0 0 0 0 0 0 0 0 0 In Fundulus heteroclitus (MDPL population), OG0001688 corresponds to these contigs: OG0001688 TRINITY_DN5678_c0_g1_i10 Transcript_90605 OG0006300 TRINITY_DN5678_c0_g1_i1 Transcript_90601 This is the block of transcripts identified by Trinity: TRINITY_DN5678_c0_g1_i1 Transcript_90601 TRINITY_DN5678_c0_g1_i4 Transcript_90602 TRINITY_DN5678_c0_g1_i5 Transcript_90603 TRINITY_DN5678_c0_g1_i3 Transcript_90604 TRINITY_DN5678_c0_g1_i10 Transcript_90605 TRINITY_DN5678_c0_g1_i8 Transcript_90606 TRINITY_DN5678_c0_g1_i9 Transcript_90607 TRINITY_DN5678_c0_g1_i6 Transcript_90608 TRINITY_DN5678_c0_g1_i7 Transcript_90609 TRINITY_DN5678_c0_g1_i2 Transcript_90610 TRINITY_DN5678_c1_g1_i5 Transcript_90611 TRINITY_DN5678_c1_g1_i4 Transcript_90612 TRINITY_DN5678_c1_g1_i6 Transcript_90613 TRINITY_DN5678_c1_g1_i1 Transcript_90614 TRINITY_DN5678_c1_g1_i7 Transcript_90615 Evigene EviGene_transcript start_y end_y length_y Funhe2EKm036495t7 0 332 332 Funhe2EKm036495t1 0 182 182 Funhe2EKm036495t2 0 173 173 Funhe2EKm036495t2 825 1570 745 Funhe2EKm036495t11 97 1172 1075 Funhe2EKm036495t9 440 496 56 Funhe2EKm036495t2 0 173 173 Funhe2EKm036495t2 0 1372 1372 Funhe2EKm036495t2 0 1372 1372 Funhe2EKm036495t2 231 1916 1685 (missing Transcript_90611) (missing Transcript_90612) (missing Transcript_90613) Funhe2EKm036495t5 0 29 29 (missing Transcript_90615) NCBI: NCBI start_x end_x length_x ( missing Transcript_90601 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 0 182 182 ( missing Transcript_90603 ) XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 0 630 630 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 97 1172 1075 ( missing Transcript_90606 ) ( missing Transcript_90607 ) XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 0 1372 1372 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 0 1372 1372 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 97 1916 1819 ( missing Transcript_90611 ) ( missing Transcript_90612 ) ( missing Transcript_90613 ) ( missing Transcript_90614 ) ( missing Transcript_90615 ) Ensembl: Ensembl_info start end length (missing Transcript_90601) (missing Transcript_90602) (missing Transcript_90603) (missing Transcript_90604) ENSFHEP00000034241.1 131 687 556 (missing Transcript_90606) (missing Transcript_90607) (missing Transcript_90608) (missing Transcript_90609) ENSFHEP00000034241.1 131 687 556 (missing Transcript_90611) (missing Transcript_90612) (missing Transcript_90613) (missing Transcript_90614) (missing Transcript_90615) In Fundulus parvipinnis (the California killifish!), OG0001688 corresponds to a block of Trinity contigs that actually includes another OG OG0013423 , unlike the previous 2 species we looked at. TRINITY_DN2419_c0_g1_i7 Transcript_38462 OG0013423 TRINITY_DN2419_c0_g1_i1 Transcript_38463 TRINITY_DN2419_c0_g1_i4 Transcript_38464 OG0001688 TRINITY_DN2419_c0_g1_i2 Transcript_38465 TRINITY_DN2419_c0_g1_i5 Transcript_38466 TRINITY_DN2419_c0_g1_i10 Transcript_38467 TRINITY_DN2419_c0_g1_i9 Transcript_38468 OG0006300 TRINITY_DN2419_c0_g1_i11 Transcript_38469 Going back to A. xenica , this additional OG0013423 is there, but annotated to a different set of contigs: OG0013423 TRINITY_DN25142_c0_g1_i1 Transcript_171197 OG0034594 TRINITY_DN25142_c0_g2_i1 Transcript_171198 TRINITY_DN25142_c0_g3_i1 Transcript_171199 TRINITY_DN25142_c0_g3_i3 Transcript_171200 TRINITY_DN25142_c0_g4_i1 Transcript_171201 In F. heteroclitus MDPL , OG0013423 corresponds to the contigs TRINITY_DN103077_c0_g1_i1 Transcript_365057, which annotate as ENSFHEP00000008638 0 128 128 ref | XP_021177365 .1 | kelch - like protein 32 [ Fundulus heteroclitus ] 0 128 128 Funhe2EKm031909t4 0 128 128 In A. xenica , OG0013423 corresponds to the contigs TRINITY_DN25142_c0_g1_i1 Transcript_171197, which annotate to: ENSFHEP00000008638 137 263 126 ref | XP_021177365 .1 | kelch - like protein 32 [ Fundulus heteroclitus ] 137 263 126 Funhe2EKm031909t4 137 263 126 The kelch-like protein 32 is. Back to F. parvipinnis , Ensembl annotations: Ensembl_info start end length (missing Transcript_38462) ENSFHEP00000008638 0 441 441 (missing Transcript_38464) ENSFHEP00000021212 129 986 857 ENSFHEP00000021212 129 986 857 ENSFHEP00000008638 33 87 54 (missing Transcript_38468) (missing Transxript_38469) NCBI: NCBI start_x end_x length_x XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 19 647 628 XP_021177365 .1 kelch - like protein 32 [ Fundulus heteroclitus ] 0 441 441 ( missing Transcript_38464 ) XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 96 1898 1802 XP_012733577 .2 extended synaptotagmin - 1 - like [ Fundulus heteroclitus ] 96 1587 1491 XP_021177365 .1 kelch - like protein 32 [ Fundulus heteroclitus ] 33 87 54 ( missing Transcript_38468 ) ( missing Transcript_38469 ) EviGene (all contigs in group annotated): EviGene_transcript start_y end_y length_y Funhe2EKm036495t2 19 647 628 Funhe2EKm031909t4 0 389 389 Funhe2EKm036495t9 573 629 56 Funhe2EKm036495t2 230 1898 1668 Funhe2EKm036495t2 230 1587 1357 Funhe2EKm031909t1 33 87 54 Funhe2EKm036495t9 19 309 290 Funhe2EKm036495t9 390 581 191 The annotations for this group of transcripts in F. parvipinnis are mixed between \"synaptotagmin 1-like\" and \"kelch-like protein 32\". The kelch-like protein 32 is what OG0013423 annotates to in A. xenica and F. heteroclitus (MDPL population). In F. parvipinnis , OG0013423 is TRINITY_DN2419_c0_g1_i1 , which according to the annotation table is XP_021177365.1 \"kelch-like protein 32 [Fundulus heteroclitus]\". Taking the sequence from this contig and searching with blastx (translated nucleotide to protein). This comes up with a very strong hit (alignment score >200) as \"extended synaptotagmin-1-like\". Which suggests that there is something funny going on with the decision-making when choosing which annotation should correspond to each contig. Rightnow I am chosing the best annotation based on the longest length of the annotation. Perhaps I should choose by E-value. (Which I have also tried, but this can lead to short annotations, so I was trying to avoid that. A combination would be nice! Titus has also suggested word clouds.) For quantification, we shouldn't be trusting isoform-level analysis. Basically, we don't have enough resolution with short-read sequencing to be confident in isoforms. Although, it would be nice. With tximport, we summarize transcript-level quantifications to the gene-level. From F. parvipinnis , here are the quantifications: OG0001688 46.069 66.116 120.001 0 109.918 78.347 129.212 185.894 OG0006300 0 0 0 0 0 0 43.543 0 vs. NCBI: XP_012733577.2 786.135 699.509 692.094 555.582 808.328 676.633 1736.456 1815.904 And then F. heteroclitus (MDPL population)*, OG0006300 130.249 0 0 0 0 0 0 0 OG0001688 3.466 0 0 3.313 3.408 4.225 10.299 0 vs. NCBI: XP_012733577.2 1680.51 1375.509 348.41 1772.139 1438.372 2272.3 2194.189 2298.48 5350.611 Example 3: More OGs, new species, new block of transcripts From the species, F. diaphanus , we see 5 OGs in a block of transcripts: OG0001948 TRINITY_DN28_c0_g1_i1 Transcript_72 TRINITY_DN28_c0_g1_i6 Transcript_73 TRINITY_DN28_c0_g1_i8 Transcript_74 TRINITY_DN28_c0_g1_i9 Transcript_75 TRINITY_DN28_c0_g1_i3 Transcript_76 TRINITY_DN28_c0_g1_i11 Transcript_77 TRINITY_DN28_c0_g1_i2 Transcript_78 TRINITY_DN28_c0_g1_i12 Transcript_79 OG0010233 TRINITY_DN28_c0_g1_i10 Transcript_80 TRINITY_DN28_c0_g1_i5 Transcript_81 OG0063265 TRINITY_DN28_c1_g1_i7 Transcript_82 TRINITY_DN28_c1_g1_i5 Transcript_83 TRINITY_DN28_c1_g1_i4 Transcript_84 OG0006676 TRINITY_DN28_c1_g1_i1 Transcript_85 TRINITY_DN28_c1_g1_i3 Transcript_86 TRINITY_DN28_c1_g1_i8 Transcript_87 OG0014822 TRINITY_DN28_c1_g1_i6 Transcript_88 TRINITY_DN28_c1_g2_i1 Transcript_89 TRINITY_DN28_c2_g1_i2 Transcript_90 Annotations: XP_012721991 .1 heterogeneous nuclear ribonucleoprotein A1 [ Fundulus heteroclitus ] 51 385 334 XP_012721990 .1 transcription factor NF - E2 45 kDa subunit [ Fundulus heteroclitus ] 100 584 484 XP_012721990 .1 transcription factor NF - E2 45 kDa subunit [ Fundulus heteroclitus ] 100 584 484 XP_012721990 .1 transcription factor NF - E2 45 kDa subunit [ Fundulus heteroclitus ] 100 584 484 XP_012721990 .1 transcription factor NF - E2 45 kDa subunit [ Fundulus heteroclitus ] 100 584 484 XP_012721990 .1 transcription factor NF - E2 45 kDa subunit [ Fundulus heteroclitus ] 100 584 484 XP_012721990 .1 transcription factor NF - E2 45 kDa subunit [ Fundulus heteroclitus ] 100 584 484 XP_012721990 .1 transcription factor NF - E2 45 kDa subunit [ Fundulus heteroclitus ] 100 584 484 XP_012730480 .1 proto - oncogene c - Rel isoform X1 [ Fundulus heteroclitus ] 36 343 307 XP_012730480 .1 proto - oncogene c - Rel isoform X1 [ Fundulus heteroclitus ] 410 864 454 XP_012730480 .1 proto - oncogene c - Rel isoform X1 [ Fundulus heteroclitus ] 399 853 454 XP_012730480 .1 proto - oncogene c - Rel isoform X1 [ Fundulus heteroclitus ] 0 453 453 XP_012730480 .1 proto - oncogene c - Rel isoform X1 [ Fundulus heteroclitus ] 13 509 496 XP_012730480 .1 proto - oncogene c - Rel isoform X1 [ Fundulus heteroclitus ] 36 667 631 Collapsing by annotation (NCBI): XP_012721991.1 2777.577 3299.293 2789.927 4046.443 5003.293 1929.718 XP_012721990.1 1688.422 2243.942 1822.958 2403.199 3567.872 1152.61 XP_012730480.1 676 714 653.055 920.826 1305.761 490.443 vs. OG OG0001948 2776.577 3295.293 2788.927 4042.443 4999.264 1928.718 OG0010233 61.159 0 0 0 370.132 41.857 OG0063265 141.881 90.625 133.878 186.441 387.625 96.307 OG0006676 45.707 50.631 46.917 57.028 61.177 71.572 OG0014822 359.112 530.675 280.426 659.912 0 196.781 Comparing to another species, F. sciadicus same anntoated transcripts (NCBI): XP_012721991.1 10.508 3.071 1.219 4.901 XP_012721990.1 12205.829 3600.654 2652.671 5755.092 XP_012730480.1 2452.693 647.976 253.854 771.763 vs OGs: OG0001948 1.458 0 1.219 2.901 OG0010233 452.224 145.99 174.95 159.422 OG0063265 (not found) OG0006676 23.076 3.142 3.247 7.264 OG0014822 2054.963 530.163 205.123 640.771 And F. grandis : XP_012721991.1 32290.661 1879.991 5055.72 13800.499 4881.964 5493.489 9913.475 4481.443 6775.423 XP_012721990.1 4515.679 248.354 650.991 1928.787 639.807 1168.021 1649.781 594.402 1059.746 XP_012730480.1 4292.514 207 729.779 1492.156 502.695 233.242 752.895 359.399 638.298 vs OGs: OG0001948 0 0 0 0 0 0 0 0 0 OG0010233 2391.307 130.033 241.47 882.285 273.383 463.887 637.584 324.89 346.008 OG0063265 (not found) OG0006676 148.906 0 0 51.066 67.894 0 21.345 0 0 OG0014822 600.002 0 361.017 1011.995 90.218 101.711 165.621 148.755 159.42 What's interesting is in the annotations above, XP_012721990 length 484 annotates to seven contigs from positions 100-584 of each contig. Is this an error in the assemblies? TRINITY_DN28_c0_g1_i8 Transcript_74 TRINITY_DN28_c0_g1_i9 Transcript_75 TRINITY_DN28_c0_g1_i3 Transcript_76 TRINITY_DN28_c0_g1_i11 Transcript_77 TRINITY_DN28_c0_g1_i2 Transcript_78 TRINITY_DN28_c0_g1_i12 Transcript_79 TRINITY_DN28_c0_g1_i10 Transcript_80 Let's take a look. TRINITY_DN28_c0_g1_i8, length 3373 TRINITY_DN28_c0_g1_i9, length 4352 TRINITY_DN28_c0_g1_i3, length 3246 TRINITY_DN28_c0_g1_i2, length 4109 I can see that these are all strongly matching to the same transcription factor. Why are they separate contigs? Let's look at how salmon quantified them. If they have overlapping k-mers, quantification should be spread across. F_diaphanus_BW_1.quant Name Length EffectiveLength TPM NumReads TRINITY_DN28_c0_g1_i8 3373 3122.308 0.000000 0.000 TRINITY_DN28_c0_g1_i9 4352 4101.308 0.000000 0.000 TRINITY_DN28_c0_g1_i3 3246 2995.308 6.292499 120.594 TRINITY_DN28_c0_g1_i11 4267 4016.308 26.524225 681.604 TRINITY_DN28_c0_g1_i2 4109 3858.308 33.421724 825.065 TRINITY_DN28_c0_g1_i12 3531 3280.308 0.000000 0.000 TRINITY_DN28_c0_g1_i10 4194 3943.308 2.424037 61.159 F_diaphanus_BW_2.quant Name Length EffectiveLength TPM NumReads TRINITY_DN28_c0_g1_i8 3373 3088.446 1.202113 29.068 TRINITY_DN28_c0_g1_i9 4352 4067.446 2.716526 86.510 TRINITY_DN28_c0_g1_i3 3246 2961.446 3.098616 71.846 TRINITY_DN28_c0_g1_i11 4267 3982.446 33.779057 1053.242 TRINITY_DN28_c0_g1_i2 4109 3824.446 33.505880 1003.276 TRINITY_DN28_c0_g1_i12 3531 3246.446 0.000000 0.000 TRINITY_DN28_c0_g1_i10 4194 3909.446 0.000000 0.000 F_diaphanus_FW_2.quant Name Length EffectiveLength TPM NumReads TRINITY_DN28_c0_g1_i8 3373 3091.748 0.519804 11.942 TRINITY_DN28_c0_g1_i9 4352 4070.748 2.246743 67.960 TRINITY_DN28_c0_g1_i3 3246 2964.748 1.067882 23.525 TRINITY_DN28_c0_g1_i11 4267 3985.748 29.020612 859.486 TRINITY_DN28_c0_g1_i2 4109 3827.748 30.238159 860.045 TRINITY_DN28_c0_g1_i12 3531 3249.748 0.000000 0.000 TRINITY_DN28_c0_g1_i10 4194 3912.748 0.000000 0.000 F_diaphanus_FW_3.quant Name Length EffectiveLength TPM NumReads TRINITY_DN28_c0_g1_i8 3373 3104.285 0.000000 0.000 TRINITY_DN28_c0_g1_i9 4352 4083.285 3.412720 143.858 TRINITY_DN28_c0_g1_i3 3246 2977.285 4.401957 135.298 TRINITY_DN28_c0_g1_i11 4267 3998.285 20.861407 861.076 TRINITY_DN28_c0_g1_i2 4109 3840.285 29.953996 1187.523 TRINITY_DN28_c0_g1_i12 3531 3262.285 2.240168 75.444 TRINITY_DN28_c0_g1_i10 4194 3925.285 0.000000 0.000 F_diaphanus_transfer_1.quant Name Length EffectiveLength TPM NumReads TRINITY_DN28_c0_g1_i8 3373 3140.825 4.490605 235.962 TRINITY_DN28_c0_g1_i9 4352 4119.825 0.000000 0.000 TRINITY_DN28_c0_g1_i3 3246 3013.825 5.011654 252.693 TRINITY_DN28_c0_g1_i11 4267 4034.825 19.506568 1316.738 TRINITY_DN28_c0_g1_i2 4109 3876.825 21.467309 1392.347 TRINITY_DN28_c0_g1_i12 3531 3298.825 0.000000 0.000 TRINITY_DN28_c0_g1_i10 4194 3961.825 5.584293 370.132 F_diaphanus_transfer_2.quant Name Length EffectiveLength TPM NumReads TRINITY_DN28_c0_g1_i8 3373 3053.084 0.871097 12.774 TRINITY_DN28_c0_g1_i9 4352 4032.084 0.000000 0.000 TRINITY_DN28_c0_g1_i3 3246 2926.084 3.676537 51.672 TRINITY_DN28_c0_g1_i11 4267 3947.084 27.276491 517.125 TRINITY_DN28_c0_g1_i2 4109 3789.084 29.076399 529.182 TRINITY_DN28_c0_g1_i12 3531 3211.084 0.000000 0.000 TRINITY_DN28_c0_g1_i10 4194 3874.084 2.249432 41.857 This is why salmon quantification software comes in handy. Even though these contigs are probably assembly confusions (the contigs were produced by the Trinity assembler but they probably belong to the same transcript), it is okay because salmon is calculating the probability that the read will be sampled based on the availability of the sequence in the graph. Since pieces occur multiple times, the quantification will be distributed among the fragments. Whereas with other quant programs, the quantification might be artificially inflated (or deflated) given the presence of nearly identical fragments. After talking to fellow DIB lab PhD student, Camille Scott , she helped me to see this with some cool figures of compact de Bruijn graphs that she made. The graph structure produced from this block of assembled sequences shows two different components: TRINITY_DN28_c0_g1_i6 Transcript_73 TRINITY_DN28_c0_g1_i8 Transcript_74 TRINITY_DN28_c0_g1_i9 Transcript_75 TRINITY_DN28_c0_g1_i3 Transcript_76 TRINITY_DN28_c0_g1_i11 Transcript_77 TRINITY_DN28_c0_g1_i2 Transcript_78 TRINITY_DN28_c0_g1_i12 Transcript_79 OG0010233 TRINITY_DN28_c0_g1_i10 Transcript_80 TRINITY_DN28_c0_g1_i5 Transcript_81 OG0063265 TRINITY_DN28_c1_g1_i7 Transcript_82 TRINITY_DN28_c1_g1_i5 Transcript_83 TRINITY_DN28_c1_g1_i4 Transcript_84 OG0006676 TRINITY_DN28_c1_g1_i1 Transcript_85 TRINITY_DN28_c1_g1_i3 Transcript_86 TRINITY_DN28_c1_g1_i8 Transcript_87 OG0014822 TRINITY_DN28_c1_g1_i6 Transcript_88 TRINITY_DN28_c1_g2_i1 Transcript_89 TRINITY_DN28_c2_g1_i2 Transcript_90 Full block: Separated into 2 components: So, (in my own words) when salmon goes through and quantifies the reads, if it sees k-mers that have been already seen before, it then distributes the counts across the pieces with similar known kmers. (Please correct me if I am mis-using terms here.) Even though contigs are annotated as the same protein, since they have a similar sequence, they will not get counted twice. Instead, counts will be distributed across all similar contigs. That is why tximport is great for summarizing the counts for contigs that have annotated as a cluster. Conclusion OG-annotated transcripts are single contigs. Looking at expression of only OGs misses the other contigs associated with that gene. Collapsing annotated contigs by unique gene ID is the best way to quantify expression with de novo transcriptome assemblies, based on the above and then use salmon and tximport to summarize counts by similar contigs. Questions that have come up: Are we really picking the right annotation per contig? What I'm doing now is to to sort by custom annotation and then pick the longest. Titus has suggested using word clouds. Yes, I might want to try this. There is an example below where one Trinity set of contigs has multiple annotations, but mixed across species. Possible that top annotations got mixed up. e.g. kelch-like and synaptotagmin. Still need to take a look at original dammit annotations to see how the decision was made. We are picking annotation-type (based on databases), then the longest annotation (not-evalue) as long as e-value was e-05 (or something like that, check the exact code and include). How would it look if we picked by e-value? Is collapsing contigs annotated with the same ID artificially inflating expression? What if the contig was constructed because there was a slightly alternative path in the assembly graph, which was not really a separate transcript? The principle of tximport is to collapse transcripts/contigs by gene , since alternative splicing is nearly impossible to measure with short-read sequencing. However, the case of de novo assembly further complicates this matter with messy assemblies and heterozygous animals. No. This is why salmon is so awesome. Because it is modeling the sampling probability from the sequences available. It takes into consideration multi-mapping. If a contig has the same identity, or nearly the same, as another contig, reads will map fewer times to it becuase of the model. How can we distinguish different fragments of the transcript vs. the same fragment just assembled slightly differently? (There might be some examples below where there are multiple contigs that are the same exact size. Look at the quantification. Salmon takes into consideration multi-mapping, where reads are seemingly matching to k-mers in found in multiple contigs. The probabilities of aligning should be taken into consideration. Theoretically, if multiple contigs have simliar sequences, the overall quantification of a block of contigs should be adjusted. Is this correct? Yes, but salmon is doing this for us. Major conclusion: Salmon is great. :)","tags":"data","url":"https://johnsolk.github.io/blog/annotating-multiple-de-novo-transcriptome-assemblies.html"},{"title":"Running clust on Jetstream","text":"Clust is a pipeline from Abu-Jamous and Kelly (2018) that will take a gene expression data table (or tables), cluster and extract so that patterns of genes that are co-expressed can be identified. This is a method similar to WGCNA . To explore clust, I used an s1.xlarge instance on Jetstream (CPU: 24, Mem: 60 GB, Disk: 240 GB, Disk: 240 GB root). This instance size might be excessive, but I already had the instance up and running Jupyter notebook trying to parse and do some dataframe manipulation between 17 large ~1 GB gff3 files. While this gff3 parsing task was frustrating me by not working, I thought that I would try using the clust program since it has been on my list for some time, while I already had the instance running. This turned out to be an encouraging task because clust is very easy to run! Make a conda environment for python 2.7. Doesn't matter if python 3 is default environment as long as you specify in command to make python 2, like this: conda create -n run_clust python=2.7 source activate run_clust Then, install clust: pip install clust Get expression data table formatted. I have one on osf to download as an example: mkdir clust_data cd clust_data/ curl -L https://osf.io/fg72b/download -o exp.tsv cd .. Format metadata file defining replicates (I have one on osf): curl -L https://osf.io/2d9em/download -o replicates_file Then run clust: clust clust_data/ -r replicates_file Takes ~5 min! Grab output files: scp -r ljcohen@<ip-address>:~/kfish_clust/Results_21_Jan_19 . Voila! Clusters in the Clusters_profiles.pdf file: Genes in the Clusters_Objects.tsv file: Getting clusters of all genes across 17 species combined is not exactly answering my question/goal, which is to find the genes with expression patterns that are either in common or different between my 17 species of killifish. But, now I know how to run the program. The next step is to make separate expression tables for each species, then run clust , which will keep track of expression patterns for each species separately. Output from clust: ljcohen @ js-168-95 :/ kfish_clust $ clust clust_data / -r replicates_file /=========================================================================== \\ | Clust | | ( Optimised consensus clustering of multiple heterogenous datasets ) | | Python package version 1 . 8 . 10 ( 2018 ) Basel Abu-Jamous | + --------------------------------------------------------------------------- + | Analysis started at : Monday 21 January 2019 ( 21 : 28 : 01 ) | | 1 . Reading dataset ( s ) | | 2 . Data pre-processing | | - Automatic normalisation mode ( default in v1 . 7 . 0 +). | | Clust automatically normalises your dataset ( s ). | | To switch it off , use the ` -n 0 ` option ( not recommended ). | | Check https :// github . com / BaselAbujamous / clust for details . | | - Flat expression profiles filtered out ( default in v1 . 7 . 0 +). | | To switch it off , use the --no-fil-flat option ( not recommended ). | | Check https :// github . com / BaselAbujamous / clust for details . | | 3 . Seed clusters production ( the Bi-CoPaM method ) | | 10 % | | 20 % | | 30 % | | 40 % | | 50 % | | 60 % | | 70 % | | 80 % | | 90 % | | 100 % | | 4 . Cluster evaluation and selection ( the M-N scatter plots technique ) | | 10 % | | 20 % | | 30 % | | 40 % | | 50 % | | 60 % | | 70 % | | 80 % | | 90 % | | 100 % | | 5 . Cluster optimisation and completion | | 6 . Saving results in | | / kfish_clust / Results_21_Jan_19 | + --------------------------------------------------------------------------- + | Analysis finished at : Monday 21 January 2019 ( 21 : 32 : 16 ) | | Total time consumed : 0 hours , 4 minutes , and 14 seconds | | | \\ ===========================================================================/ /=========================================================================== \\ | RESULTS SUMMARY | + --------------------------------------------------------------------------- + | Clust received 1 dataset with 27775 unique genes . After filtering , 20289 | | genes made it to the clustering step . Clust generated 12 clusters of | | genes , which in total include 5941 genes . The smallest cluster includes | | 73 genes , the largest cluster includes 1972 genes , and the average | | cluster size is 495 . 083333333 genes . | + --------------------------------------------------------------------------- + | Citation | | ~~~~~~~~ | | When publishing work that uses Clust , please include this citation : | | Basel Abu-Jamous and Steven Kelly ( 2018 ) Clust : automatic extraction of | | optimal co-expressed gene clusters from gene expression data . Genome | | Biology 19 : 172 ; doi : https :// doi . org / 10 . 1186 / s13059-018-1536-8 . | + --------------------------------------------------------------------------- + | For enquiries contact : | | Basel Abu-Jamous | | Department of Plant Sciences , University of Oxford | | basel . abujamous @ plants . ox . ac . uk | | baselabujamous @ gmail . com | \\ ===========================================================================/ Also, I learned that clust will take output from OrthoFinder , which I have been previously exploring . I am very pleased with this possibility and will be exploring further with .pep files made with Transdecoder. The challenge now will be to quantify expression based on orthogroups.","tags":"Proposals","url":"https://johnsolk.github.io/blog/running-clust-on-jetstream.html"},{"title":"XRAC research proposal","text":"Last week (1/15/2019), we submitted a proposal for an XSEDE research allocation . Below is our proposal. This is to support the last bits of computational work required for my dissertation, which I hope to complete this year. Tessa Pierce's eelpond automated de novo transcriptome assembly pipeline. In addition, we want to implement and test the Oyster River Protocol across a large dataset. Plus some extra compute for incoming PhD rotation students. We've been successful with getting startup allocations approved by XSEDE since 2016. Our summer dibsi/angus courses use Jetstream , due to several successfull education proposals . But with my last round requesting a renewal for my startup, apparently I have exceeded the limits. So they asked for a full proposal to the XSEDE resource allocation committee (XRAC). (Fingers crossed) We will learn the outcome of the proposal in March. UPDATE: (3/15/2019) We received our allocation! Here is our proposal (pdf) and code performance and resource usage document === Improving reference genomes and transcriptomes towards gene expression profiling of sequencing data sets containing multiple eukaryotic species PI: N. Tessa Pierce Co-PI: Lisa Johnson UC Davis Data Intensive Biology Lab Startup Allocations: TG-MCB170160 and TG-BIO160028 Background and Support High-throughput sequencing methods have revolutionized ecological and evolutionary studies by facilitating research at the whole-genome level for multiple species. However, generation of high-quality references remains a complex and computationally-intensive endeavor, complicated by the biological properties of the sequence (e.g. polymorphism, gene splicing and duplication, etc), technical limitations of the sequencing platforms and analysis tools, and the scale (often hundreds of samples) required for balanced experimental designs. Furthermore, we have shown that the goal to generate a single reference genome or transcriptome as a final product must be re-evaluated in light of the information gained via reanalysis with new tools (Johnson et al. 2018). Our research aims to address these issues by systematically evaluating transcriptome and genome assembly, annotation and analysis methods to produce best practices, user-friendly, automated pipelines that facilitate future research. Here, we leverage biological datasets at two scales: first a broad-scale analysis of 678 species in the Marine Microbial Eukaryotic Transcriptome Sequencing Project (MMETSP), and second, finer-scale analysis on the evolution of salinity tolerance in sixteen species of a relatively well-studied species group, Fundulus killifish, exposed to acute hypersalinity challenge. The phylogenetic breadth of these projects enables us to build better bioinformatic pipelines agnostic of species that can be applied by investigators to sequencing data from multiple species as well as repeatedly to our own data as software tools and annotation databases improve. These projects are supported through November 2019 by a Moore Foundation Data-Driven Science investigator award to the PI of our lab, Dr. C. Titus Brown. PI Dr. N. Tessa Pierce is funded by an NSF Postdoctoral Research Fellowship in Biology (#1711984; 2017-2020), entitled \"Improving RNA-Seq Analysis through Graph-based Analysis and Computational Indexing,\" which uses the MMETSP data to test RNAseq analyses. Co-PI Lisa Johnson (Cohen) received an NSF Graduate Research Fellowship Honorable Mention in 2016 and has authored the first publication for this project leveraging the MMETSP dataset (Johnson et al. 2018). Results from these analyses are essential preliminary data for funding proposals to be submitted in the coming year. A. Research Objectives Project 1: RNA-Seq tool development using the MMETSP dataset The MMETSP consists of RNAseq data from 678 cultured protist species spanning more than 40 eukaryotic phyla, generated to broaden the diversity of sequenced marine protists and facilitate our understanding of their evolution and roles in marine ecosystems and biogeochemical cycles (Keeling et al. 2014). This database forms a rich reference which can be queried to help functionally characterize genes from environmental samples containing species that cannot be cultured. However, the MMETSP data also provides a unique opportunity to assess RNAseq analysis methods using one of the largest and most diverse publicly-available RNAseq data sets at this time. MMETSP is also unusually well-suited for benchmarking analyses, as data submitted by a large consortium of international investigators was prepared with a single, standardized library preparation and sequenced at a single facility. MMETSP was first analyzed in 2013, with de novo reference transcriptomes generated by the National Center for Genomic Resources (NCGR) using the Trans-ABySS assembler (Robertson et al. 2010). However, transcriptome assembly is an area of active research, with many new tools and improvements since this initial analysis. The first part of this project (now published) demonstrated that application of a new assembly software tool, Trinity (Grabherr et al. 2011) improved the quality of the assemblies and uncovered new content (Johnson et al. 2018). Comparing the original Trans-ABySS \"NCGR\" pipeline and our Trinity-based \"DIB\" assembly pipeline, we see differences in the number of contigs (Figure 1, left), the assembly content measured by alignments (Figure 1, middle) and unique k -mers ( k =25) (Figure 1, right). These assemblies are annotated with the dammit annotation program , developed by Camille Scott who is a graduate student in our lab. Figure 1. (left) Number of contigs compared between NCGR (yellow) and DIB (blue) de novo transcriptome assemblies. (middle) Proportion of contig sequence similarity as measured by a conditional reciprocal best BLAST (CRBB) algorithm between NCGR (yellow) and DIB (blue) assemblies, indicating that the two assemblies have different content. (right) Comparison of the unique k-mer (k=25) content between NCGR assemblies (x-axis) and DIB assemblies (y-axis) indicating that the DIB assemblies had more unique content. The MMETSP data is publicly available from the NCBI Sequence Read Archive (PRJNA231566) from 678 species, 168,152,967,701 PE50 reads total, 1TB raw data storage with the following phyla (N=species): Bacillariophyta (N=173), Dinophyta (N=114), Ochrophyta (N=73), Haptophyta (N=61), Ciliophora (N=25), Chlorophyta (N=62), Cryptophyta (N=22), Others (N=13). Improving Reference Generation with eelpond Given these improvements to old data using new tools, in the second part of this project we are developing an automated best-practices pipeline, called eelpond (https://github.com/dib-lab/eelpond), for non-model species within a framework that facilitates software installation and new tool integration. Eelpond accepts a set of raw fastq RNA sequencing reads and an experimental design matrix, and generates annotated de novo transcriptome assemblies. It will also run preliminary differential expression analysis with the DESeq2 R package. A new multi-assembler protocol, the Oyster River Protocol (ORP; MacManes 2018) suggests that merging contigs from several assemblers (Spades; Bushmanova et. al 2018, Shannon, Trinity; Grabherr et. al 2011) and multiple k-mer lengths may outperform assemblies with the currently-favored assembler, Trinity. We are integrating each of these assemblers (including the full ORP pipeline) into eelpond for benchmarking and comparison with the MMETSP data. Gene Pathway and Structure Discovery (glympsed) & Phylotranscriptomics The final goal of our MMETSP project is to develop an unsupervised deep learning pipeline, called glympsed, using the \"denoising autoencoder\" algorithm (Vincent et al 2008) to discover common expression patterns in large RNA sequencing data sets (>100 samples and 30,000 genes/sample). This method has been shown to successfully identify common metabolic patterns of gene expression where there are unknown relationships between samples (Tan et al. 2016). Code for the glympsed project was written as a collaborative group in 2016 using a a large test set from all publicly-available Pseudomonas bacteria data, with the aim of testing and optimizing the glympsed pipeline with MMETSP data. Now that the MMETSP re-assemblies and expression quantification results are complete (Johnson et al. 2018), we can use these results as input to run the glympsed pipeline. To start, we will examine data from members of the dinoflagellata phylum in the MMETSP set, which exhibit two distinct lifestyles (endosymbiotic and free-living). With glympsed, we will investigate fundamental metabolic transcriptional differences between the two groups. We will then expand the this approach to additional taxonomic groups within the MMETSP. Following the development and successful analysis with the glympsed pipeline, the agalma phylotranscriptomics pipeline (Dunn et al. 2013) will be used to generate species trees based on pathway genes identified with glympsed. Project 2: Comparative genomics and gene expression analysis of 16 Fundulus killifish species in response to osmotic stress Killifish in the genus Fundulus have emerged as a comparative model system for studying the physiological and genetic mechanisms underlying osmotic tolerance (Whitehead 2010). Some Fundulus species can tolerate a range of environmental salinities (euryhaline) by switching osmoregulatory mechanisms while other Fundulus species require a narrower salinity range (stenohaline) in either fresh or marine waters. In collaboration with Andrew Whitehead's Environmental Genomics lab at UC Davis, we are studying the evolutionary history of adaptation in these 16 marine, estuarine and freshwater Fundulus species with representation from each of the three clades where species have independently radiated into freshwater environments. As these species are closely related and have a well-characterized phylogeny, this study presents a unique opportunity to study parallel evolution of physiological adaptation. This project leverages both stress-response RNA-seq data and long-read genome data. RNAseq data were collected after exposure of 130 individuals to a common, controlled acute hypersalinity challenge experiment, and deeply sequenced on the Illumina HiSeq platform, generating over 4 billion PE100 reads, 500GB raw data storage: Adinia xenica (N=9), Fundulus catenatus (N=7), Fundulus chrysotus (N=8), Fundulus diaphanus (N=7), Fundulus grandis (N=9), Fundulus heteroclitus (MDPL population) (N=9), Fundulus heteroclitus (MDPP population) (N=9), Fundulus notatus (N=9), Fundulus nottii (N=2), Fundulus olivaceus (N=8), Fundulus parvapinis (N=8), Fundulus rathbuni (N=9), Fundulus sciadicus (N=5), Fundulus similis (N=9), Fundulus zebrinus (N=4), Lucania goodei (N=9), Lucania parva (N=9) (Rodgers et al. 2018; Johnson and Whitehead 2018). After the experiment, a subset of four species (3 freshwater: Fundulus nottii , Fundulus catenatus , Fundulus olivaceus , and one marine: Adinia xenica ) were selected for genome assembly via Oxford Nanopore Technologies (ONT) long-read sequencing . Transcriptome Analysis with eelpond Following the success of transcriptome assemblies of the MMETSP dataset, we are applying the same methods to assemble de novo transcriptomes for each of these 16 Fundulus species, which we can then use to analyze transcriptional responses to salinity change by clade and physiology. The end product for this project will be sets of candidate genes with common expression patterns and their corresponding physiological pathways. Based on previous work with microarrays and protein assays, we expect to see isoform switching in relevant genes (e.g. ATPase Na+/K+ transporting subunit alpha 1b vs. alpha 1a). These differences are hard to detect and require high quality reference transcriptomes. Improving the quality of the assemblies has included assembling and re-assembling with the new version of Trinity (2.8.4), which improved the contiguity with fewer contigs (Figure 2, left) and benchmarking evaluation scores (BUSCO; Simo et. al 2015) (Figure 2, right). De novo transcriptome assemblies have been generated, twice with updated versions of Trinity. Now, they need to be improved with ORP and automated using eelpond. Figure 2. (left) Numbers of contigs decreased from old assemblies (purple) to new assemblies (green). (right) Benchmarking scores (BUSCO) increased with a tighter distribution in new assemblies (green) compared to the old assemblies (purple). Generating High-Quality Genome References Genomic sequence data from four species were generated (3 freshwater: Fundulus nottii, Fundulus catenatus, Fundulus olivaceus, and one marine: Adinia xenica) via Oxford Nanopore Technologies (ONT) PromethION desktop sequencer (approximately $3,500/species). Each genome is estimated to be around 1.1 Gb, based on the size of the sister species, Fundulus heteroclitus; therefore, we have collected between 30-50x coverage of ONT data. These genomes will better enable us to detect these small changes in the transcriptomes, as well as to discover changes in genomic architecture underlying adaptive responses to environmental change. Below is a table listing ONT data collected for each species: ONT data collected thus far, which will contribute to de novo genomes of 4 Fundulus species: Species bases called n reads avg length largest reads N50 Adinia xenica 38,467,326,719 15,704,522 2,449 953,774 5,733, n = 1,373,426 Fundulus nottii 33,440,866,723 5,160,367 6,480 667,947 12,995, n=700,534 Fundulus catenatus 40,274,806,587 23,701,206 1,699 590,485 3,439, n = 2,687,295 Fundulus olivaceus (MinION) 4,962,626,713 740,248 6,704 973,552 12,726, n = 117,202 Fundulus olivaceus (PromethION) 50,093,027,850 10,902,817 4,595 779,368 11,670, n = 987,921 ONT is a new sequencing technology that has only become available within the past 3 years; however, it is showing promise improving genome assemblies because of the length of the reads (Ebbert et al. 2018). Because of the high error rate (~10-15%) associated with ONT data, we are in the process of generating 50x coverage of paired-end 150 bp short-read Illumina data, which has a low error rate (<1%) and will be used to correct the noisy long reads. The size of the Illumina short read data will be around 500 GB total. Gene Pathway and Structure Discovery (glympsed) & Phylotranscriptomics The killifish data also have more than 100 samples and will be run through the glympsed pipeline. We are looking for common patterns of gene expression across the clades, hypothesizing that there will be some genes with parallel avenues of evolution whereas some genes will have divergent modes of evolution. B. Computational Methods Transcriptome assemblies We are working towards improving the quality of reference transcriptomes for these two projects: 16 Fundulus species + 678 MMETSP species = 694 species, around 700 transcriptomes. RNAseq samples from Fundulus species have approximately 10x more reads (average = 238,363,948) than samples from the MMETSP (average = 23,387,060 reads). Figure 3. Workflow diagram for the eelpond RNAseq de novo transcriptome assembly pipeline, indicating milestones for each step: Download, quality assessment, digital normalization, assembly, annotation, and evaluation. Eelpond To improve each MMETSP and Fundulus killifish transcriptome, we will run our automated eelpond pipeline (Figure 3; https://github.com/dib-lab/eelpond) to conduct read quality and error trimming, generate additional transcriptome assemblies, and perform assembly annotation and quality evaluation. Eelpond uses Python's snakemake automation tool to manage each run and uses conda for tool installation. While some of the components of eelpond are not compute intensive, the main assembler, Trinity, is very compute intensive (see \"Code Performance and Resource Usage\" document). To run this pipeline in parallel to process files from from start to finish, all tools must be run on the HPC. Since the pipeline is modularized, individual milestones can be adjusted (separate jobs submitted requesting different resources) so the whole pipeline does not require the same large amount of resources as Trinity. We are in the process of integrating the Oyster River Protocol, a multi-kmer, multi-assembler tool that may outperform Trinity assemblies (MacManes 2018). Since we already have Trinity assemblies, testing ORP will involve running the remaining assemblers (Spades x2 and Shannon) as well as the assembly merging script, Orthofuser. The resulting assemblies will also require annotations, which eelpond will perform with dammit (Scott in prep). Downstream analyses will be conducted with Salmon (Patro et al 2017) quantification and DESeq2 (Love et al. 2014), already integrated into the pipeline. While eelpond currently works on AWS and Jetstream, only minor modifications are needed to scale up to the Bridges HPC. Resource allocations requested by this proposal will allow us to develop the eelpond protocol for any hpc users with slurm job management, such as PSC Bridges. Genome Assemblies Four Fundulus killifish genomes will be assembled from Oxford Nanopore Technologies (ONT) long read data. New tools and methods for assembling and analyzing these noisy ONT long reads are being developed at a fast rate (de Lannoy et al. 2017). We will perform hybrid genome assemblies with ONT and Illumina data, which can produce high quality (>2 Mb contig N50) reference genomes (Tan et al 2018, Miller et al. 2018, Liu et al. 2018). We have found the best available assembler to be MaSuRCA, as evaluated by complete BUSCO identification (eukaryota database; Simo et. al 2015): Seq data input Tool bases n_contigs average largest N50 BUSCO (eukaryota) ONT (MinION) Canu 9,804,264 540 18,156 365,191 40,681, n = 43 0.7% ONT (MinION) Miniasm 4,917,546 153 32,140 233,136 50,056, n = 25 0.0 % Illumina (NovaSeq) Megahit 1,183,861,293 1038799 1,139 88,218 3,846, n = 77,800 45.6 % Illumina (NovaSeq) ABySS 1,381,148,284 1024759 1,347 140629 9833, n = 37,013 77.9% Hybrid Masurca 1,134,160,060 90,237 12,568 386,222 42,823, n = 7,616 86.2% The job to run the MaSuRCA software tool required 1TB storage for intermediate files and >1 week to run for the 1.1 Gb genome assembly. Our measurements were validated by the manual and communications with the MaSuRCA authors. Following genome assembly, the intermediate files are deleted. We have successfully produced one MaSuRCA assembly on the MSU hpcc with an abbreviated set of ONT data. However, we were not able to benchmark this on PSC Bridges before this proposal, due to time and resource constraints on our startup allocations. After assembly with MaSuRCA, the quality of the reference genome assemblies will be improved by applying the Pilon or Racon scaffolding and consensus improvement tools, which are also resource intensive. These four genomes will be annotated using the MAKER pipeline (Cantarel 2008; latest version v2.31.10 May 4, 2018), used to align RNAseq reads and to perform comparative genomic analysis to test for parallel loss of the genomic architecture that supports plasticity to salinity environments. Data have been collected and now need to be assembled and annotated. Genomes will then be used for comparative genomics analysis. Gene Pathway and Structure Discovery (glympsed) & Phylotranscriptomics To discover common expression patterns in large RNA sequencing data sets (>100 samples and 30,000 genes/sample), we are in the process of developing a pipeline using the unsupervised deep learning library, Keras (Chollet 2018). This pipeline, called \"glympsed\" (https://github.com/glympsed/glympsed) uses the \"denoising autoencoder\" algorithm (Vincent et al 2008) to extract features or patterns from the noisy dataset. This method has been shown to successfully identify common metabolic patterns of gene expression where there are unknown relationships between samples (Tan et al. 2016). The results of this pipeline will be used to generate hypotheses about pathway \"features\" that are present or absent in MMETSP and killifish samples. Preliminary code and notebooks are available on GitHub, which were developed on AWS. Simulated data were created to test the denoising autoencoder model by randomly sampling beta values, assuming gene expression data follows negative binomial distribution. Jetstream resources will be used to develop code and run Jupyter notebooks for visualizations. Scripts will be written generate output tables containing gene networks and connect to gene regulatory network and pathway databases, such as KEGG (Kanehisa et. al 2000). Tutorials will be developed and Jetstream images with required software installed will be made public. Following the development and successful completion of results produced by the glympsed pipeline, the agalma phylotranscriptomics pipeline (Dunn et al. 2013) will be used to generate species trees based on pathway genes identified with glympsed. Program Resources Total SUs Development of glympsed and tutorials for phylotranscriptomics pipelines Jetstream 110,000 C. Computational Research Plan: Resource usage with accompanying application efficiencies to achieve the research objectives. We are requesting PSC Bridges (LM, RM and Pylon storage) in addition to Jetstream cloud computing resources. Jetstream is great for troubleshooting custom software, developing pipeline scripts and performing calculations and visualizations with data table inputs. PSC Bridges is great for storing raw sequencing data files, scaling up assembly pipelines with high memory processes in parallel, then evaluating and generating data tables. Jetstream, like AWS elastic cloud computing where we have done some of our initial pipeline development and software installation, allows the freedom of installing with root privileges and frequently updating custom software, as well as hosting open data analysis collaboration with tools such as Jupyter notebooks. For students in a classroom setting and in order to provide persistent documentation, Jetstream resources are key to loading pre-installed software from images and install new software to test. Since we do not need the intermediate files beyond the time during computation, the de novo assembly products and gene expression quantification tables can be downloaded to local computers and the instances can be deleted. We need access to an HPC with high memory nodes, like PSC Bridges to launch high memory jobs because the maximum Jetstream instance (s1.xxlarge: CPU 44, Mem 120 GB, Disk 480 GB) is too small to accommodate some of our assemblies and is also incapable of scaling to running hundreds of assemblies in parallel. We will delete intermediate files after assemblies have been generated and downloaded. Persistent storage is also a feature of Bridges that is not possible with Jetstream. One of the more frustrating practical aspects of using Jetstream is having to store large raw files for a data set that is around 1TB in volumes and keep the volume accessible each time you want to work with them from a different instance. Our experience and SUs measured during resources are explained in the attached \"Code Performance and Resource Usage\" document. D. Justification of the SU allocation amounts for all resources and resource types. Based on the \"Code Performance and Resource Usage\" document below, the total amount of SUs requested for the following year of research are outlined as follows: Resource SUs requested Jetstream 110,000 Bridges LM 44,876 Bridges RM 139,750 Bridges Pylon storage 25 TB E. Resource Appropriateness We've used two XSEDE startups of PSC Bridges allocations (LM, RM, and Pylon storage) and two startups of Jetstream allocations generating de novo transcriptome assemblies and annotations for the MMETSP dataset and 17 killifish as well as a separate project on Doryteuthis opalescens squid, which is being prepared for publication by PI Pierce. As a result, we are experienced Jetstream and Bridges users. These XSEDE startup allocations on Jetstream have contributed to one publication thus far (Johnson et al. 2018) and will contribute to another two manuscripts before the conclusion of Ms. Johnson's PhD research. However, additional compute time is essential to complete this work and prepare results for publication. Completion of Lisa Johnson's PhD dissertation research depends on these resources. We have participated as active members of the XSEDE community both on Jetstream and PSC Bridges, communicating with the help team with questions and reporting problems with jobs and compute nodes. We have served as PIs, Co-PIs and users of 5 education allocations where we have taught researchers (>300 total) in workshop settings how to use Jetstream resources. We have created two public images as a result of these allocations to make future software installation on new instances easier: \"RNASeq_1DayWorkshop\" and \"dammit_annotation_v1.0\". The following are workshops delivered with tutorials using Jetstream materials: Society for Environmental Toxicology And Chemistry meeting, November 2018 (TG-MCB180142), Global Invertebrate Genomics Alliance meeting, October 2018 (TG-MCB180140), Scripps Institution of Oceanography, Bioinformatics User's Group workshop, October 2017 (TG-BIO170083), Data Intensive Biology Summer Institute (DIBSI), non-model RNAseq topic workshop, July 2017 (TG-BIO170017), 2-week DIBSI beginner's workshop (ANGUS), June-July 2018, ANGUS 2017 (TG-BIO170017). The following is an example website for a workshop using Jetstream materials that is available to the public and persistent for users beyond the time of the workshop: https://setac-omics.readthedocs.io/en/latest/. Local computing environment As a members of Dr. C. Titus Brown's lab at UC Davis, we have access to two high performance computing clusters at Michigan State University (iCER's hpcc) and UC Davis (farm cluster). However, there are several issues with these resources. Our time on the MSU hpcc is expiring this fall 2019, following Dr. Brown's transition from MSU to UCD faculty in 2015. In addition, there has been limited capacity on the MSU hpcc for supporting the scale of these multi-species pipelines as we only have access to 4 nodes with 1TB of resources and the queue time is long. Moving 700 transcriptome through this queue takes more than one month and our storage capacity is limited. With the PSC Bridges startup, we have been extremely pleased with the ease of submitting jobs and having them run smoothly on both the LM and RM partitions. Research funds for AWS compute time are limited, as the Moore Foundation grant that has contributed to funding these projects ends in November 2019. On the UCD farm cluster, we do not have access to the high memory node without buying in to the high memory partition of the system. As our current account stands, we are limited to 12 jobs running at a time with 75 GB RAM each and less than 1TB of total storage. Following the computing processes described in this proposal, raw sequence files will be stored locally on storage hard-drives and deposited in NCBI's Sequence Read Archive public repository. Data products, e.g. assemblies will be stored in public repositories such as Zenodo and the Open Science Framework (Foster and Deardorff, 2017). F. Additional Considerations Research outreach and training Graduate students at UC Davis are required to rotate through labs during their first year prior to choosing a primary advisor. During rotations, small projects are completed. This allocation will contribute to computational training of several rotation students in our lab who are working on the MMETSP data sets using the eelpond protocol. New protocols developed, e.g. glympsed and eelpond-integrated Oyster River Protocol will contribute to tutorials taught during our Data Intensive Biology Summer Institute at UC Davis. Funding Analyses for the above-mentioned projects are funded until November 2019 by the Moore Foundation Data-Driven-Science investigator award to the PI of our lab, Dr. C. Titus Brown. Lisa Johnson (Cohen) received an NSF Graduate Research Fellowship Honorable Mention in 2016, which enabled her to serve as PI for the XSEDE startup allocation TG-BIO160028) that has contributed to most of her dissertation work. Tessa Pierce has an NSF Postdoctoral Fellowship and has been a PI for the XSEDE startup allocation TG-MCB170160, which has contributed to her postdoctoral research. Ms. Johnson and Dr. Pierce have both served as co-PIs for the XSEDE education proposals listed above. Results from these analyses are necessary to contribute as preliminary analyses for future proposals requesting funding from agencies such as the NSF. Code Performance and Resource Usage This document is based on 70-80% usage (PSC Bridges LM: 113 jobs, RM: 1042 jobs) of startup allocation TG-BIO160028 on PSC Bridges since we began our allocation on 11/26/2018 and 100% usage of 150,000 SUs on Jetstream from 2016-2018 with TG-BIO160028 and 72% usage of 100,000 SUs on Jetstream with TG-MCB170160. Analyses run The following table includes processes run previously and their resources used that are directly related to the processes we are requesting additional resources for in this proposal. Transcriptomic analysis Jetstream resources were estimated, as long-running tools are often run overnight, and instances used for training and workshops are kept running during instruction and troubleshooting, so active program runtimes are not continuous. Bridges jobs are listed for the species with the longest time and highest resource usage. Process Species Data size Resource SU/run used Workshop tutorial (single commands run by hand), de novo transcriptome assembly pipeline Nematostella vectensis 30,000 reads subset Jetstream, m1.medium 12 = 6 CPU x 8hrs workshop time dib-MMETSP, automated de novo transcriptome assembly pipeline (Johnson et al. 2018) Chaetoceros neogracile (Phylum: Bacillariophyta), MMETSP0751 33,150,207 reads Jetstream, s1.xxlarge 264 = 44 CPU x 6hrs Digital normalization with khmer Fundulus grandis 467,432,867 trimmed reads Bridges RM; jobID 4524354 48.10 De novo transcriptome assembly with Trinity Fundulus heteroclitus (MDPL population) 79,341,401 reads (after digital normalization) Bridges LM; jobID 4555613 232.07 Transcriptome annotation, dammit Fundulus heteroclitus (MDPP population) 668,487 contigs Bridges LM; jobID 4710488 57.48 Transcriptome and genome evaluation, BUSCO Fundulus grandis 809,060 contigs Bridges RM; jobID 4612712 292.97 Transcriptome evaluation, Transrate Fundulus grandis 809,060 contigs Bridges RM; jobID 4609550 33.33 Expression quantification, salmon Loop with all 9 individuals from the Fundulus grandis species 467,432,867 trimmed reads Bridges RM; jobID 4707781 14.37 Genome alignment of RNAseq reads, STAR Fundulus grandis , BW treatment, sample 1 81,736,962 trimmed reads Bridges RM; jobID 4702491 6.76 Species trees, agalma Subset of data from 6 invertebrate siphonophore and cnidarian species, RNAseq reads Jetstream, m1.medium 12 = 6 CPU x 2hrs Genomic analysis: Process Data type Data size Resource SUs used De novo genome assembly of a bacteria, ONT reads with canu and Pilon correction Tenacibaculum sp. bacteria (3Mb) Jetstream, s1.large 80 = 10 CPU x 8 hrs MaSuRCA Killifish genome (1.1Gb) 4x ONT coverage subset with 50x coverage Illumina data MSU hpcc 168 hrs = 1 TB for 1 week; MSU deleted my log file :( Racon and Pilon Killifish genome (1.1Gb) 4x ONT coverage subset with 50x coverage Illumina data MSU hpcc Job failed \"Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\" with 500 GB memory Extrapolation for Future work We have not had a chance to benchmark everything that we want to yet for the de novo genome assemblies, which is why we're writing this proposal requesting more computing resources on these platforms. As mentioned in the main document, because long reads from Oxford Nanopore Technologies (ONT) are \"third-generation\" sequencing technology, software tools that are being developed at a fast rate to keep up with the developing technology. The raw signal files used to call bases to generate fastq sequence files must be saved because the base-calling software algorithms are improving. We have not tested the new base-calling software, and thus have not included it in our pipeline here. But, in the next year, if the software improves, we will need to return to the beginning of our pipeline to acquire more accurate bases for our reads. There is not yet a set of best practices for these software tools for de novo genome assembly using ONT reads, therefore several must still be tested. As mentioned in the main document, we have tested several de novo genome assemblers for ONT data with a subset of data and have found that MaSuRCA to be the best. However, if we scale up our data and find that another tool is better, we would like to use the better set of tools. SUs requested for running genome assembly tools for 4 Fundulus genomes, 1.1Gb each: Program Description # runs SUs/run Resources Total SUs MaSuRCA assembler 4 168 = 1 TB x 168 hrs Bridges LM 672 Racon and/or Pilon Consensus improvement 24 = 4 x 6 iterations/species* 48 = 1 TB x 48 hrs Bridges LM 1,152 khmer Error correction 4 50 Bridges RM 200 sourmash decontamination 4 50 Bridges RM 200 MAKER annotation 4 48 = 1 TB x 48 hrs Bridges LM 1,152 storage: ONT signal files 1TB/species + 100 GB fastq sequence data/species x 4 species + 500 GB for 2 lanes Illumina PE150 fastq data + 3 TB/species x 4 species for intermediate storage during assemblies Bridges Pylon storage 17 TB *Racon and Pilon are separate tools that take Illumina data and polish the final assembly to improve it. The reason why we have more runs for either Racon and Pilon in our list of SUs requested below is because these require multiple iterations for correcting the consensus genome sequence (Miller et al. 2018). We have done this and confirmed for a 3 Mb bacteria (Tenacibaculum) genome on Jetstream (m1.large), but not on larger assemblies because the jobs crashed without enough memory on our local MSU hpcc We have not attempted on Bridges yet because we didn't have enough SUs in our startup nor the time between 11/26/18 and the completion of this proposal (1/15/19). SUs requested for running de novo transcriptome assembly tools for 700 transcriptomes: Program Description # runs SUs/run Resources Total SUs khmer digital normalization and error correction 700 50 Bridges RM 3,500 Spades x2 k-mers, Shannon de novo transcriptome assemblers 16 500 Bridges LM 8,000 Spades x2 k-mers, Shannon de novo transcriptome assemblers 678 50 Bridges LM 33,900 Dammit annotation pipeline 700 50 Bridges RM 35,500 Transrate transcriptome evaluation 1,400 (run twice for each assembly, comparing against reference) 35 Bridges RM 49,000 BUSCO genome and transcriptome evaluation 700 50 Bridges RM 35,000 OrthoFinder identification of 1-1 orthologs from data sets 2 = MMETSP assemblies + killifish assemblies 250 Bridges RM 400 Salmon gene expression quantification 700 20 Bridges RM 14,000 STAR alignments 130 Fundulus individuals 15 Bridges RM 1,950 Oyster River Protocol contig merging 700 ~200 hours of s1.xxlarge at 44 SU Jetstream 10,000 storage 2 TB raw sequence data + 6 TB for intermediate files and alignment .bam files Bridges Pylon storage 8 TB Bioinformatics Tools, performance and scaling In general, the bioinformatics software tools that we use have been published and optimized by the software authors - including the PI of our lab, Dr. C. Titus Brown who developed and maintains khmer (digital normalization and reads error correction) and sourmash (decontamination) - and are made available for users, like ourselves, to generate data products such as de novo genome and transcriptome assemblies and annotations for our first two objectives. The third deep learning objective is an exploratory analysis, and we do not have all of the code yet. The scripts that we have written collaboratively thus far do not require computing resources beyond a cloud instance that will accomodate the software and tables containing expression data from hundreds of species by >30,000 genes each, which is beyond what can be run on a laptop computer. Therefore, we are requesting Jetstream resources to further explore this type of analysis. We have been successful in running these tools on systems with slurm management, such as MSU hpcc and UCD farm cluster. However, these hpc resources will not support the work that we want to do to assemble genomes with ONT and Illumina data, re-assemble more transcriptomes, and complete machine learning analyses. Therefore, we are hoping to use PSC Bridges to conduct the assemblies of four genomes. References Bushmanova, E., Antipov, D., Lapidus, A., & Przhibelskiy, A. D. (2018). rnaSPAdes: a de novo transcriptome assembler and its application to RNA-Seq data. bioRxiv, 420208. Cantarel, B. L., Korf, I., Robb, S. M., Parra, G., Ross, E., Moore, B., ... & Yandell, M. (2008). MAKER: an easy-to-use annotation pipeline designed for emerging model organism genomes. Genome research, 18(1), 188-196. Chollet, F. (2018). Keras: The python deep learning library. Astrophysics Source Code Library. de Lannoy, C., de Ridder, D., & Risse, J. (2017). The long reads ahead: de novo genome assembly using the MinION. F1000Research, 6. Dunn, C. W., Howison, M., & Zapata, F. (2013). Agalma: an automated phylogenomics workflow. BMC bioinformatics, 14(1), 330. Ebbert, M. T., Jensen, T. D., Jansen-West, K., Sens, J. P., Reddy, J. S., Ridge, P. G., ... & Keene, D. (2019). Systematic analysis of dark and camouflaged genes: disease-relevant genes hiding in plain sight. bioRxiv, 514497. Foster, E. D., & Deardorff, A. (2017). Open science framework (OSF). Journal of the Medical Library Association: JMLA, 105(2), 203. Kannan, S., Hui, J., Mazooji, K., Pachter, L., & Tse, D. (2016). Shannon: An information-optimal de novo rna-seq assembler. bioRxiv, 039230. Keeling, P. J., Burki, F., Wilcox, H. M., Allam, B., Allen, E. E., Amaral-Zettler, L. A., ... & Beszteri, B. (2014). The Marine Microbial Eukaryote Transcriptome Sequencing Project (MMETSP): illuminating the functional diversity of eukaryotic life in the oceans through transcriptome sequencing. PLoS biology, 12(6), e1001889. Grabherr, M. G., Haas, B. J., Yassour, M., Levin, J. Z., Thompson, D. A., Amit, I., ... & Chen, Z. (2011). Trinity: reconstructing a full-length transcriptome without a genome from RNA-Seq data. Nature biotechnology, 29(7), 644. Johnson, Lisa K., Alexander, Harriet, & Brown, C. Titus. (2018). Re-assembly, quality evaluation, and annotation of 678 microbial eukaryotic reference transcriptomes, GigaScience, giy158, https://doi.org/10.1093/gigascience/giy158 Johnson, Lisa K., Alexander, Harriet, & Brown, C. Titus. (2018). MMETSP re-assemblies [Data set]. Zenodo. https://doi.org/10.5281/zenodo.740440 Kanehisa, M., & Goto, S. (2000). KEGG: kyoto encyclopedia of genes and genomes. Nucleic acids research, 28(1), 27-30. Love MI, Huber W, Anders S. 2014. Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. GenomeBiology. 15:550. MacManes, M. D. (2018). The Oyster River Protocol: a multi-assembler and kmer approach for de novo transcriptome assembly. PeerJ, 6, e5428. Patro R, Duggal G, Love MI, Irizarry RA, Kingsford C. 2017. Salmon: fast and bias-aware quantification of transcript expression using dual-phase inference. Nature Methods. 14(4): 417-419. Rodgers, R., Roach, J. L., Reid, N. M., Whitehead, A., & Duvernell, D. D. (2018). Phylogenomic analysis of Fundulidae (Teleostei: Cyprinodotiformes) using RNA-sequencing data. Molecular phylogenetics and evolution, 121, 150-157. Robertson, G., Schein, J., Chiu, R., Corbett, R., Field, M., Jackman, S. D., ... & Griffith, M. (2010). De novo assembly and analysis of RNA-seq data. Nature methods, 7(11), 909. Scott C, dammit: an open and accessible de novo transcriptome annotator; 2016. In prep. Available at: https://dib-lab.github.io/dammit/. Accessed 15 Jan 2019. Simo, F. A., Waterhouse, R. M., Ioannidis, P., Kriventseva, E. V., & Zdobnov, E. M. (2015). BUSCO: assessing genome assembly and annotation completeness with single-copy orthologs. Bioinformatics, 31(19), 3210-3212. Tan, J., Hammond, J. H., Hogan, D. A., & Greene, C. S. (2016). ADAGE-based integration of publicly available Pseudomonas aeruginosa gene expression data with denoising autoencoders illuminates microbe-host interactions. MSystems, 1(1), e00025-15. Tan MH, Austin CM, Hammer MP, lee YP, Croft LJ, Gan HM. 2018. Finding nemo: hybrid assembly with Oxford Nanopore and Illumina reads greatly improves the clownfish (Amphiprion ocellaris) genome assembly. GigaScience. 7(3): gix137. https://doi.org/10.1093/gigascience/gix137 Vincent, P., Larochelle, H., Bengio, Y., & Manzagol, P. A. (2008, July). Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning (pp. 1096-1103). ACM. Whitehead, A. (2010). The evolutionary radiation of diverse osmotolerant physiologies in killifish (Fundulus sp.). Evolution: International Journal of Organic Evolution, 64(7), 2070-2085. Zimin, A. V., Marais, G., Puiu, D., Roberts, M., Salzberg, S. L., & Yorke, J. A. (2013). The MaSuRCA genome assembler. Bioinformatics, 29(21), 2669-2677.","tags":"Proposals","url":"https://johnsolk.github.io/blog/xrac-research-proposal.html"},{"title":"New vs. Old versions of Trinity","text":"Does the newer version of Trinity (v2.8.4) make better transcriptome assemblies (for killifish) than the old (v2.2.0)? tl;dr Yes. XSEDE Computing Resources Since I'm overdrawn on my new XSEDE allocation on the PSC Bridges LM (large memory) partition after receiving it on 11/26/2018, a report is required to apply for more. In case you couldn't tell, I'm a really big fan of the PSC Bridges hpc! I've used 1862 SU out of the 1000 SU they allocated to me on the LM partition and 11484 out of 13000 SU on the RM (regular memory). In case it might benefit others, here is roughly the progress report that I will submit with my request. Re-assembling 17 Fundulus killifish transcriptomes Since receiving my allocation to the PSC Bridges LM partition on 11/26/2018, I have re-assembled 17 transcriptomes from 16 species of Fundulus killifish using the Trinity de novo transcriptome assembler version 2.8.4. I originally assembled transcriptomes from the 16 Fundulus species using Trinity version 2.2.0. There is some evidence that assemblies can be improved by using the updated versions of Trinity. I wanted to re-assemble these killifish transcriptomes to see if the assemblies could be improved. These transcriptomes will be used for the purpose of studying the history of adaptation to different salinities . Some Fundulus species can tolerate a range of salinities (euryhaline) by switching osmoregulatory mechanisms while others require a more narrow salinity range (stenohaline) in either fresh or marine waters. This large set of RNAseq data from multiple species is going to help us better characterize the divergence of these molecular osmoregulatory mechanisms between euryhaline and stenohaline freshwater species. To that end, I am attempting to develop a pipeline for orthologous gene expression profiling analysis across species . This profiling analysis will enable us to compare expression patterns of salinity-responsive genes, e.g. aquaporin 3 (shown below) across clades of species. The nature of the fragmented assembly output from Trinity makes it especially challenging in our attempt to develop this pipeline. We want to make sure that the reference transcriptomes are the best that they can be so that expression profiles are accurately reflected across species. Are there differences between the v2.2.0 and v2.8.4 assemblies? Yes. Are they better? It seems so. But, there are a few items to investigate further. Below are comparisons of various evaluation metrics between the 17 old v2.2.0 (blue) and new v2.8.4 (green) Trinity assemblies. On the left are slope graphs comparing the metric between both assemblies and on the right are split violin plots showing the distributions of all the assemblies. Shout out to Dr. Harriet Alexander for helping with the code to visualize these types of comparisons for our paper comparing re-assemblies for the MMETSP - coming out soon in GigaScience ! Code for making these plots is here . There are fewer contigs. However, the large numbers >100,000 contigs indicates that these assemblies are still very fragmented. Orthogroup/ortholog prediction is required for downstream analysis. The BUSCO scores are higher. What makes these assemblies especially appealing is their higher scores with the Benchmarking Universal Single-Copy Ortholog (BUSCO) assessment tool. One species, Lucania goodei had 100%! The distribution of scores is tighter for the new assemblies compared to the old. Lower % ORF? For some reason, mean % ORF decreases. This was not expected. Number of contigs with ORF This is roughly the same and a very low number. Similar unique k-mers The number of unique k-mers (k=25) does not really change. This means that the content is similar. Conditional Reciprocal Best Blast Using transrate --assembly reference mode to examine the comparative metrics with Conditional Reciprocal Best BLAST (CRBB) between assemblies consistently did not work, for some reason. (Whereas it did work for comparison with the NCBI version of the Fundulus heteroclitus sister species ) This requires further investigation. Version: Transrate v1.0.3 by Richard Smith-Unna, Chris Boursnell, Rob Patro, Julian Hibberd, and Steve Kelly Command: transrate --assembly=/pylon5/bi5fpmp/ljcohen/kfish_trinity/F_parvapinis.trinity_out.Trinity.fasta --reference=/pylon5/bi5fpmp/ljcohen/kfish_assemblies_old/F_parvapinis.trinity_out.Trinity.fasta --threads=8 --output=/pylon5/bi5fpmp/ljcohen/kfish_transrate/F_parvapinis_trinity_v_old/ Output: [ INFO] 2018-12-06 22:23:47 : Loading assembly: /pylon5/bi5fpmp/ljcohen/kfish_trinity/F_parvapinis.trinity_out.Trinity.fasta [ INFO] 2018-12-06 22:24:45 : Analysing assembly: /pylon5/bi5fpmp/ljcohen/kfish_trinity/F_parvapinis.trinity_out.Trinity.fasta [ INFO] 2018-12-06 22:24:45 : Results will be saved in /pylon5/bi5fpmp/ljcohen/kfish_transrate/F_parvapinis_trinity_v_old/F_parvapinis.trinity_out.Trinity [ INFO] 2018-12-06 22:24:45 : Calculating contig metrics... [ INFO] 2018-12-06 22:25:37 : Contig metrics: [ INFO] 2018-12-06 22:25:37 : ----------------------------------- [ INFO] 2018-12-06 22:25:37 : n seqs 298549 [ INFO] 2018-12-06 22:25:37 : smallest 183 [ INFO] 2018-12-06 22:25:37 : largest 27771 [ INFO] 2018-12-06 22:25:37 : n bases 310786992 [ INFO] 2018-12-06 22:25:37 : mean len 1040.98 [ INFO] 2018-12-06 22:25:37 : n under 200 15 [ INFO] 2018-12-06 22:25:37 : n over 1k 77121 [ INFO] 2018-12-06 22:25:37 : n over 10k 802 [ INFO] 2018-12-06 22:25:37 : n with orf 62453 [ INFO] 2018-12-06 22:25:37 : mean orf percent 43.26 [ INFO] 2018-12-06 22:25:37 : n90 340 [ INFO] 2018-12-06 22:25:37 : n70 1141 [ INFO] 2018-12-06 22:25:37 : n50 2512 [ INFO] 2018-12-06 22:25:37 : n30 4111 [ INFO] 2018-12-06 22:25:37 : n10 6966 [ INFO] 2018-12-06 22:25:37 : gc 0.46 [ INFO] 2018-12-06 22:25:37 : bases n 0 [ INFO] 2018-12-06 22:25:37 : proportion n 0.0 [ INFO] 2018-12-06 22:25:37 : Contig metrics done in 52 seconds [ INFO] 2018-12-06 22:25:37 : No reads provided, skipping read diagnostics [ INFO] 2018-12-06 22:25:37 : Calculating comparative metrics... [ INFO] 2018-12-06 23:23:24 : Comparative metrics: [ INFO] 2018-12-06 23:23:24 : ----------------------------------- [ INFO] 2018-12-06 23:23:24 : CRBB hits 0 [ INFO] 2018-12-06 23:23:24 : n contigs with CRBB 0 [ INFO] 2018-12-06 23:23:24 : p contigs with CRBB 0.0 [ INFO] 2018-12-06 23:23:24 : rbh per reference 0.0 [ INFO] 2018-12-06 23:23:24 : n refs with CRBB 0 [ INFO] 2018-12-06 23:23:24 : p refs with CRBB 0.0 [ INFO] 2018-12-06 23:23:24 : cov25 0 [ INFO] 2018-12-06 23:23:24 : p cov25 0.0 [ INFO] 2018-12-06 23:23:24 : cov50 0 [ INFO] 2018-12-06 23:23:24 : p cov50 0.0 [ INFO] 2018-12-06 23:23:24 : cov75 0 [ INFO] 2018-12-06 23:23:24 : p cov75 0.0 [ INFO] 2018-12-06 23:23:24 : cov85 0 [ INFO] 2018-12-06 23:23:24 : p cov85 0.0 [ INFO] 2018-12-06 23:23:24 : cov95 0 [ INFO] 2018-12-06 23:23:24 : p cov95 0.0 [ INFO] 2018-12-06 23:23:24 : reference coverage 0.0 [ INFO] 2018-12-06 23:23:24 : Comparative metrics done in 3467 seconds [ INFO] 2018-12-06 23:23:24 : ----------------------------------- [ INFO] 2018-12-06 23:23:24 : Writing contig metrics for each contig to /pylon5/bi5fpmp/ljcohen/kfish_transrate/F_parvapinis_trinity_v_old/F_parvapinis.trinity_out.Trinity/contigs.csv [ INFO] 2018-12-06 23:23:43 : Writing analysis results to assemblies.csv Conclusion Trinity v2.8.4 is better than v2.2.0. While v2.8.4 still produces very fragmented assemblies, the higher BUSCO content is exciting. There are questions requiring further investigation Why, if the unique k-mer content is similiar, would the BUSCO scores improve between versions? ORF content (number of contigs with ORF and mean ORF %) are parodoxical. Why would the ORF content decrease in the newer assemblies? Why wouldn't transrate ---reference work to get CRBB metrics between these assemblies? What has improved in Trinity v2.8.4? According to the release notes from Trinity, major improvements have included using salmon expression quantification to help with filtering out assembly artifacts and overhauling the \"supertranscript module\" to deal with high polymorphism situations. Since killifish are highly heterozygous, we are likely benefitting from these improvements. Future directions In addition to annotating these transcritomes and continuing to investigate the questions above, I would also like to try Matt MacManes' Oyster River Protocol and accompanying 'orthofuser' script to combine contigs from different assemblers.","tags":"transcriptome","url":"https://johnsolk.github.io/blog/new-vs-old-versions-of-trinity.html"},{"title":"Explorations with Orthofinder","text":"Here's my weekend exploration of Orthofinder with amino acid translations from de novo transcriptome assemblies of 17 species of killifish . Translations of Trinity de novo transcirptomes were done with transdecoder by the dammit annotation pipeline . Orthofinder manual and a preprint on biorxiv . Found this via a tweet , and tweet recommendation . Here's the script that I ran, submitted to the barbera cluster at UC Davis (which took ~1 week to get through the queue over Thanksgiving break): johnsolk@barbera:/share/pilot-johnsolk$ cat orthofinder.sbatch #!/bin/bash -l #SBATCH -J orthofinder_kfish #SBATCH -t 72:00:00 #SBATCH -c 8 #SBATCH -p production #SBATCH --mem=48000 aklog source ~/.bashrc source activate py27 orthofinder -f killifish_RNAseq_fasta -S diamond Output files: johnsolk@barbera:/share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26$ ls -lah total 388M drwxrwxr-x 4 johnsolk johnsolk 12 Nov 27 16:50 . drwxrwxr-x 5 johnsolk johnsolk 22 Nov 26 11:23 .. -rw-rw-r-- 1 johnsolk johnsolk 411M Nov 27 16:49 Orthogroups.csv -rw-rw-r-- 1 johnsolk johnsolk 6.1M Nov 27 16:49 Orthogroups.GeneCount.csv -rw-rw-r-- 1 johnsolk johnsolk 3.6K Nov 27 16:50 Orthogroups_SpeciesOverlaps.csv -rw-rw-r-- 1 johnsolk johnsolk 449M Nov 27 16:48 Orthogroups.txt -rw-rw-r-- 1 johnsolk johnsolk 45M Nov 27 16:49 Orthogroups_UnassignedGenes.csv drwxrwxr-x 6 johnsolk johnsolk 14 Nov 28 10:35 Orthologues_Nov27 -rw-rw-r-- 1 johnsolk johnsolk 69 Nov 27 16:50 SingleCopyOrthogroups.txt -rw-rw-r-- 1 johnsolk johnsolk 1.7K Nov 27 16:50 Statistics_Overall.csv -rw-rw-r-- 1 johnsolk johnsolk 9.7K Nov 27 16:50 Statistics_PerSpecies.csv drwxrwxr-x 2 johnsolk johnsolk 330 Nov 27 16:47 WorkingDirectory Orthofinder workflow : Orthologues directory: johnsolk@barbera:/share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthologues_Nov27$ ls -lah total 462M drwxrwxr-x 6 johnsolk johnsolk 14 Nov 28 10:35 . drwxrwxr-x 4 johnsolk johnsolk 12 Nov 27 16:50 .. -rw-rw-r-- 1 johnsolk johnsolk 2.2G Nov 28 10:35 Duplications.csv drwxrwxr-x 2 johnsolk johnsolk 93K Nov 28 03:09 Gene_Trees drwxrwxr-x 20 johnsolk johnsolk 20 Nov 28 03:10 Orthologues -rw-rw-r-- 1 johnsolk johnsolk 4.1K Nov 28 10:35 OrthologuesStats_many-to-many.csv -rw-rw-r-- 1 johnsolk johnsolk 4.1K Nov 28 10:35 OrthologuesStats_many-to-one.csv -rw-rw-r-- 1 johnsolk johnsolk 3.9K Nov 28 10:35 OrthologuesStats_one-to-many.csv -rw-rw-r-- 1 johnsolk johnsolk 4.1K Nov 28 10:35 OrthologuesStats_one-to-one.csv -rw-rw-r-- 1 johnsolk johnsolk 4.1K Nov 28 10:35 OrthologuesStats_Totals.csv drwxrwxr-x 2 johnsolk johnsolk 93K Nov 28 10:35 Recon_Gene_Trees -rw-rw-r-- 1 johnsolk johnsolk 1.3K Nov 28 03:10 SpeciesTree_rooted_node_labels.txt -rw-rw-r-- 1 johnsolk johnsolk 1.4K Nov 28 03:10 SpeciesTree_rooted.txt drwxrwxr-x 5 johnsolk johnsolk 8 Nov 28 10:37 WorkingDirectory Trees Files are in newick format ( Wiki ), install Dendroscope to view. Orthologue output Orthologues are located here, these are directories (one for each species): johnsolk@barbera:/share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthologues_Nov27/Orthologues$ ls -lah total 1.4M drwxrwxr-x 20 johnsolk johnsolk 20 Nov 28 03:10 . drwxrwxr-x 6 johnsolk johnsolk 14 Nov 28 10:35 .. drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_A_xenica.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_catanatus.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_chrysotus.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_diaphanus.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_grandis.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_heteroclitusMDPL.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_heteroclitusMDPP.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_notatus.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_notti.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_olivaceous.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:12 Orthologues_F_parvapinis.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_rathbuni.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_sciadicus.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_similis.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_F_zebrinus.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_L_goodei.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 Orthologues_L_parva.trinity_out.Trinity.fasta.transdecoder.pep drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:10 Putative_Xenologues johnsolk@barbera:/share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthologues_Nov27/Orthologues$ pwd /share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthologues_Nov27/Orthologues These are for A. xenica : johnsolk@barbera:/share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthologues_Nov27/Orthologues/Orthologues_A_xenica.trinity_out.Trinity.fasta.transdecoder.pep$ ls -lah total 180M drwxrwxr-x 2 johnsolk johnsolk 19 Nov 28 03:11 . drwxrwxr-x 20 johnsolk johnsolk 20 Nov 28 03:10 .. -rw-rw-r-- 1 johnsolk johnsolk 27M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_catanatus.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 27M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_chrysotus.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 24M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_diaphanus.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 29M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_grandis.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 26M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_heteroclitusMDPL.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 26M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_heteroclitusMDPP.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 28M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_notatus.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 17M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_notti.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 26M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_olivaceous.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 24M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_parvapinis.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 29M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_rathbuni.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 22M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_sciadicus.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 24M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_similis.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 21M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__F_zebrinus.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 25M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__L_goodei.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 27M Nov 28 10:35 A_xenica.trinity_out.Trinity.fasta.transdecoder.pep__v__L_parva.trinity_out.Trinity.fasta.transdecoder.pep.csv -rw-rw-r-- 1 johnsolk johnsolk 220K Nov 28 08:33 Putative_Horizontal_Gene_Transfer.txt Output: johnsolk@barbera:/share/pilot-johnsolk$ cat slurm-20077670.out Auks API request failed : krb5 cred : unable to read credential cache /bin/bash: /home/johnsolk/.bash_profile: Permission denied aklog: Couldn't determine realm of user:aklog: No credentials cache found while getting realm /var/spool/slurmd/job20077670/slurm_script: line 9: /home/johnsolk/.bashrc: Permission denied OrthoFinder version 2.2.7 Copyright (C) 2014 David Emms 2018-11-26 11:23:26 : Starting OrthoFinder 16 thread(s) for highly parallel tasks (BLAST searches etc.) 1 thread(s) for OrthoFinder algorithm Checking required programs are installed ---------------------------------------- Test can run \"mcl -h\" - ok Test can run \"fastme -i /share/pilot-johnsolk/killifish_RNAseq_fasta/SimpleTest.phy -o /share/pilot-johnsolk/killifish_RNAseq_fasta/SimpleTest.tre\" - ok Dividing up work for BLAST for parallel processing -------------------------------------------------- 2018-11-26 11:24:53 : Creating diamond database 1 of 17 2018-11-26 11:24:56 : Creating diamond database 2 of 17 2018-11-26 11:24:59 : Creating diamond database 3 of 17 2018-11-26 11:25:01 : Creating diamond database 4 of 17 2018-11-26 11:25:03 : Creating diamond database 5 of 17 2018-11-26 11:25:06 : Creating diamond database 6 of 17 2018-11-26 11:25:09 : Creating diamond database 7 of 17 2018-11-26 11:25:11 : Creating diamond database 8 of 17 2018-11-26 11:25:14 : Creating diamond database 9 of 17 2018-11-26 11:25:15 : Creating diamond database 10 of 17 2018-11-26 11:25:18 : Creating diamond database 11 of 17 2018-11-26 11:25:20 : Creating diamond database 12 of 17 2018-11-26 11:25:24 : Creating diamond database 13 of 17 2018-11-26 11:25:26 : Creating diamond database 14 of 17 2018-11-26 11:25:28 : Creating diamond database 15 of 17 2018-11-26 11:25:30 : Creating diamond database 16 of 17 2018-11-26 11:25:32 : Creating diamond database 17 of 17 Running diamond all-versus-all ------------------------------ Using 16 thread(s) 2018-11-26 11:25:35 : This may take some time.... 2018-11-27 01:40:54 : Done 200 of 289 2018-11-26 19:56:13 : Done 100 of 289 2018-11-26 11:25:35 : Done 0 of 289 2018-11-27 05:34:15 : Done all-versus-all sequence search Running OrthoFinder algorithm ----------------------------- 2018-11-27 05:34:33 : Initial processing of each species 2018-11-27 05:56:01 : Initial processing of species 0 complete 2018-11-27 06:19:34 : Initial processing of species 1 complete 2018-11-27 06:42:17 : Initial processing of species 2 complete 2018-11-27 07:00:28 : Initial processing of species 3 complete 2018-11-27 07:26:45 : Initial processing of species 4 complete 2018-11-27 07:48:38 : Initial processing of species 5 complete 2018-11-27 08:10:19 : Initial processing of species 6 complete 2018-11-27 08:34:54 : Initial processing of species 7 complete 2018-11-27 08:45:45 : Initial processing of species 8 complete 2018-11-27 09:07:35 : Initial processing of species 9 complete 2018-11-27 09:27:08 : Initial processing of species 10 complete 2018-11-27 09:52:50 : Initial processing of species 11 complete 2018-11-27 10:08:33 : Initial processing of species 12 complete 2018-11-27 10:27:23 : Initial processing of species 13 complete 2018-11-27 10:42:00 : Initial processing of species 14 complete 2018-11-27 11:00:31 : Initial processing of species 15 complete 2018-11-27 11:25:14 : Initial processing of species 16 complete 2018-11-27 11:59:32 : Connected putatitive homologs [mcl] cut <61> instances of overlap 2018-11-27 16:47:54 : Ran MCL Writing orthogroups to file --------------------------- A duplicate accession was found using just first part: Transcript_74|m.19 Tried to use only the first part of the accession in order to list the sequences in each orthogroup more concisely but these were not unique. The full accession line will be used instead. Orthogroups have been written to tab-delimited files: /share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthogroups.csv /share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthogroups.txt (OrthoMCL format) /share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthogroups_UnassignedGenes.csv 2018-11-27 16:50:24 : Done orthogroups Analysing Orthogroups ===================== Calculating gene distances -------------------------- 2018-11-27 23:24:44 : Done 0 of 95230 2018-11-27 23:36:14 : Done 1000 of 95230 2018-11-27 23:38:41 : Done 2000 of 95230 2018-11-27 23:40:18 : Done 3000 of 95230 2018-11-27 23:41:34 : Done 4000 of 95230 2018-11-27 23:42:36 : Done 5000 of 95230 2018-11-27 23:43:31 : Done 6000 of 95230 2018-11-27 23:44:19 : Done 7000 of 95230 2018-11-27 23:45:03 : Done 8000 of 95230 2018-11-27 23:45:44 : Done 9000 of 95230 2018-11-27 23:46:21 : Done 10000 of 95230 2018-11-27 23:46:58 : Done 11000 of 95230 2018-11-27 23:47:32 : Done 12000 of 95230 2018-11-27 23:48:04 : Done 13000 of 95230 2018-11-27 23:48:32 : Done 14000 of 95230 2018-11-27 23:49:01 : Done 15000 of 95230 2018-11-27 23:49:27 : Done 16000 of 95230 2018-11-27 23:49:53 : Done 17000 of 95230 2018-11-27 23:50:18 : Done 18000 of 95230 2018-11-27 23:50:43 : Done 19000 of 95230 2018-11-27 23:51:07 : Done 20000 of 95230 2018-11-27 23:51:29 : Done 21000 of 95230 2018-11-27 23:51:49 : Done 22000 of 95230 2018-11-27 23:52:09 : Done 23000 of 95230 2018-11-27 23:52:28 : Done 24000 of 95230 2018-11-27 23:52:47 : Done 25000 of 95230 2018-11-27 23:53:07 : Done 26000 of 95230 2018-11-27 23:53:26 : Done 27000 of 95230 2018-11-27 23:53:44 : Done 28000 of 95230 2018-11-27 23:54:01 : Done 29000 of 95230 2018-11-27 23:54:19 : Done 30000 of 95230 2018-11-27 23:54:37 : Done 31000 of 95230 2018-11-27 23:54:54 : Done 32000 of 95230 2018-11-27 23:55:14 : Done 33000 of 95230 2018-11-27 23:55:32 : Done 34000 of 95230 2018-11-27 23:55:50 : Done 35000 of 95230 2018-11-27 23:56:06 : Done 36000 of 95230 2018-11-27 23:56:22 : Done 37000 of 95230 2018-11-27 23:56:37 : Done 38000 of 95230 2018-11-27 23:56:53 : Done 39000 of 95230 2018-11-27 23:57:09 : Done 40000 of 95230 2018-11-27 23:57:25 : Done 41000 of 95230 2018-11-27 23:57:41 : Done 42000 of 95230 2018-11-27 23:57:58 : Done 43000 of 95230 2018-11-27 23:58:15 : Done 44000 of 95230 2018-11-27 23:58:31 : Done 45000 of 95230 2018-11-27 23:58:47 : Done 46000 of 95230 2018-11-27 23:59:02 : Done 47000 of 95230 2018-11-27 23:59:18 : Done 48000 of 95230 2018-11-27 23:59:35 : Done 49000 of 95230 2018-11-27 23:59:51 : Done 50000 of 95230 2018-11-28 00:00:06 : Done 51000 of 95230 2018-11-28 00:00:21 : Done 52000 of 95230 2018-11-28 00:00:37 : Done 53000 of 95230 2018-11-28 00:00:52 : Done 54000 of 95230 2018-11-28 00:01:07 : Done 55000 of 95230 2018-11-28 00:01:23 : Done 56000 of 95230 2018-11-28 00:01:40 : Done 57000 of 95230 2018-11-28 00:01:56 : Done 58000 of 95230 2018-11-28 00:02:11 : Done 59000 of 95230 2018-11-28 00:02:29 : Done 60000 of 95230 2018-11-28 00:02:47 : Done 61000 of 95230 2018-11-28 00:03:07 : Done 62000 of 95230 2018-11-28 00:03:27 : Done 63000 of 95230 2018-11-28 00:03:48 : Done 64000 of 95230 2018-11-28 00:04:09 : Done 65000 of 95230 2018-11-28 00:04:31 : Done 66000 of 95230 2018-11-28 00:04:52 : Done 67000 of 95230 2018-11-28 00:05:13 : Done 68000 of 95230 2018-11-28 00:05:35 : Done 69000 of 95230 2018-11-28 00:05:55 : Done 70000 of 95230 2018-11-28 00:06:14 : Done 71000 of 95230 2018-11-28 00:06:34 : Done 72000 of 95230 2018-11-28 00:06:55 : Done 73000 of 95230 2018-11-28 00:07:16 : Done 74000 of 95230 2018-11-28 00:07:36 : Done 75000 of 95230 2018-11-28 00:07:56 : Done 76000 of 95230 2018-11-28 00:08:18 : Done 77000 of 95230 2018-11-28 00:08:39 : Done 78000 of 95230 2018-11-28 00:09:02 : Done 79000 of 95230 2018-11-28 00:09:22 : Done 80000 of 95230 2018-11-28 00:09:42 : Done 81000 of 95230 2018-11-28 00:10:03 : Done 82000 of 95230 2018-11-28 00:10:24 : Done 83000 of 95230 2018-11-28 00:10:47 : Done 84000 of 95230 2018-11-28 00:11:09 : Done 85000 of 95230 2018-11-28 00:11:29 : Done 86000 of 95230 2018-11-28 00:11:51 : Done 87000 of 95230 2018-11-28 00:12:13 : Done 88000 of 95230 2018-11-28 00:12:33 : Done 89000 of 95230 2018-11-28 00:12:55 : Done 90000 of 95230 2018-11-28 00:13:16 : Done 91000 of 95230 2018-11-28 00:13:37 : Done 92000 of 95230 2018-11-28 00:13:57 : Done 93000 of 95230 2018-11-28 00:14:19 : Done 94000 of 95230 2018-11-28 00:14:40 : Done 95000 of 95230 2018-11-27 23:24:40 : Done Inferring gene and species trees -------------------------------- 15603 trees had all species present and will be used by STAG to infer the species tree A duplicate accession was found using just first part: Transcript_74|m.19 Tried to use only the first part of the accession in order to list the sequences in each orthogroup more concisely but these were not unique. The full accession line will be used instead. Best outgroup(s) for species tree --------------------------------- 2018-11-28 03:10:02 : Starting STRIDE 2018-11-28 03:10:53 : Done STRIDE Observed 863 well-supported, non-terminal duplications. 852 support the best root and 11 contradict it. Best outgroup for species tree: L_goodei.trinity_out.Trinity.fasta.transdecoder.pep, L_parva.trinity_out.Trinity.fasta.transdecoder.pep Reconciling gene trees and species tree --------------------------------------- Outgroup: L_goodei.trinity_out.Trinity.fasta.transdecoder.pep, L_parva.trinity_out.Trinity.fasta.transdecoder.pep 2018-11-28 03:10:53 : Starting Recon and orthologues 2018-11-28 03:10:53 : Starting OF Orthologues 2018-11-28 03:11:01 : Done 0 of 95230 2018-11-28 03:29:22 : Done 1000 of 95230 2018-11-28 03:41:32 : Done 2000 of 95230 2018-11-28 03:52:16 : Done 3000 of 95230 2018-11-28 04:02:05 : Done 4000 of 95230 2018-11-28 04:11:43 : Done 5000 of 95230 2018-11-28 04:21:05 : Done 6000 of 95230 2018-11-28 04:29:57 : Done 7000 of 95230 2018-11-28 04:38:22 : Done 8000 of 95230 2018-11-28 04:46:36 : Done 9000 of 95230 2018-11-28 04:54:32 : Done 10000 of 95230 2018-11-28 05:02:18 : Done 11000 of 95230 2018-11-28 05:09:55 : Done 12000 of 95230 2018-11-28 05:17:17 : Done 13000 of 95230 2018-11-28 05:24:32 : Done 14000 of 95230 2018-11-28 05:31:34 : Done 15000 of 95230 2018-11-28 05:38:29 : Done 16000 of 95230 2018-11-28 05:45:15 : Done 17000 of 95230 2018-11-28 05:52:01 : Done 18000 of 95230 2018-11-28 05:58:46 : Done 19000 of 95230 2018-11-28 06:05:27 : Done 20000 of 95230 2018-11-28 06:11:59 : Done 21000 of 95230 2018-11-28 06:17:55 : Done 22000 of 95230 2018-11-28 06:24:06 : Done 23000 of 95230 2018-11-28 06:29:51 : Done 24000 of 95230 2018-11-28 06:35:17 : Done 25000 of 95230 2018-11-28 06:40:36 : Done 26000 of 95230 2018-11-28 06:45:52 : Done 27000 of 95230 2018-11-28 06:50:55 : Done 28000 of 95230 2018-11-28 06:55:59 : Done 29000 of 95230 2018-11-28 07:00:50 : Done 30000 of 95230 2018-11-28 07:05:44 : Done 31000 of 95230 2018-11-28 07:10:23 : Done 32000 of 95230 2018-11-28 07:14:57 : Done 33000 of 95230 2018-11-28 07:19:32 : Done 34000 of 95230 2018-11-28 07:24:08 : Done 35000 of 95230 2018-11-28 07:28:38 : Done 36000 of 95230 2018-11-28 07:32:55 : Done 37000 of 95230 2018-11-28 07:37:05 : Done 38000 of 95230 2018-11-28 07:41:05 : Done 39000 of 95230 2018-11-28 07:45:02 : Done 40000 of 95230 2018-11-28 07:48:54 : Done 41000 of 95230 2018-11-28 07:52:39 : Done 42000 of 95230 2018-11-28 07:56:18 : Done 43000 of 95230 2018-11-28 08:00:01 : Done 44000 of 95230 2018-11-28 08:03:32 : Done 45000 of 95230 2018-11-28 08:07:08 : Done 46000 of 95230 2018-11-28 08:10:37 : Done 47000 of 95230 2018-11-28 08:14:11 : Done 48000 of 95230 2018-11-28 08:17:40 : Done 49000 of 95230 2018-11-28 08:21:08 : Done 50000 of 95230 2018-11-28 08:24:28 : Done 51000 of 95230 2018-11-28 08:27:52 : Done 52000 of 95230 2018-11-28 08:31:12 : Done 53000 of 95230 2018-11-28 08:34:27 : Done 54000 of 95230 2018-11-28 08:37:47 : Done 55000 of 95230 2018-11-28 08:40:57 : Done 56000 of 95230 2018-11-28 08:44:03 : Done 57000 of 95230 2018-11-28 08:47:14 : Done 58000 of 95230 2018-11-28 08:50:24 : Done 59000 of 95230 2018-11-28 08:53:25 : Done 60000 of 95230 2018-11-28 08:56:33 : Done 61000 of 95230 2018-11-28 08:59:42 : Done 62000 of 95230 2018-11-28 09:02:51 : Done 63000 of 95230 2018-11-28 09:05:54 : Done 64000 of 95230 2018-11-28 09:08:57 : Done 65000 of 95230 2018-11-28 09:12:00 : Done 66000 of 95230 2018-11-28 09:15:01 : Done 67000 of 95230 2018-11-28 09:18:00 : Done 68000 of 95230 2018-11-28 09:20:56 : Done 69000 of 95230 2018-11-28 09:23:48 : Done 70000 of 95230 2018-11-28 09:26:43 : Done 71000 of 95230 2018-11-28 09:29:36 : Done 72000 of 95230 2018-11-28 09:32:30 : Done 73000 of 95230 2018-11-28 09:35:23 : Done 74000 of 95230 2018-11-28 09:38:15 : Done 75000 of 95230 2018-11-28 09:41:06 : Done 76000 of 95230 2018-11-28 09:44:01 : Done 77000 of 95230 2018-11-28 09:46:57 : Done 78000 of 95230 2018-11-28 09:49:53 : Done 79000 of 95230 2018-11-28 09:52:48 : Done 80000 of 95230 2018-11-28 09:55:42 : Done 81000 of 95230 2018-11-28 09:58:35 : Done 82000 of 95230 2018-11-28 10:01:28 : Done 83000 of 95230 2018-11-28 10:04:15 : Done 84000 of 95230 2018-11-28 10:07:00 : Done 85000 of 95230 2018-11-28 10:09:47 : Done 86000 of 95230 2018-11-28 10:12:32 : Done 87000 of 95230 2018-11-28 10:15:18 : Done 88000 of 95230 2018-11-28 10:18:09 : Done 89000 of 95230 2018-11-28 10:20:57 : Done 90000 of 95230 2018-11-28 10:23:45 : Done 91000 of 95230 2018-11-28 10:26:34 : Done 92000 of 95230 2018-11-28 10:29:21 : Done 93000 of 95230 2018-11-28 10:32:06 : Done 94000 of 95230 2018-11-28 10:34:49 : Done 95000 of 95230 2018-11-28 10:35:27 : Done OF Orthologues 2018-11-28 10:35:50 : Done Recon Writing results files ===================== 2018-11-28 10:37:28 : Done orthologues Orthogroups have been written to tab-delimited files: /share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthogroups.csv /share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthogroups.txt (OrthoMCL format) /share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthogroups_UnassignedGenes.csv Gene trees: /share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthologues_Nov27/Gene_Trees/ Rooted species tree: /share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthologues_Nov27/SpeciesTree_rooted.txt Species-by-species orthologues: /share/pilot-johnsolk/killifish_RNAseq_fasta/Results_Nov26/Orthologues_Nov27/Orthologues/ Orthogroup statistics: Statistics_PerSpecies.csv Statistics_Overall.csv Orthogroups_SpeciesOverlaps.csv OrthoFinder assigned 2542304 genes (91.4% of total) to 133965 orthogroups. Fifty percent of all genes were in orthogroups with 43 or more genes (G50 was 43) and were contained in the largest 18142 orthogroups (O50 was 18142). There were 15603 orthogroups with all species present and 7 of these consisted entirely of single-copy genes. CITATION: When publishing work that uses OrthoFinder please cite: Emms D.M. & Kelly S. (2015), Genome Biology 16:157 If you use the species tree in your work then please also cite: Emms D.M. & Kelly S. (2017), MBE 34(12): 3267-3278 Emms D.M. & Kelly S. (2018), bioRxiv https://doi.org/10.1101/267914 Next step, figure out what it all means!","tags":"Orthologues","url":"https://johnsolk.github.io/blog/explorations-with-orthofinder.html"},{"title":"De novo transcriptome assembly, annotation and evaluation of the branching tube Caribbean sponge, Aiolochroia crassa","text":"Authors: Lisa K. Johnson and Sara Edge This blog post was inspired by attending the Global Invertebrate Genomics Alliance (GIGA) conference in Curaao (Oct. 19-21, 2018). There is a paucity of invertebrate sequencing information available , especially for Poriferans. Invertebrate genomes and transcriptomes allow us to better understand global biodiversity towards conservation efforts. This is raw RNA-seq data and an assembled transcriptome that has been sitting on servers since 2013. We want to release it so that perhaps this can help anyone who is interested. Writing manuscripts for publication is time consuming. Submitting raw reads and transcriptomes to ENA or NCBI, SRA and TSA is also time consuming and confusing. Since this is an old project that is not related to my dissertation research, instead of spending valuable research time on these efforts, we make these data and information available. Link to the data: https://doi.org/10.17605/OSF.IO/B972E Citation: Johnson, L., & Edge, S. (2018, August 13). De novo transcriptome of the branching tube Caribbean sponge, Aiolochroia crassa. https://doi.org/10.17605/OSF.IO/B972E Eventually, we would like to write and submit. GIGA provides a central resource for submitting these data as well. I should probably release these data there as well. In the meantime, here you go! If you are interested, please get in touch. Advice and comments welcome below for the time when we do work towards a publishable manuscript. In particular, what additional information, analysis and discussion would be needed with these data that would be required for an original and useful publication in this field? Special thanks This work was done at FAU-HBOI in 2012 when I worked for the Robertson Coral Reef Research Program . Ameer Tohamy and Thomas Camacho provided assistance while conducting the exposure experiment in 2012. Dr. Joe Lopez provided logistics and assistance in arranging for the NSEU diving boat for sample collections. Dr. Shirley Pomponi for sample identification and consultation with the experiment. Dr. Joshua Voss provided support while finishing out the project in 2013. Funding for this project and sequencing was provided to Dr. Sara Edge through the FAU-HBOI Foundation 'Save Our Seas' specialty license plate funds awarded in June 2011. Thank you to the Moore Foundation Data-Driven-Discovery award to my advisor, Dr. C. Titus Brown at UC Davis, for funding my time while I am currently working on my PhD, towards projects related to open science, reproducibility and data sharing. Thank you to Titus and the DIB lab for advice, support and development of the Eel Pond Protocol , old version and new (with snakemake, by Dr. Tessa Pierce . Summary Aiolochroia crassa is a common sponge species found on FL reefs Samples from six colonies of Aiolochroia crassa collected off Fort Lauderdale, FL in 2012 Samples were fragmented and exposed (in a balanced experimental design) to an acute 40 hrs challenge in 100 ppm crude WAF (Water accomodated fraction) oil and 10 ppm dispersant (Corexit 9500A, Nalco) Sponge cDNA samples were hybridized to a cDNA coral microarray (n=72) RNAseq data from pooled samples (n=18) in each treatment group (4 groups) De novo transcriptome assembly and annotation was performed using the eel pond protocol , files available Download all files via commandline: pip install osfclient osf -p b972e clone A_crassa This sequencing effort adds to the sparse sequencing information available from this species Introduction Porifera are among the major phyla inhabiting marine hard substrate benthos, in terms of number of species and biomass (Carballo et al. 1996). As sessile filter-feeders, sponges are sensitive to stressors and thus are important indicator species of environmental perturbations (Carballo et al., 1996; Bachinski et al., 1997). Gene expression profiling has been used as a method for identifying suites of genes in functional pathways expressed under certain controlled conditions in reef-building corals (Edge et al. 2005) and verified as a tool to detect similar sub-lethal responses from ex situ environmental coral samples (Edge et al. 2012, Edge et al. 2008). Applying similar gene expression profiling methods to sponges under controlled conditions, such as exposure to oil and dispersant, could assist resource managers in monitoring the effects of these stressors on reef sponges. The experiment aimed to assess the responses of a single sponge species following exposure to oil with a similar composition to the MC252 Macondo wellhead after the Deepwater Horizon event and to the dispersant used during clean-up efforts. A reference de novo transcriptome was assembled and annotated for Aiolochroia crassa . This sequence information is the first from this species (aside from 25 NCBI nucleotide records ). TO DO: check to see if these genes are repsented in the transcriptome . Methods Fragments from six Aiolocroia crassa sponge colonies were collected from a Ft. Lauderdale reef and transported to the aquaculture facility at Harbor Branch Oceanographic Institute at Florida Atlantic University within two hours of collection (29 May 2012). Sponges were maintained in aerated raceways with filtered and UV-sterilized seawater (32 ppt, 28C) pumped from the Indian River Lagoon. Each sponge colony was cut into twelve pieces (4-6 cm2) resulting in 72 fragments. Colored plastic zip ties were used to identify sponge colonies and secure fragments to twelve 20 cm by 45 cm PVC grids (Figure 1). Treatment Description N OD oil, dispersant 18 OC oil, no dispersant 18 UD no oil, dispersant 18 UC no oil, no dispersant 18 Figure 1. Sponge fragments from each parent colony attached to PVC grid and assigned a unique colored cable tie for identification. One grid per tank x 12 tanks, four treatment groups x 3 replicates per treatment. Experimental Exposures After 24 hours of acclimation, fragmented sponges were placed into twelve 5 gallon aquaria filled with 15 L of UV-treated seawater. Aquaria were randomly distributed between two aquaculture raceways. There were four treatment groups, each replicated three times, in a randomized block experimental design. Treatments consisted of exposure to 100 ppm oil and 10 ppm dispersant (Corexit 9500A, Nalco) (OD), oil and no dispersant (OC), dispersant and no oil (UD) and a control (UC). Oil used for the experiment was a fresh oil/source sample collected by Cardno ENTRIX for British Petroleum from near the MS252 Macondo wellhead (Coordinates: 28.866014, -88.056264, Sample ID: SO-20110212-HMPA4-003) on 12 February 2011. A 3,000 ppm stock solution of oil was made by mixing with sea water and stirring continuously on low heat for 18 hrs. Oil concentration was based on previously-published experiments (Epstein et al. 2000, Dodge et al 1984, Cook and Knap 1983) and dispersant concentration was based on the Environmental Protection Agency's recommended application range (dispersant to oil ratio of 1:50 to 1:10, EPA). Environmental conditions were consistent throughout the exposures. Salinity was adjusted to 32 ppt in each tank using Instant Ocean while temperature was maintained at approximately 28C. During the exposure period, tank chemistry, including dissolved oxygen, pH, salinity, and temperature, were recorded three times daily. Additionally, 80% of the water in all aquaria was replaced and the same concentrations of oil and dispersant, or seawater only, were added 15 hours after the initial exposure. Sponges in each tank were photographed twice daily and monitored for visible signs of stress. After 40 hours, all samples were collected, preserved in TRIzol nucleic acid preservative (Invitrogen, Inc.) and frozen at -80C for molecular analyses. Microarray A custom stress-focused microarray with 1,695 scleractinian coral genes divided into 51 functional groups was used to assess gene expression profiles (Agilent Technologies). Gene functions range from normal cellular responses, such as metabolism, signaling and protein binding, to a suite of responsive genes related to biotransformation, apoptosis, DNA damage and repair, xenobiotics, drug metabolism, oxidative stress and antioxidant defense. Probes on the microarray consist of 60 base pair oligonucleotides that are representative of coral gene exon open reading frames (ORF) coding for a specific protein. The coral microarray can be used with sponge samples to test the similarity between these metazoan species. Sample processing for molecular analyses Total RNA was extracted from each sponge sample, reverse transcribed then copy RNA was synthesized and labeled with Cy3 dye following the manufacturer's protocol (Agilent Technologies, Catalogue# 5190-2305). All Cy3-labeled cRNA samples were hybridized to individual microarrays using methods described in the manufacturer's protocol (Agilent Technologies). Microarray chips were hybridized at 65C for 17 hrs then washed using solutions from Agilent. Chips were scanned and imaged at 535 nm using a high-resolution fluorescent GenePix 4200A microarray scanner (Axon, Molecular Devices, California, USA). RNA-Seq Total RNA samples from each colony were combined by treatment group to yield 10 g at a concentration of >200 ng/L. The quality and quantity of the pooled samples were verified on a Nanodrop 1000 and Agilent Bioanalyzer 2100. RNA samples were shipped on dry ice to BaseClear (Netherlands) in November 2012 for library preparation and sequencing on an Illumina HiSeq 2000. Microarray Data analyses Raw intensity data were extracted from scanned images using GenePixPro 6.0 software (Molecular Devices) and imported into the R statistical computing environment. LIMMA (Linear Models for Microarray Data) was used for microarray analysis and data quality was assessed using arrayQualityMetrics. Computational resources NSF-XSEDE Jetstream cloud computing resources were used (allocation TG-BIO160028 to LKJ) s1.xxlarge (CPU: 44, Mem: 120 GB, Disk: 480 GB, Disk: 480 GB root) Ubuntu 16.04 Devel and Docker v1.13 Trinity v2.6.6 Patch Release Eel Pond protocol Results Data files storage: OSF respository: https://osf.io/b972e/ DOI 10.17605/OSF.IO/B972E Raw reads, trimming TruSeq Poly-A library PEx50 Diginorm read 231786047 reads, 11815700653 bp wrote 67649858 reads, 3433851077 bp looked at 63961781 reads twice (1.28 passes) removed 164136189 reads and trimmed 3055905 reads (72.13%) trimmed or removed 70.94%% of bases (8381849576 total) 231786047 reads were high coverage (100.00%); skipped 0 reads/0 bases because of low coverage fp rate estimated to be 0.019 output streamed to stdout DONE; read 67649858 sequences, 32964374 pairs and 1721110 singletons wrote to: paired.fq.gz and single.fq.gz DONE; split 65928748 sequences (32964374 left, 32964374 right, 0 orphans) /1 reads in paired.fq.gz.1 /2 reads in paired.fq.gz.2 Assembly Total reads for assembly: 58,354,750 Trinity version: Trinity-v2.6.6 Statistics: =========== Trinity Version: Trinity-v2.6.6 Compiler: GCC Trinity Parameters: --left left.fq --right right.fq --seqType fq --max_memory 14G --CPU 16 Paired mode Input data Left.fasta 2903 MByte Right.fasta 2902 MByte Number of unique KMERs: 419308215 Number of reads: 58354750 Output data Trinity.fasta 80 MByte Runtime ======= Start: Sat May 5 21:19:46 EDT 2018 End: Sun May 6 08:06:56 EDT 2018 Trinity 38830 seconds Inchworm (phase 1 - read clustering) 2967 seconds Chrysalis (phase 1 - read clustering) 33642 seconds Rest (phase 2 - parallel assembly) 2221 seconds Trinity new version: [ INFO] 2018-05-06 21:52:20 : Loading assembly: /home/ljcohen/A_crassa/assembly/trinity_out_dir/Trinity.fasta [ INFO] 2018-05-06 21:52:36 : Analysing assembly: /home/ljcohen/A_crassa/assembly/trinity_out_dir/Trinity.fasta [ INFO] 2018-05-06 21:52:36 : Results will be saved in /home/ljcohen/A_crassa/assembly/A_crassa_transrate/Trinity [ INFO] 2018-05-06 21:52:36 : Calculating contig metrics... [ INFO] 2018-05-06 21:52:51 : Contig metrics: [ INFO] 2018-05-06 21:52:51 : ----------------------------------- [ INFO] 2018-05-06 21:52:51 : n seqs 119109 [ INFO] 2018-05-06 21:52:51 : smallest 201 [ INFO] 2018-05-06 21:52:51 : largest 9126 [ INFO] 2018-05-06 21:52:51 : n bases 70503595 [ INFO] 2018-05-06 21:52:51 : mean len 591.93 [ INFO] 2018-05-06 21:52:51 : n under 200 0 [ INFO] 2018-05-06 21:52:51 : n over 1k 16468 [ INFO] 2018-05-06 21:52:51 : n over 10k 0 [ INFO] 2018-05-06 21:52:51 : n with orf 37747 [ INFO] 2018-05-06 21:52:51 : mean orf percent 82.28 [ INFO] 2018-05-06 21:52:51 : n90 262 [ INFO] 2018-05-06 21:52:51 : n70 458 [ INFO] 2018-05-06 21:52:51 : n50 787 [ INFO] 2018-05-06 21:52:51 : n30 1373 [ INFO] 2018-05-06 21:52:51 : n10 2748 [ INFO] 2018-05-06 21:52:51 : gc 0.52 [ INFO] 2018-05-06 21:52:51 : bases n 0 [ INFO] 2018-05-06 21:52:51 : proportion n 0.0 Trinity 2014 version [ INFO] 2018-05-06 21:55:00 : Loading assembly: /home/ljcohen/baseclear/sponge_Trinity_old.fasta [ INFO] 2018-05-06 21:55:13 : Analysing assembly: /home/ljcohen/baseclear/sponge_Trinity_old.fasta [ INFO] 2018-05-06 21:55:13 : Results will be saved in /home/ljcohen/baseclear/transrate_results/sponge_Trinity_old [ INFO] 2018-05-06 21:55:13 : Calculating contig metrics... [ INFO] 2018-05-06 21:55:24 : Contig metrics: [ INFO] 2018-05-06 21:55:24 : ----------------------------------- [ INFO] 2018-05-06 21:55:24 : n seqs 95532 [ INFO] 2018-05-06 21:55:24 : smallest 201 [ INFO] 2018-05-06 21:55:24 : largest 14515 [ INFO] 2018-05-06 21:55:24 : n bases 57208347 [ INFO] 2018-05-06 21:55:24 : mean len 598.84 [ INFO] 2018-05-06 21:55:24 : n under 200 0 [ INFO] 2018-05-06 21:55:24 : n over 1k 13778 [ INFO] 2018-05-06 21:55:24 : n over 10k 3 [ INFO] 2018-05-06 21:55:24 : n with orf 30120 [ INFO] 2018-05-06 21:55:24 : mean orf percent 84.06 [ INFO] 2018-05-06 21:55:24 : n90 252 [ INFO] 2018-05-06 21:55:24 : n70 456 [ INFO] 2018-05-06 21:55:24 : n50 860 [ INFO] 2018-05-06 21:55:24 : n30 1558 [ INFO] 2018-05-06 21:55:24 : n10 3151 [ INFO] 2018-05-06 21:55:24 : gc 0.53 [ INFO] 2018-05-06 21:55:24 : bases n 0 [ INFO] 2018-05-06 21:55:24 : proportion n 0.0 [ INFO] 2018-05-06 21:55:24 : Contig metrics done in 12 seconds [ INFO] 2018-05-06 21:55:24 : No reads provided, skipping read diagnostics [ INFO] 2018-05-06 21:55:24 : No reference provided, skipping comparative diagnostics [ INFO] 2018-05-06 21:55:24 : Writing contig metrics for each contig to /home/ljcohen/baseclear/transrate_results/sponge_Trinity_old/contigs.csv [ INFO] 2018-05-06 21:55:28 : Writing analysis results to assemblies.csv BUSCO Benchmarking universal single-copy orthologs (BUSCO) is a metric for assessing the completeness of a transcriptome against databases of genes expected to be found in a group of species Simo et al. 2015 . This A. crassa transcriptome had an approximately 75% complete BUSCO scores compared to both the metazoan and eukaryota databases. Metazoa # BUSCO version is: 3.0.2 # The lineage dataset is: metazoa_odb9 (Creation date: 2016-02-13, number of species: 65, number of BUSCOs: 978) # To reproduce this run: python /home/ljcohen/busco/scripts/run_BUSCO.py -i /home/ljcohen/A_crassa/assembly/A_crassa.Trinity.fasta -o A_crassa.busco.metazoa -l /home/ljcohen/reference/metazoa_odb9/ -m transcriptome -c 4 # # Summarized benchmarking in BUSCO notation for file /home/ljcohen/A_crassa/assembly/A_crassa.Trinity.fasta # BUSCO was run in mode: transcriptome C:75.0%[S:47.3%,D:27.7%],F:12.4%,M:12.6%,n:978 734 Complete BUSCOs (C) 463 Complete and single-copy BUSCOs (S) 271 Complete and duplicated BUSCOs (D) 121 Fragmented BUSCOs (F) 123 Missing BUSCOs (M) 978 Total BUSCO groups searched Eukaryota # BUSCO version is: 3.0.2 # The lineage dataset is: eukaryota_odb9 (Creation date: 2016-11-02, number of species: 100, number of BUSCOs: 303) # To reproduce this run: python /home/ljcohen/busco/scripts/run_BUSCO.py -i /home/ljcohen/A_crassa/assembly/A_crassa.Trinity.fasta -o A_crassa.euk -l /home/ljcohen/reference/eukaryota_odb9/ -m transcriptome -c 4 # # Summarized benchmarking in BUSCO notation for file /home/ljcohen/A_crassa/assembly/A_crassa.Trinity.fasta # BUSCO was run in mode: transcriptome C:75.2%[S:42.2%,D:33.0%],F:16.2%,M:8.6%,n:303 228 Complete BUSCOs (C) 128 Complete and single-copy BUSCOs (S) 100 Complete and duplicated BUSCOs (D) 49 Fragmented BUSCOs (F) 26 Missing BUSCOs (M) 303 Total BUSCO groups searched Annotation Used protein .fa from Amphimedon queenslandica on NCBI, RefSeq 75.6% of contigs (90,047 out of 119,109) had annotations 59,136 annotations from Amphimedon queenslandica including isoforms. Filtering those with E-value < 1e-05, dropping those with \"NA\" in gene name and choosing only one gene name per contig with top E-value score = 63,084 annotations with 1,910 (3%) from Amphimedon queenslandica . Quantification With salmon v0.9.1. Files are here: https://osf.io/b972e/ ./UC_CTTGTA.quant/aux_info/meta_info.json: \"percent_mapped\": 77.32713084784302, ./OC_GCCAAT.quant/aux_info/meta_info.json: \"percent_mapped\": 79.4706051208496, ./OD_CGATGT.quant/aux_info/meta_info.json: \"percent_mapped\": 79.15476656959088, ./UD_TGACCA.quant/aux_info/meta_info.json: \"percent_mapped\": 79.85752052382412, Comparison with the coral microarray Assembly of these data with the Trinity software package, 2014 version had fewer hits to the microarray than the latest version in 2018. makeblastdb -in sponge_Trinity_old.fasta -dbtype nucl blastn -query Agilent_microarray.fasta -db sponge_Trinity_old.fasta -out microarray_v_Trinity -evalue 1e-5 -outfmt 6 -max_target_seqs 1 blastn -query ../microarray/032951_1467726033734/FASTA\\032951_D_Fasta_20110309.txt -db sponge_Trinity_old.fasta -out microarray_v_Trinity -evalue 1e-5 -outfmt 6 -max_target_seqs 1 ljcohen@js-156-111:~$ cat Trinity_v_microarray comp789632_c0_seq1 CUST_3_PI426266271 100.00 29 0 0 641 669 17 45 8e-09 54.7 comp936137_c4_seq3 CUST_33_PI426226915 94.83 58 3 0 714 771 58 1 6e-20 91.6 comp936137_c4_seq4 CUST_33_PI426226915 94.83 58 3 0 719 776 58 1 6e-20 91.6 comp936137_c4_seq5 CUST_33_PI426226915 94.83 58 3 0 714 771 58 1 6e-20 91.6 comp937294_c5_seq3 CUST_3_PI426226915 100.00 35 0 0 92 126 10 44 1e-12 65.8 comp937846_c1_seq1 CUST_372_PI426266615 93.33 60 4 0 2516 2575 60 1 6e-19 89.8 ljcohen@js-156-111:~$ cat microarray_v_Trinity CUST_371_PI426266615 comp937846_c1_seq1 93.33 60 4 0 1 60 2574 2515 2e-18 89.8 CUST_3_PI426226915 comp937294_c5_seq3 100.00 35 0 0 10 44 92 126 3e-11 65.8 CUST_3_PI426266271 comp789632_c0_seq1 100.00 29 0 0 17 45 641 669 7e-08 54.7 CUST_370_PI426266615 comp937846_c1_seq1 93.33 60 4 0 1 60 2571 2512 2e-18 89.8 CUST_33_PI426226915 comp936137_c4_seq5 94.83 58 3 0 1 58 771 714 6e-19 91.6 CUST_372_PI426266615 comp937846_c1_seq1 93.33 60 4 0 1 60 2575 2516 2e-18 89.8 Trinity 2.6.6 makeblastdb -in /home/ljcohen/A_crassa/assembly/A_crassa.Trinity.fasta -dbtype nucl blastn -query /home/ljcohen/A_crassa/assembly/A_crassa.Trinity.fasta -db /home/ljcohen/microarray/032951_1467726033734/Agilent_microarray.fasta -out microarray_v_Trinity.2.6.6 -evalue 1e-5 -outfmt 6 -max_target_seqs 1 blastn -query /home/ljcohen/microarray/032951_1467726033734/Agilent_microarray.fasta -db /home/ljcohen/A_crassa/assembly/A_crassa.Trinity.fasta -out Trinity.2.6.6_v_microarray -evalue 1e-5 -outfmt 6 -max_target_seqs 1 ljcohen@js-156-111:~/A_crassa/assembly$ cat Trinity.2.6.6_v_microarray CUST_395_PI426266615 TRINITY_DN48401_c7_g1_i10 93.18 44 2 1 12 55 5 47 1e-10 63.9 CUST_301_PI426266615 TRINITY_DN49097_c3_g1_i20 89.47 57 5 1 5 60 184 240 9e-13 71.3 CUST_396_PI426266615 TRINITY_DN48401_c7_g1_i10 93.18 44 2 1 13 56 5 47 1e-10 63.9 CUST_371_PI426266615 TRINITY_DN48711_c1_g1_i3 96.67 60 2 0 1 60 300 359 1e-21 100 CUST_3_PI426226915 TRINITY_DN49097_c3_g1_i5 98.11 53 1 0 1 53 224 276 2e-19 93.5 CUST_12_PI426227955 TRINITY_DN47826_c2_g1_i1 94.74 38 2 0 12 49 74 111 2e-09 60.2 CUST_3_PI426266271 TRINITY_DN36002_c0_g1_i1 100.00 29 0 0 17 45 641 669 9e-08 54.7 CUST_394_PI426266615 TRINITY_DN48401_c7_g1_i10 93.18 44 2 1 11 54 5 47 1e-10 63.9 CUST_370_PI426266615 TRINITY_DN48711_c1_g1_i3 96.67 60 2 0 1 60 303 362 1e-21 100 CUST_33_PI426226915 TRINITY_DN50786_c0_g2_i3 94.83 58 3 0 1 58 120 63 7e-19 91.6 CUST_372_PI426266615 TRINITY_DN48711_c1_g1_i3 96.67 60 2 0 1 60 299 358 1e-21 100 CUST_106_PI426266615 TRINITY_DN45854_c4_g1_i1 90.00 60 6 0 1 60 130 189 5e-15 78.7 CUST_302_PI426266615 TRINITY_DN49097_c3_g1_i20 89.29 56 5 1 6 60 184 239 3e-12 69.4 ljcohen@js-156-111:~/A_crassa/assembly$ cat microarray_v_Trinity.2.6.6 TRINITY_DN49097_c3_g1_i18 CUST_3_PI426226915 98.11 53 1 0 144 196 1 53 3e-21 93.5 TRINITY_DN49097_c3_g1_i5 CUST_3_PI426226915 98.11 53 1 0 224 276 1 53 4e-21 93.5 TRINITY_DN49097_c3_g1_i15 CUST_301_PI426266615 89.47 57 5 1 184 240 5 60 3e-14 71.3 TRINITY_DN49097_c3_g1_i20 CUST_301_PI426266615 89.47 57 5 1 184 240 5 60 3e-14 71.3 TRINITY_DN49097_c3_g2_i1 CUST_3_PI426226915 95.45 44 2 0 96 139 10 53 2e-14 71.3 TRINITY_DN49097_c3_g2_i3 CUST_3_PI426226915 95.45 44 2 0 96 139 10 53 2e-14 71.3 TRINITY_DN45854_c4_g1_i1 CUST_106_PI426266615 90.00 60 6 0 130 189 1 60 7e-17 78.7 TRINITY_DN47826_c2_g1_i1 CUST_12_PI426227955 94.74 38 2 0 74 111 12 49 3e-11 60.2 TRINITY_DN47776_c2_g1_i2 CUST_394_PI426266615 97.14 35 1 0 209 243 23 57 4e-11 60.2 TRINITY_DN47776_c2_g1_i4 CUST_394_PI426266615 97.14 35 1 0 209 243 23 57 4e-11 60.2 TRINITY_DN47776_c2_g1_i3 CUST_394_PI426266615 97.14 35 1 0 245 279 23 57 5e-11 60.2 TRINITY_DN48711_c1_g1_i2 CUST_372_PI426266615 93.33 60 4 0 299 358 1 60 3e-19 89.8 TRINITY_DN48711_c1_g1_i3 CUST_372_PI426266615 96.67 60 2 0 299 358 1 60 5e-23 100 TRINITY_DN48711_c1_g1_i6 CUST_372_PI426266615 93.33 60 4 0 299 358 1 60 7e-20 89.8 TRINITY_DN48711_c1_g1_i7 CUST_372_PI426266615 93.33 60 4 0 299 358 1 60 1e-19 89.8 TRINITY_DN48711_c1_g1_i8 CUST_372_PI426266615 93.33 60 4 0 299 358 1 60 1e-19 89.8 TRINITY_DN48401_c7_g1_i10 CUST_394_PI426266615 93.18 44 2 1 5 47 11 54 3e-12 63.9 TRINITY_DN51186_c1_g1_i1 CUST_33_PI426226915 94.83 58 3 0 600 657 58 1 5e-20 91.6 TRINITY_DN51186_c1_g1_i4 CUST_33_PI426226915 94.83 58 3 0 605 662 58 1 5e-20 91.6 TRINITY_DN51186_c1_g1_i2 CUST_33_PI426226915 94.83 58 3 0 600 657 58 1 4e-20 91.6 TRINITY_DN51186_c1_g1_i11 CUST_33_PI426226915 94.83 58 3 0 605 662 58 1 4e-20 91.6 TRINITY_DN51186_c1_g1_i15 CUST_33_PI426226915 94.83 58 3 0 616 673 58 1 6e-20 91.6 TRINITY_DN51186_c1_g1_i14 CUST_33_PI426226915 94.83 58 3 0 600 657 58 1 6e-20 91.6 TRINITY_DN51186_c1_g1_i13 CUST_33_PI426226915 94.83 58 3 0 616 673 58 1 6e-20 91.6 TRINITY_DN51186_c1_g1_i18 CUST_33_PI426226915 94.83 58 3 0 616 673 58 1 6e-20 91.6 TRINITY_DN51186_c1_g1_i9 CUST_33_PI426226915 94.83 58 3 0 605 662 58 1 7e-20 91.6 TRINITY_DN51186_c1_g1_i19 CUST_33_PI426226915 94.83 58 3 0 605 662 58 1 7e-20 91.6 TRINITY_DN51186_c1_g1_i20 CUST_33_PI426226915 94.83 58 3 0 370 427 58 1 2e-20 91.6 TRINITY_DN51186_c1_g1_i17 CUST_33_PI426226915 94.83 58 3 0 600 657 58 1 6e-20 91.6 TRINITY_DN51186_c1_g1_i6 CUST_33_PI426226915 94.83 58 3 0 605 662 58 1 7e-20 91.6 TRINITY_DN51186_c1_g1_i12 CUST_33_PI426226915 94.83 58 3 0 605 662 58 1 6e-20 91.6 TRINITY_DN36002_c0_g1_i1 CUST_3_PI426266271 100.00 29 0 0 641 669 17 45 8e-09 54.7 TRINITY_DN50786_c0_g1_i15 CUST_33_PI426226915 97.22 36 1 0 1 36 36 1 3e-11 62.1 TRINITY_DN50786_c0_g1_i9 CUST_33_PI426226915 97.22 36 1 0 1 36 36 1 3e-11 62.1 TRINITY_DN50786_c0_g1_i12 CUST_33_PI426226915 97.22 36 1 0 1 36 36 1 1e-11 62.1 TRINITY_DN50786_c0_g2_i5 CUST_33_PI426226915 94.83 58 3 0 132 189 58 1 1e-20 91.6 TRINITY_DN50786_c0_g2_i3 CUST_33_PI426226915 94.83 58 3 0 63 120 58 1 2e-20 91.6 TRINITY_DN49887_c4_g2_i14 CUST_3_PI426226915 100.00 34 0 0 1 34 5 38 4e-12 63.9 References The Analysis of Eight Transcriptomes from All Poriferan Classes Reveals Surprising Genetic Complexity in Sponges Deep developmental transcriptome sequencing uncovers numerous new genes and enhances gene annotation in the sponge Amphimedon queenslandica The genome of the contractile demosponge Tethya wilhelma and the evolution of metazoan neural signalling pathways Gene Expression Dynamics Accompanying the Sponge Thermal Stress Response A Large and Consistent Phylogenomic Dataset Supports Sponges as the Sister Group to All Other Animals Elements of a nervous system' in sponges Comparative transcriptome analysis reveals insights into the streamlined genomes of haplosclerid demosponges Transcriptome sequencing and delimitation of sympatric Oscarella species (O. carmela and O. pearsei sp. nov) from California, USA Carballo, JL et al. 1996. Use of marine sponges as stress indicators in marine ecosystems at Algeciras Bay (southern Iberian Peninsula). Marine Ecology Progress Series. 135:109-122 . A genomic view of 500 million years of cnidarian evolution Existing Genome/Transcriptome resources for other sponge species Amphimedon queenslandica genome (model sponge species) A. queenslandica transcriptome and annotation resources Aplysina aerophoba 1,360 Porifera SRA records, 475 RNAseq Haliclona tubifera transcriptome shotgun sequencing (TSA) porifera Oscarella carmela only 25 NCBI Nucleotide records for Aiolochroia crassa Reconstruction of Family-Level Phylogenetic Relationships within Demospongiae (Porifera) Using Nuclear Encoded Housekeeping Genes aldolase (ALD) catalase (CAT) EF1alpha metagenomes collected from sponge","tags":"transcriptomes","url":"https://johnsolk.github.io/blog/de-novo-transcriptome-assembly-annotation-and-evaluation-of-the-branching-tube-caribbean-sponge-aiolochroia-crassa.html"},{"title":"Killifish PromethION data sharing","text":"We have new long-read ONT PromethION genomic DNA sequencing data from 4 species of killifish! While we're working with it and generating ~40x coverage of new Illumina PE150 data for each species, we wanted to share this information and data. Since there are not many PromethION instruments out there, to my knowledge, these data are probably some of the first of its kind to be shared, besides human data . Update (06/27/2019) Killifish genome sequence ONT fastq and signal files and Illumina fastq can all be found in the ENA: PRJEB29136 Questions for the community The purpose of this blog post is mainly to share our plan, seek advice, and see if anyone is interested in these data. We are planning to post these data publicly in the hopes that they can serve methods and workflow developers! How can we make these data most useful to the bioinformatics community? Contact me: twitter or email We're generating these ONT data for the purpose of assembling four reference genomes for Fundulus killifish species for whom we have a genome from a close (5-20 million years diverged) relative, Fundulus heteroclitus . We're in the process of generating 40x coverage of Illumina data for each species to improve the consensus assemblies. By today's standards of genome assembly , this project is not the most sophisticated. We're not planning to do anything fancy to improve the scaffolding with optical and chromatin interaction mapping . Just quick and dirty genomes for around $3500 each. This is also by no means a perfect data set by ONT standards. Our DNA samples - from all species - have consistently not been utilizing the pores as efficiently as possible on either MinION or PromethION flowcells, despite using fresh tissue and troubleshooting efforts with different DNA extraction and cleanup methods. Therefore, our yields from ONT sequencing have not been as high as other groups have gotten. But, from our experience, this is what can happen when new species are sequenced with a new technology. Despite these shortcomings, 20-30 Gb/species ONT data are great to have! Better than Illumina data alone. Since this is a new type of ONT data from the PromethION, we're in unchartered territory here about some things, so have questions: What does one do with the millions of raw fast5 signal files from the PromethION? Our basecalled fastq are 70-100 GB, but the directory of raw fast5 signal files is >600 GB for each run! Wow. To our knowledge, the only software that uses these files to polish the reads is Nanopolish by Jared Simpson's group . Since the files are so large, I haven't tried using Nanopolish yet. It it a good idea to try? What is the best way to share >600 GB of millions of fast5 files with people who might be interested in using it? Data are unfiltered. Should we remove DNA CS lambda phage spike-in before sharing in a public repository? What is the best way to look for reads suspected as errors (>900 Mb)? Should these be removed before sharing in a public repository? Where is the best place to share these sequencing data? Even though zenodo has recently upgraded their file size limits to 50 GB , it is not a good place to store these data based on their response to my request: Dear Lisa, We're terribly sorry for the delay in response. Upload quota increase requests for bigger datasets have to be passed up the hierarchy, which usually takes time. I'm sorry to inform you that we, unfortunately, will not be able to host the dataset as it surpasses our max allowed quota increase too much. By default, we provide a one-time quota increase up to 100GB for a dataset that will be cited from a peer-reviewed article. Any other advice, questions, or comments on things that perhaps we are not considering? Our plan from here: 1. Post data to a public nucleic acid repository European Nucleotide Archive (ENA)-EBI or SRA-NCBI? (I will post the links to records here once they are available.) What to do with the raw fast5 signal files? 2. Write a data note Either F1000 research or GigaScience 3. Use the genome assemblies! The ultimate goal is to do some comparative genomics analyses between these genomes, use them as references for RNAseq data, and scan for regulatory regions of interest, e.g. salinity-responsive enhancing elements (OSRE) . Thank you! This is a collaborative effort between my two advisors, Dr. C. Titus Brown and Dr. Andrew Whitehead. (Seriously, I can't believe how fortunate I am to have two such supportive advisors and a willingly collaborative network of lab members and colleagues.) Other contributors to this project have been Dr. Ruta Sahasrabudhe, Dr. Lutz Froenicke , Tony Gill , Jen Roach , and committee member Dr. Megan Dennis . Special thanks to instructors at PoreCamp, Texas , Dr. Charlie Johnson and Dr. Richard Metz at Texas A&M Agrilife Research Sequencing Facility (where PoreCamp was held) for Illumina NovaSeq data from Fundulus olivaceus ). THANK YOU to Dr. David Duvernell at Missouri University of Science & Technology and Dr. Jake Schaefer at the University of Southern Mississippi for sending us all of these fish to us, live! All of this is made possible by funding support from the Moore Foundation Data-Driven-Discovery investigator award to Dr. C. Titus Brown. Pics from Porecamp at Texas A&M , June 2017: Below is a bit of background on the project, methods, preliminary results, references, and a list of some tools I've compiled. New PromethION instrument at the UC Davis Genome Center In May 2018 , the UC Davis Genome Center received its PromethION from Oxford Nanopore Technologies (ONT)! This was part of a collaborative effort to acquire the instrument through the PromethION early release program (PEAP) back in early 2016 by Genome Center faculty: Dr. Richard Michelmore , Dr. John McPherson , Dr. Megan Dennis (who is on my dissertation committee!), Dr. Luis Carvajal-Carmona , and Dr. C. Titus Brown . Around 2016, I started working with the ONT MinION to see if it is feasible to use for genome assemblies. First, with bacteria then scaling up to see if it would work for eukaryotic organisms . It turns out that the MinION does not scale up very well for us, at least for these killifish. To achieve 30-50x coverage for a genome assembly of a 1-1.1 Gb killifish genome, we would need to use 30-50 MinION flowcells. At a cost of $800-900/flowcell , that is not a reasonably affordable genome ($24,000-45,000). Whereas it took 39 flowcells to get ~30x (90Gb) coverage of the human genome , five PromethION flowcells sequenced ~80x coverage of the human genome . PromethION promises higher yield than MinION, allowing it to be more feasible for larger genomes >1 Gb. The cost of one PromethION run for UC Davis folks is $2,880. As you see below, we have not achieved maximally high yields from the Promethtion, but we have at least achieved ~20-30x coverage of our killifish genomes. The avalance of data commences! Four species of Fundulus killifish These four species of killifish have never been sequenced before. We have a genome for the classic lab and field model species, Fundulus heteroclitus for understanding how organisms and populations interact with the external environment. But we do not have any genomic information from other important related freshwater species of Fundulus . Why killifish? The Fundulus heteroclitus killifish has been well-studied because of its physiological resilience to environmental change, including temperature and salinity. It has also been found to have rapidly evolved in polluted environments . While F. heteroclitus and most marine Fundulus species are capable of osmoregulating across a wide range of salinities, freshwater Fundulus species have lost much of their osmoregulatory flexibility at higher salinities compared to their marine ancestors. The physiological requirement for high (or low) concentrations of salt ions in the water has affected geographic distributions of aquatic animals throughout evolutionary history and will continue to impact species worldwide in view of future changes in climate. To study the history of adaptation to different salinities and understand the mechanisms of evolution at the molecular level, we have been studying this group of related killifish from the genus Fundulus , which contains species that are natively adapted to different salinities. Some Fundulus species can tolerate a range of salinities (euryhaline) by switching osmoregulatory mechanisms while others require a more narrow salinity range (stenohaline) in either fresh or marine waters. Unique to this family of killifish is evidence that some species from the genus Fundulus have radiated into fresh water bodies on three separate, parallel occasions between 5-25 million years ago, and have lost their ability to tolerate higher salinities like their euryhaline relatives. We use Fundulus as a comparative model system for studying the physiological and genetic mechanisms that diverge between euryhaline and stenohaline freshwater species. Four Fundulus killifish representing clades 1 and 3 were selected for genome sequencing to study the physiological and genetic mechanisms that diverge between euryhaline and stenohaline freshwater species. Other species of freshwater and brackish water killifish from Clade 2 ( Lucania parva and Lucania goodei ) are being sequenced by Dr. Becky Fuller's lab at U. Illinois Urbana-Champaign . If you find these questions compelling and would consider collaborating, please contact us. Whitehead et al. 2010. Phylogeny of killifish osmotic tolerance. Whitehead et al. 2011. Genomic mechanisms of evolved physiological plasticity in killifish distributed along an environmental salinity gradient. Brennan et al. 2015. Reciprocal osmotic challenges reveal mechanisms of divergence in phenotypic plasticity in the killifish Fundulus heteroclitus. Brennan et al. 2018. Integrative Population and Physiological Genomics Reveals Mechanisms of Adaptation in Killifish. DNA extractions Live samples were shipped to UC Davis by Dr. David Duvernell and Dr. Jake Schaefer . When possible (all but Fundulus catenatus ), samples were extracted from fresh tissue. Fish heads were used for high molecular weight (hmw) DNA extractions. Ultra-long read sequencing protocol by Josh Quick and Nick Loman works well, TLB + phenol:chloroform extraction Agilent #200600 DNA extraction kit found by Tony works well also, requires cleanup We have all found that fresh tissue is better than frozen tissue Pulse field gels (Thanks, Tony!) are necessary to visualize hmw DNA With Dr. Ruta, we tried the Qiagen - p. 39 \"DNA purification from tissue using the Gentra puregene Tissue Kit\" and found it was similar to the Agilent kit. Our DNA was still difficult to get into solution, which then required phenol:chloroform cleanup. DNA has always been difficult to get into solution (see pic of 0.5 mL tube on the far right below) Dr. Ruta found that adding urea to lysis buffer helped the final DNA pellet to be less brittle and go into solution easier Wasko et al. 2003 In general, there is something weird with our samples. ONT has mentioned that this could a fish-specific sample issue, based on experiences with other customers. Has anyone else found this to be the case with fish hmw DNA being difficult to get into solution, then becoming brittle, breaking easily during cleanup? These are Ruta's pulse field gels showing samples that were sequenced (in order from left to right: F. catenatus (sheared vs. unsheared), F. olivaceus , F. nottii , A. xenica ). Fresh tissues were extracted from A. xenica , F. nottii . Frozen tissue from F. catenatus and F. olivaceus . Sequencing Here are the ONT sequencing data we have so far: Species bases called n reads avg length largest reads N50 Adinia xenica 38,467,326,719 15,704,522 2,449 953,774 5,733, n = 1,373,426 Fundulus nottii 33,440,866,723 5,160,367 6,480 667,947 12,995, n=700,534 Fundulus catenatus 40,274,806,587 23,701,206 1,699 590,485 3,439, n = 2,687,295 Fundulus olivaceus (MinION) 4,962,626,713 740,248 6,704 973,552 12,726, n = 117,202 Fundulus olivaceus (PromethION) 50,093,027,850 10,902,817 4,595 779,368 11,670, n = 987,921 Overall project status: Species native physiology clade ONT data Illumina miniasm assembly N50 miniasm assembly size Adinia xenica Marine 3 38 Gb TBD 369,038; n = 794 961,615,159 Fundulus nottii Freshwater 3 33 Gb TBD 2,025,917 Mb; n = 151 1,141,443,860 Fundulus catenatus Freshwater 1 40 Gb TBD 119,326; n = 2,612 975,193,007 Fundulus olivaceus Freshwater 3 50 Gb (PromethION) 180 Gb 2,154,700, n = 158 1,284,878,256 Assemblies of Fundulus olivaceus data with ONT (MinION) and Illumina (NovaSeq) data: Seq data input Tool bases n_contigs average largest N50 BUSCO (eukaryota) ONT (MinION) Canu 9,804,264 540 18,156 365,191 40,681, n = 43 0.7% ONT (MinION) Miniasm 4,917,546 153 32,140 233,136 50,056, n = 25 0.0 % Illumina (NovaSeq) Megahit 1,183,861,293 1038799 1,139 88,218 3,846, n = 77,800 45.6 % Illumina (NovaSeq) ABySS 1,381,148,284 1024759 1,347 140629 9833, n = 37,013 77.9% Hybrid Masurca 1,134,160,060 90,237 12,568 386,222 42,823, n = 7,616 86.2% Hybrid SSPACE TBD Preliminary data products Rough assemblies and QC are available on osf . pip install osfclient export OSF_PASSWORD=password export OSF_USERNAME=email.address # list files in OSF osf -p zjv86 ls # copy files from OSF to local (or hpc - wherever you're working) osf -p zjv86 clone Folivaceus_hybrid_genome_assembly References Jain M et al. 2017. A Fast Approximate Algorithm for Mapping Long Reads to Large Reference Databases. Loose et al. 2016. Real time selective sequencing using nanopore technology. The anticipated increasing speed of nanopore sequencing ('fast mode') and the scaling up of the MinION to 3,000 channels, and the PromethION with 144,000 channels, will challenge the implementation of 'Read Until' in real time and require algorithmic enhancements and computational power Jain C et al. 2017. Nanopore sequencing and assembly of a human genome with ultra-long reads. Nice slides by Benjamin Schwessinger explaining wet lab comparisons for hmw DNA prep. Tan et al. 2018. Finding Nemo: hybrid assembly with Oxford Nanopore and Illumina reads greatly improves the clownfish ( Amphiprion ocellaris ) genome assembly. Miller et al. 2018. GENOME REPORT: Highly Contiguous Genome Assemblies of 15 Drosophila Species Generated Using Nanopore Sequencing. Evidence that the consensus assembly can be dramatically improved by using the pilon/racon program with Illumina data. Cao et al. 2016. Scaffolding and Completing Genome Assemblies in Real-time with Nanopore Sequencing Tyler et al. 2018. Evaluation of Oxford Nanopore's MinION Sequencing Device for Microbial Whole Genome Sequencing Applications De Roeck et al. 2018. Accurate characterization of expanded tandem repeat length and sequence through whole genome long-read sequencing on PromethION. List of available tools for ONT data: Suggestions for additions to this list are welcome! (Especially if you have experience with them working well!) QC & Evaluation: Porechop : removing adapters NanoPlot : quality report sourmash : identify contamination BUSCO : content evaluation Nanocomp : compare multiple ONT runs mashmap : for visualization (replaces nucmer) Assemblers: canu, long reads : quick start , tutorial and paper masurca, hybrid assembly (on conda ) SPAdes hybrid assembly (for bacteria) Alpaca, hybrid assembly and paper SSPACE-LongRead scaffolding (no instructions, but a paper !), here are my instructions for SSPACE-LongRead miniasm , uses minimap overlapper, no consensus step Polishing and consensus improvement: Unicycler polish ( paper ) Nanopore polish, input ONT assembly with Illumina data Nanopolish Nanocorr : error correction (See presentation ) racon pilon npScarf , paper Here is the pipeline from Miller et al. 2018 15 Drosophila genomes paper . Example rough workflow steps: Install miniconda Python package manager: wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b echo export PATH=\"$HOME/miniconda3/bin:$PATH\" >> ~/.bashrc source ~/.bashrc Create a conda environment for software installation: conda create -n ONT python=3 source activate ONT conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda Install minimap2 (minimap2-2.12), miniasm and other tools: conda install assembly-stats minimap2 miniasm bwa samtools busco pilon mashmap Assemble: # Overlap minimap2 -x ava-ont -t8 A_xenica_combined.fastq | gzip -1 > A_xenica_reads.paf.gz # Layout miniasm -f A_xenica_combined.fastq A_xenica_reads.paf.gz > A_xenica_reads.gfa awk '/&#94;S/{print \">\"$2\"\\n\"$3}' A_xenica_reads.gfa | fold > A_xenica.fa assembly-stats A_xenica.fa Use pilon to improve consensus: bwa index <genome.fasta> bwa mem -t 4 genome.fasta R1.fq R2.fq | samtools view -Sb - | samtools sort - -o ecto_old_paired_qc.sorted.bam samtools index ecto_old_paired_qc.sorted.bam pilon --genome F_olivaceus.canu.ONT.unitigs.fasta --bam F_olivaceus_ONT_polished.sorted.bam --threads 8 BUSCO for evaluation: run_BUSCO.py -i F_nottii_miniasm_ONT.fa -l ${ BUSCO_LINEAGE } /eukaryota_odb9 -o F_nottii_miniasm --mode genome -c 8 Visualize 1:1 comparison with closely-related reference genome, run mashmap : mashmap \\ -r /mnt/home/ljcohen/reference/GCF_000826765.1_Fundulus_heteroclitus-3.0.2_genomic.fna \\ -q ../../masurca_Folivaceus/CA.mr.41.15.15.0.02/final.genome.scf.fasta Plot: generateDotPlot png large mashmap.out Edit the out.gp file, because of reasons. vi out.gp Delete word 'tiny' in first line Comment out 3 lines with word 'mouse' in them. Then run: gnuplot out.gp which will then produce the png file. Tony Gil demonstrating how to wash a hmw DNA pellet with 70% ethanol:","tags":"data","url":"https://johnsolk.github.io/blog/killifish-promethion-data-sharing.html"},{"title":"Blogging with Pelican","text":"Luiz Irber was gratious in spending time to walk us through how to set up a blog with Pelican, host it on github.io , and have travis-ci test and deploy. Video: Some Background I've had a blog that I started when I was first learning and developing computational skills. Wordpress blogs are great for quickly publishing a post, with capacity for formatting pictures and links to Tweets. At the encouragement of Titus a few years ago during our summer DIBSI workshop , I started taking notes on the online collaborative document site, hackmd.io . This cool site lets you write in markdown and immediately see it rendered. The link can be shared with collaborators, and the document can be exported to .pdf , .html , or .md formats. I started writing all of my notes with markdown. But then wanting to write a blog post with my notes? Manually converting .md documents and code chunks into the online wordpress user interface is tedious. This week, I casually mentioned that I wanted to migrate from wordpress to a more markdown-friendly format for my blog. Luiz immediately chimed in that Pelican is great! And uses it for his blog . It turns out that Titus' blog and other lab mate, Charles Reid both have their blogs in Pelican as well. One of the many advantages of Pelican is that it is written in Python, unlike Jekyll which is written in Ruby. (I'm more comfortable coding and debugging in Python than in Ruby.) And Charles provided lots of documentation on how to use Pelican! It's great to be in a lab group where people have such useful and encouraging advice. Tutorial We started with the tutorial from Charles Reid , which has practice content . Make a working directory for your blog: mkdir blog cd blog/ git init Get the configuration file, pelicanconf.py : wget https://raw.githubusercontent.com/charlesreid1/magic-flying-pelican/master/pelican/pelicanconf.py mkdir content cd content/ Navigate to some of the example posts and download: wget https://raw.githubusercontent.com/charlesreid1/magic-flying-pelican/master/pelican/content/posts/hello-world-1.md Taking a look, we see that each .md post file must have this header: Title : Hello world number one Date : 2018 - 05 - 13 22 : 00 Category : Python Tags : pelican Author : charlesreid1 Summary : First hello world post Now we have to install Pelican and markdown. Luiz uses pipenv . This will activate a new virtualenv with pelican and markdown available without messing with dependencies in your computer's $PATH. pip install --user pipenv export PATH=~/.local/bin:$PATH pipenv install pelican markdown pipenv shell Now we need some Pelican themes . (All themes are downloaded, so put them in a different location.) cd ../../ git clone --recursive https://github.com/getpelican/pelican-themes cd pelican-themes pelican-themes -i bootstrap Change the path location of the theme in the pelicanconf.py file: cd blog cp -a ../pelican-themes/bootstrap theme vi pelicanconf.py Specify the path where the theme will be saved to (this was copied with the command above). THEME = './theme' Change other information, such as your name and site name and site url in the pelicanconf.py file. wget https://raw.githubusercontent.com/charlesreid1/magic-flying-pelican/master/pelican/content/posts/hello-world-1.md mkdir posts mv hello-world-1.md posts/ mv hello-world-1.md 2018-09-05.md vi 2018-09-05.md Then run: pelican This will produce output like this: WARNING : Removed extraneous trailing slash from SITEURL . Done : Processed 2 articles , 0 drafts , 0 pages and 0 hidden pages in 0.55 seconds . Check how it looks: python -m http.server (Go to http://localhost:8000/output/ ) At anytime, you can remove the output/ directory, because this is what is generated when running pelican . rm -rf output/ Hosting on github.io Add/commit changes, then set up a repository on GitHub (don't initialize with README.md ) to add as remote. git add --all git status git commit -m \"first commit\" git remote add origin https://github.com/ljcohen/blog.git git push -u origin master Install ghp-import: pipenv ghp - import pipenv install ghp - import git status git diff ghp - import which ghp - import ghp - import -- help ghp - import pelican pelican ghp - import output git branch ghp - import - p output Using travis-ci Make a travis .yml file: vi .travis.yml With this content: language : python dist : xenial python : '3.6' branches : only : - master install : - pip install pipenv - pipenv install script : - pipenv run pelican deploy : provider : pages skip - cleanup : true local - dir : output github - token : $GITHUB_TOKEN # Set in the settings page of your repository on : branch : master Add/commit: git add .travis.yml git commit -m \"added travis-ci\" git push -u origin master Go to travis-ci and login to GitHub. Set up your personal access tokens on your GitHub account. Go to 'Settings', 'Integration & services' in the GitHub repository to configure Travis CI with a key to give push access. Add the repository to Travis CI: https://travis-ci.com/ljcohen/blog You're done! Now add more posts! vi 2018-09-05_b.md vi 2018-09-05_b.md git add 2018-09-05_b.md git commit -m \"new post\" git push vi pelicanconf.py Travis CI will automatically run pelican and push to github. With Pelican, you can also import an existing site, like from WordPress . Thank you, Luiz!","tags":"Python","url":"https://johnsolk.github.io/blog/blogging-with-pelican.html"}]}